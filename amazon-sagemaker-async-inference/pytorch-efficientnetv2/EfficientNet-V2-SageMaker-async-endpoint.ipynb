{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを実行したら、メニューの「Kernel」→「Restart」をクリックしてカーネルを再起動してから続きのセルを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -q -U sagemaker ipywidgets\n",
    "!pip install --upgrade --no-cache-dir torchvision torch==\"1.11.0+cu102\"\n",
    "!pip install timm==0.4.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch EfficientNet-V2 を SageMaker 非同期エンドポイントにデプロイ\n",
    "\n",
    "このノートブックでは、学習済みの PyTorch の EfficientNet-V2 モデルを SageMaker 非同期推論エンドポイントにデプロイし、エンドポイントの AutoScaling 設定をします。AutoScaling 設定で最小インスタンス数をゼロに設定することにより、推論リクエストがない場合はエンドポイントで使用するインスタンス数をゼロまで Scale In することができます。\n",
    "\n",
    "**NOTE**: このノートブックは、ノートブックインスタンスの conda_python_p38 で動作を確認しました。\n",
    "\n",
    "## 推論エンドポイント作成の流れ\n",
    "\n",
    "まず Model を作成し、Model を使って Endpoint Config を作成します。Endpoint Config を使って Endpoint を起動します。Endpoint には AutoScaling 設定をアタッチすることができます。いったん起動した Endpoint の中の Endpoint Config はダウンタイムなしで更新することが可能です。このしくみによって、エンドポイント起動後にデプロイしたモデルをダウンタイムなしで入れかえることが可能です。\n",
    "\n",
    "<img src=\"structure.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するライブラリの Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Session, get_execution_role\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "endpoints = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timm からモデルをダウンロードし、Amazon SageMaker がこのモデルをデプロイするための形式にします。PyTorch を使用する場合、SageMakerは `.tar.gz` フォーマットの単一のアーカイブファイルを期待しているため、モデルファイルをルートフォルダに、推論用のコードを `code` フォルダに格納します。アーカイブの構造は以下のようになります。\n",
    "\n",
    "\n",
    "```\n",
    "/model.tar.gz\n",
    "/--- model.pth\n",
    "/--- code/\n",
    "/--- /--- inference.py\n",
    "/--- /--- requirements.txt (optional)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import tarfile\n",
    "\n",
    "# Load the model\n",
    "model = timm.create_model(\"tf_efficientnetv2_b0\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "input_shape = torch.rand(1, 3, 224, 224)\n",
    "model_trace = torch.jit.trace(model, input_shape)\n",
    "model_trace.save('model.pth')\n",
    "\n",
    "with tarfile.open('gpu_model.tar.gz', 'w:gz') as f:\n",
    "    f.add('model.pth')\n",
    "    f.add('code/gpu-inference.py', 'code/inference.py')\n",
    "f.close()\n",
    "\n",
    "pytorch_efficientnetv2_prefix = 'pytorch/efficientnetv2'\n",
    "gpu_model_data = sess.upload_data('gpu_model.tar.gz', bucket, pytorch_efficientnetv2_prefix)    \n",
    "   \n",
    "print(f'Model stored in {gpu_model_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and test on GPU (ml.g4dn.xlarge)\n",
    "\n",
    "The instance chosen this time is a `ml.g4dn.xlarge`. It has great throughput and the cheapest way of running GPU inferences on the AWS cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model = PyTorchModel(model_data=gpu_model_data,\n",
    "     entry_point='gpu-inference.py',\n",
    "     source_dir='code',\n",
    "     role=role,\n",
    "     framework_version='1.10',\n",
    "     py_version='py38'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pth_model.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge')\n",
    "\n",
    "endpoints['g4dn'] = predictor.endpoint_name\n",
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンテナイメージの取得\n",
    "\n",
    "エンドポイントで使用するコンテナイメージを取得します。このサンプルノートブックでは、SageMaker が用意した PyTorch のコンテナイメージを使用します。独自のコンテナイメージを使いたい場合は、[こちらのドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html) や [こちらのサンプルコード](https://github.com/aws/amazon-sagemaker-examples/tree/main/advanced_functionality/multi_model_bring_your_own) を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from time import strftime,gmtime\n",
    "from sagemaker.image_uris import retrieve\n",
    "boto_session = boto3.session.Session()\n",
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "region = boto_session.region_name\n",
    "sm_runtime = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "pytorch_inference_image_uri = retrieve('pytorch',\n",
    "                                       region,\n",
    "                                       version='1.10',\n",
    "                                       py_version='py38',\n",
    "                                       instance_type = 'ml.g4dn.xlarge',\n",
    "                                       accelerator_type=None,\n",
    "                                       image_scope='inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sagemaker-efficientnetv2-{0}'.format(str(int(time.time())))\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': pytorch_inference_image_uri,\n",
    "        'ModelDataUrl': gpu_model_data,\n",
    "        'Environment': {\n",
    "            'TS_MAX_REQUEST_SIZE': '100000000', #default max request size is 6 Mb for torchserve, need to update it to support the 70 mb input payload\n",
    "            'TS_MAX_RESPONSE_SIZE': '100000000',\n",
    "            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\n",
    "        }\n",
    "    },    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint Config の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"PyTorchAsyncEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "bucket_prefix = \"async-result\"\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            \"S3OutputPath\": f\"s3://{bucket}/{bucket_prefix}/output\",\n",
    "            #  Optionally specify Amazon SNS topics\n",
    "            \"NotificationConfig\": {\n",
    "#               \"SuccessTopic\": success_topic,\n",
    "#               \"ErrorTopic\": error_topic,\n",
    "            }\n",
    "        },\n",
    "        \"ClientConfig\": {\n",
    "            \"MaxConcurrentInvocationsPerInstance\": 2\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Created EndpointConfig: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint の作成\n",
    "\n",
    "以下のセルを実行して SageMaker 非同期推論エンドポイントを起動します。エンドポイントが InService になるまで数分かかるので待ちます。SageMaker コンソールの左側のメニューから「推論」→「エンドポイント」とクリックすると、今起動中のエンドポイントのステータスが `Creating` になっているはずです。ここが `InService` になれば推論を開始することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"sm-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "print(f\"Creating Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoScaling の有効化\n",
    "\n",
    "エンドポイントのステータスが `InService` になったら、以下のセルを実行してエンドポイントの AutoScaling の設定をします。このときに `MinCapacity` にゼロを指定することで、推論リクエストが発生していない時にインスタンス数をゼロまで Scale In することができます。現在のところ、SageMaker の推論エンドポイントの機能でインスタンス数をゼロまで Scale In できるのは、非同期推論エンドポイントのみです（リアルタイム推論エンドポイントは最小インスタンス数が 1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('application-autoscaling') # Common class representing Application Auto Scaling for SageMaker amongst other services\n",
    "\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'variant1' # This is the format in which application autoscaling references the endpoint\n",
    "\n",
    "response = client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=0,  \n",
    "    MaxCapacity=5\n",
    ")\n",
    "\n",
    "response = client.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id, # Endpoint name \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 3.0, # The target value for the metric. \n",
    "        'CustomizedMetricSpecification': {\n",
    "            'MetricName': 'ApproximateBacklogSizePerInstance',\n",
    "            'Namespace': 'AWS/SageMaker',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name }\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "        },\n",
    "        'ScaleInCooldown': 20, # The cooldown period helps you prevent your Auto Scaling group from launching or terminating \n",
    "                                # additional instances before the effects of previous activities are visible. \n",
    "                                # You can configure the length of time based on your instance startup time or other application needs.\n",
    "                                # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \n",
    "        'ScaleOutCooldown': 120 # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        \n",
    "        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled. \n",
    "                            # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非同期推論の実行\n",
    "\n",
    "以下の画像を使って非同期推論を実行してみましょう。非同期推論は推論の入力データが S3 に保存されていることを期待しているため、画像を S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('cat.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_s3_path = sess.upload_data('cat.jpg', bucket, 'image')\n",
    "image_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`invoke_endpoint_async` API で非同期推論を実行します。response の中には、推論結果そのものではなく、推論結果が記載されたファイルの S3 パスの情報が格納されておいます。バックログが溜まっていなければすぐにこのファイルは作成されますが、バックログにリクエストが溜まっている場合は順次推論が実行されるためファイルが作成されるまでに時間がかかることがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=image_s3_path)\n",
    "output_location = response['OutputLocation']\n",
    "print(f\"OutputLocation: {output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://deep-learning-models/image-models/imagenet_class_index.json ./\n",
    "    \n",
    "import json\n",
    "with open(\"./imagenet_class_index.json\", \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論結果のファイルを読み込んで確認します。beagle と表示されれば正しい推論結果が得られたと言えるでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "output_url = urllib.parse.urlparse(output_location)\n",
    "output = sess.read_s3_file(bucket=output_url.netloc, key_prefix=output_url.path[1:])\n",
    "result = ast.literal_eval(output)[0]\n",
    "pred = np.argmax(result)\n",
    "class_idx[str(pred)][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコマンドを実行すると、エンドポイントが使用しているインスタンス数を `CurrentInstanceCount` として確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.describe_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n",
    "response['ProductionVariants']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの更新\n",
    "\n",
    "現在の SageMaker 非同期エンドポイントでは、AutoScaling によっていったんインスタンス数がゼロになった場合、推論リクエストがバックログに 4つ以上貯まらないと再度インスタンス数の Scale Out が実行されません。そのため、推論リクエストが 1件でも発生したらインスタンスを起動するためのワークアラウンドを紹介します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンドポイントの数を更新\n",
    "\n",
    "以下のように、エンドポイントに対して `DesiredInstanceCount` を設定し直すことで指定された台数のインスタンスが起動します。以下のセルを実行すると推論絵dのポイントが Updating の状態になり、このサンプルコードの例だと 4分ほどでインスタンスが起動してバックログに積まれた推論リクエストが実行されます。\n",
    "\n",
    "実際のワークフローでは、上記 `describe_endpoint` API を実行して `CurrentInstanceCount` を確認し、インスタンス数がゼロだったら以下のように `update_endpoint_weights_and_capacities` API を実行してインスタンスを 1つ以上起動するなどの使い方が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            'VariantName': 'variant1',\n",
    "            'DesiredInstanceCount': 1\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントに関するその他の設定を更新する場合は、エンドポイント設定をパラメタに指定して `update_endpoint` API を実行します。この操作でもいったんゼロになったインスタンス数が `InitialInstanceCount` で指定した値で再度起動されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.update_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "以下のセルを実行して、不要になったエンドポイントを削除してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sagemaker.predictor.Predictor(endpoint_name=endpoint_name\n",
    "pred.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
