{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル実験開発 LightGBM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. はじめに\n",
    "\n",
    "このノートブックでは、SageMakerのトレーニングジョブを使用して、機械学習モデル（LightGBM）の実験開発を行うためのサンプルを用意しています。[こちらの AWS ブログ](https://aws.amazon.com/jp/blogs/news/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform/) の内容を元にしているため、まずはブログを読んで全体の流れを把握してからノートブックを実行していくのがおすすめです。\n",
    "\n",
    "### 参考URL\n",
    "\n",
    "Amazon SageMaker Python SDKのドキュメント    \n",
    "- https://sagemaker.readthedocs.io/en/stable/index.html\n",
    "\n",
    "SageMaker XGBoostのコンテナGithub\n",
    "- https://github.com/aws/sagemaker-xgboost-container\n",
    "\n",
    "英語のSageMakerサンプル集\n",
    "- https://github.com/aws/amazon-sagemaker-examples\n",
    "\n",
    "日本語のSageMakerサンプル集\n",
    "- https://github.com/aws-samples/aws-ml-jp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.セットアップ\n",
    "\n",
    "まず、このノートブックインスタンスに付与されている IAM role を `get_execution_role()` から取得しましょう。後ほど、SageMaker の学習やホスティングを行いますが、そこで IAM role が必要になります。そこで、ノートブックインスタンスの IAM role を、学習やホスティングでも利用します。\n",
    "通常、role を取得するためにはAWS SDKを利用した数行のコードを書く必要があります。ここでは `get_execution_role()` のみで role を取得可能です。SageMaker Python SDK は、データサイエンティストが機械学習以外のコードを簡潔に済ませるために、このような関数を提供しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade sagemaker 'pandas>=1.0.5' shap lightgbm　\n",
    "!{sys.executable} -m pip install --upgrade 'scikit-learn>=0.24.0,<1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket_name=sagemaker_session.default_bucket()\n",
    "user_name = 'demo' # 同一アカウント内で重複した名前を防ぐためのものです\n",
    "ver = 'v01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以降で利用するライブラリをここで読み込んでおきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "\n",
    "\n",
    "print('Current SageMaker Python SDK Version ={0}'.format(sagemaker.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルにディレクトリ作成\n",
    "SRC_DIR = f'./src_lgb'\n",
    "if not os.path.exists(SRC_DIR):\n",
    "    os.makedirs(SRC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.データ\n",
    "\n",
    "このNotebookではUCIリポジトリにある[Census-Income (KDD) Data Set](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = './input/'\n",
    "if not os.path.exists(INPUT_DIR):\n",
    "    os.makedirs(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのダウンロード\n",
    "s3 = boto3.client('s3')\n",
    "input_data = 's3://sagemaker-sample-data-{}/processing/census/census-income.csv'.format(region)\n",
    "!aws s3 cp $input_data ./input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込みとターゲット変数の変換\n",
    "df = pd.read_csv('./input/census-income.csv')\n",
    "print(df.income.value_counts())\n",
    "df['income'] = np.where(df['income'] == ' 50000+.', 1, 0) #ターゲット変数の(0, 1)変換\n",
    "print(df.income.value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データには94年と95年のものが含まれているため、94年は開発用、95年はテストおよび再学習・推論パイプライン作成に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_94 = df[df['year'] == 94].reset_index(drop=True)\n",
    "df_95 = df[df['year'] == 95].reset_index(drop=True)\n",
    "\n",
    "del df_94['year'], df_95['year']\n",
    "\n",
    "print(df_94.shape)\n",
    "print(df_94.income.value_counts())\n",
    "print(df_95.shape)\n",
    "print(df_95.income.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.EDA\n",
    "\n",
    "データの統計量や相関を確認します。\n",
    "\n",
    "**このセクションではSageMakerの機能は使用していません。pythonのライブラリを使用しています。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in df_94.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=df_94[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(df_94.describe())\n",
    "%matplotlib inline\n",
    "hist = df_94.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_94.corr())\n",
    "pd.plotting.scatter_matrix(df_94, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.データの加工とS3へのアップロード\n",
    "\n",
    "LightGBMのトレーニングジョブが適用できるようにデータを加工します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_95 = pd.concat([df_95['income'], df_95.drop(['income'], axis=1)], axis=1)\n",
    "df_95_raw = df_95.copy() # 再学習・推論パイプライン用\n",
    "df_95.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここで使用する学習用スクリプトはターゲット列の位置に制限はありませんが、取り回しのしやすさから0列目にしています\n",
    "df_94 = pd.concat([df_94['income'], df_94.drop(['income'], axis=1)], axis=1)\n",
    "df_94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  開発用データを学習・検証データへと分割します。\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df_94, test_size=0.1, shuffle=True, random_state=1, stratify=df_94.income)\n",
    "\n",
    "print(train_df.income.value_counts())\n",
    "print(val_df.income.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリー列の抽出\n",
    "categorical_columns = [x for x in df_94.columns if df_94[x].dtypes == 'object']\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリー列を数値へ変換します。\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder(\n",
    "    handle_unknown = 'use_encoded_value', # sklearn 0.24以降\n",
    "    unknown_value = -9999, # sklearn 0.24以降\n",
    ")\n",
    "\n",
    "# 欠損を文字列Noneへ変更\n",
    "train_df.loc[:, categorical_columns] = train_df[categorical_columns].fillna('None')\n",
    "val_df.loc[:, categorical_columns] = val_df[categorical_columns].fillna('None')\n",
    "df_95.loc[:, categorical_columns] = df_95[categorical_columns].fillna('None')\n",
    "\n",
    "enc.fit(train_df[categorical_columns])\n",
    "pkl.dump(enc, open('preprocessor', 'wb')) # 推論パイプライン用にdumpする\n",
    "#enc = pkl.load(open('preprocessor', 'rb'))\n",
    "\n",
    "train_df.loc[:, categorical_columns] = enc.transform(train_df[categorical_columns])\n",
    "val_df.loc[:, categorical_columns] = enc.transform(val_df[categorical_columns])\n",
    "df_95.loc[:, categorical_columns] = enc.transform(df_95[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class of worker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_95.head() # バッチ推論の動作確認用データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_95_raw.head() # 推論・再学習パイプライン用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データをローカル（Notebookインスタンス）に出力して、S3へアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルにディレクトリ作成\n",
    "DATASET_DIR = f'./dataset/{ver}/'\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    os.makedirs(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**このあとの 5.トレーニングジョブの実行 のところで作成する LightGBMの学習スクリプト（変更可能）がヘッダーありのファイルを入力とする**よう記述しているため、学習・検証用ファイルはヘッダありで書き出します。一方、（バッチ推論時のフィルタリングの取り回しの関係で）推論テスト用のファイルはヘッダなしでファイルを書き出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルにファイル書き出し\n",
    "train_df.to_csv(DATASET_DIR+'train.csv', header=True, index=False)\n",
    "val_df.to_csv(DATASET_DIR+'validation.csv', header=True, index=False)\n",
    "df_95.to_csv(DATASET_DIR+'test.csv', header=False, index=False) # バッチ推論テスト\n",
    "\n",
    "#df_95_raw.to_csv('new_train_data.csv', header=True, index=False) # 再学習パイプライン用\n",
    "#del df_95_raw['income']\n",
    "#df_95_raw.to_csv('20220415.csv', header=True, index=False) # 推論パイプライン用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3へのデータアップロード\n",
    "key = f'dataset/{ver}'\n",
    "train_prefix = 'train'\n",
    "val_prefix = 'validation'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(f'{key}/{train_prefix}.csv').upload_file(f'{key}/{train_prefix}.csv')\n",
    "s3_input_train = f's3://{bucket_name}/{key}/{train_prefix}.csv'\n",
    "print('Done writing to {}'.format(s3_input_train))\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(f'{key}/{val_prefix}.csv').upload_file(f'{key}/{val_prefix}.csv')\n",
    "s3_input_validation = f's3://{bucket_name}/{key}/{val_prefix}.csv'\n",
    "print('Done writing to {}'.format(s3_input_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データがcsv形式と想定して学習スクリプトを書いているので、S3へアップロードしたデータがcsv形式であると指定\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "content_type = 'text/csv'\n",
    "\n",
    "train_input = TrainingInput(s3_input_train, content_type=content_type)\n",
    "validation_input = TrainingInput(s3_input_validation, content_type=content_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.トレーニングジョブの実行\n",
    "\n",
    "以下のセルでスクリプトを書き出します。    \n",
    "`model_fn()`に加えて、`input_fn()`、`predict_fn()`がXGBoostのサンプルにはなかった関数となり、推論時に使用される関数です。    \n",
    "XGBoostの場合はデフォルト関数そのままで動作しますが、LightGBMでは動作しない部分もあるため、新たに定義しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src_lgb/lightgbm_train.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle as pkl\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# バッチ推論用関数\n",
    "def model_fn(model_dir):\n",
    "    '''モデルを読み込む関数'''\n",
    "    model_file = 'lgb-model'\n",
    "    booster = lgb.Booster(model_file=os.path.join(model_dir, model_file))\n",
    "    return booster\n",
    "\n",
    "\n",
    "# バッチ推論用関数\n",
    "def input_fn(input_data, content_type):\n",
    "    '''入力データを読み込む関数'''\n",
    "    if content_type == 'text/csv':\n",
    "        df = pd.read_csv(StringIO(input_data), header=None)\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "# バッチ推論用関数\n",
    "def predict_fn(input_data, model):\n",
    "    '''推論する関数'''\n",
    "    output = model.predict(input_data) # 入力データと結合する場合はヘッダあり、なしを入力データと統一する\n",
    "    return output\n",
    "\n",
    "\n",
    "# モデルの学習（estimator.fit()）をする際はここから実行されます\n",
    "# 学習データを読み込むパスを引数で受け取るようにすればローカルPCで LightGBM を実行する際と同等の記述ができます\n",
    "# 最後に学習済みモデルを指定されたパス（model_dir）に保存しています\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--model-name', type=str, default='lgb-model')\n",
    "    \n",
    "    # CustomFramework specific arguments.\n",
    "    parser.add_argument('--categorical', type=str, default=None)  # in this script we ask user to explicitly name categorical features\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "    \n",
    "    # LightGBM specific arguments. Defaults are set in LightGBM default values.\n",
    "    parser.add_argument('--device_type', type=str, default='cpu')\n",
    "    parser.add_argument('--seed', type=int, default=None)    \n",
    "    parser.add_argument('--objective', type=str, default='regression')\n",
    "    parser.add_argument('--metric', type=str, default='')\n",
    "    parser.add_argument('--boosting', type=str, default='gbdt')\n",
    "    parser.add_argument('--num-class', type=int, default=1)\n",
    "    parser.add_argument('--num-iterations', type=int, default=100)\n",
    "    parser.add_argument('--early-stopping-rounds', type=int, default=None)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    parser.add_argument('--max_depth', type=int, default=-1)\n",
    "    parser.add_argument('--num_leaves', type=int, default=31)\n",
    "    parser.add_argument('--max_bin', type=int, default=255)\n",
    "    parser.add_argument('--bin_construct_sample_cnt', type=int, default=200000)\n",
    "    parser.add_argument('--bagging_fraction', type=float, default=1.0)\n",
    "    parser.add_argument('--bagging_freq', type=int, default=0)\n",
    "    parser.add_argument('--feature_fraction', type=float, default=1.0)\n",
    "    parser.add_argument('--lambda_l1', type=float, default=0.0)\n",
    "    parser.add_argument('--lambda_l2', type=float, default=0.0)\n",
    "    parser.add_argument('--min_gain_to_split', type=float, default=0.0)\n",
    "    parser.add_argument('--min_sum_hessian_in_leaf', type=float, default=1e-3)\n",
    "    parser.add_argument('--min_data_in_leaf', type=int, default=20)\n",
    "    parser.add_argument('--scale_pos_weight', type=float, default=1.0)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    params = vars(args)\n",
    "    exclude_param = ['model_dir', 'train', 'validation', 'categorical', 'target', 'model_name']\n",
    "    params = {k : v for k, v in params.items() if k not in exclude_param}\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logging.info('Reading data and building dataset')\n",
    "    \n",
    "    if args.categorical != None:\n",
    "        categorical = args.categorical.split(', ')\n",
    "    else:\n",
    "        categorical = \"auto\"\n",
    "    logging.info('Set categorical_feature: {}'.format(categorical))\n",
    "    \n",
    "    # read train data\n",
    "    input_files = [os.path.join(args.train, file) for file in os.listdir(args.train)]\n",
    "    raw_data = [pd.read_csv(file, engine=\"python\") for file in input_files]\n",
    "    train_df = pd.concat(raw_data)\n",
    "    X_train = train_df[[x for x in train_df.columns if x not in [args.target]]]\n",
    "    y_train = train_df[args.target]\n",
    "    d_train = lgb.Dataset(X_train, label=y_train, feature_name='auto', categorical_feature=categorical)\n",
    "    \n",
    "    # read validation data\n",
    "    input_files = [os.path.join(args.validation, file) for file in os.listdir(args.validation)]\n",
    "    if len(input_files) != 0:\n",
    "        raw_data = [pd.read_csv(file, engine=\"python\") for file in input_files]\n",
    "        val_df = pd.concat(raw_data)\n",
    "        X_val = val_df[[x for x in val_df.columns if x not in [args.target]]]\n",
    "        y_val = val_df[args.target]\n",
    "        d_val = lgb.Dataset(X_val, label=y_val, reference=d_train, feature_name='auto', categorical_feature=categorical)\n",
    "    else:\n",
    "        d_val = None\n",
    "    \n",
    "    valid_sets = [d_train, d_val] if d_val is not None else [d_train]\n",
    "    valid_names = ['train', 'valid'] if d_val is not None else ['train']\n",
    "        \n",
    "    logging.info('Run LightGBM')\n",
    "    \n",
    "    # train LightGBM\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      valid_sets=valid_sets,\n",
    "                      valid_names=valid_names,\n",
    "                      fobj=None,\n",
    "                      feval=None,\n",
    "                      init_model=None,\n",
    "                      evals_result=None,\n",
    "                      verbose_eval=True,\n",
    "                      keep_training_booster=False,\n",
    "                      callbacks=None,\n",
    "                     )\n",
    "    \n",
    "    # Save model\n",
    "    output_path = os.path.join(args.model_dir, args.model_name)\n",
    "    model.save_model(output_path, num_iteration=model.best_iteration)\n",
    "    logging.info(\"Stored trained model at {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src_lgb/requirements.txt\n",
    "lightgbm==3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、XGBoostのコンテナを利用してLightGBMを実行します。    \n",
    "以下に、カスタムコンテナを作成するサンプルもありますが、XGBoostコンテナを使用する方法が最も簡単です。\n",
    "\n",
    "\n",
    "Amazon SageMaker におけるカスタムコンテナ実装パターン詳説 〜学習編〜\n",
    "- https://aws.amazon.com/jp/blogs/news/sagemaker-custom-containers-pattern-training/\n",
    "\n",
    "BYO Container Example: lightGBM    \n",
    "- https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/lightgbm-byo/lightgbm-byo.ipynb\n",
    "\n",
    "custom-training-containers    \n",
    "- https://github.com/aws/amazon-sagemaker-examples/tree/main/advanced_functionality/custom-training-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "output_path = 's3://{}/{}/{}'.format(bucket_name, 'model', ver)\n",
    "print(output_path)\n",
    "\n",
    "lgb_estimator = XGBoost(\n",
    "    entry_point='lightgbm_train.py',\n",
    "    source_dir='src_lgb', # このdirのrequirements.txtに追加したいライプラリ（LightGBM）を記載\n",
    "    framework_version='1.3-1',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge', # データのサイズやアルゴリズムによって変更する\n",
    "    instance_count=1, \n",
    "    base_job_name=f'lightgbm-training-{user_name}',\n",
    "    code_location=output_path,\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        'target': 'income',\n",
    "        'categorical': ', '.join(categorical_columns),\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.モデルの解釈\n",
    "\n",
    "ここでは [SHAP](https://github.com/slundberg/shap) を利用して、SageMaker で学習したモデルの解釈・分析を行います。\n",
    "\n",
    "**このセクションではSageMakerの機能は使用していません。pythonのライブラリを使用しています。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "S3Downloader.download(\n",
    "    s3_uri=lgb_estimator.model_data, \n",
    "    local_path=MODEL_DIR, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "# OUTPUT_DIRに解凍します\n",
    "!tar -zxvf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ（train_df）の場合、m5.xlargeで40min程度\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "\n",
    "model_file = 'lgb-model'\n",
    "booster = lgb.Booster(model_file=os.path.join(MODEL_DIR, model_file))\n",
    "booster.params[\"objective\"] = \"binary:logistic\"\n",
    "\n",
    "explainer = shap.TreeExplainer(booster)\n",
    "#shap_values = explainer.shap_values(train_df.iloc[:, 1:])\n",
    "shap_values = explainer.shap_values(val_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values, train_df.iloc[:, 1:])\n",
    "shap.summary_plot(shap_values, val_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.バッチ推論\n",
    "\n",
    "SageMakerのバッチ変換ジョブを使用してバッチ推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = f'dataset/{ver}'\n",
    "test_prefix = 'test'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(f'{key}/{test_prefix}.csv').upload_file(f'{key}/{test_prefix}.csv')\n",
    "s3_input_test = f's3://{bucket_name}/{key}/{test_prefix}.csv'\n",
    "print('Done writing to {}'.format(s3_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location to store the results of the batch transform job\n",
    "batch_output_path = 's3://{}/{}/{}/{}'.format(bucket_name, 'batch-inference', ver, 'test')\n",
    "print(batch_output_path)\n",
    "model_name=f'lightgbm-{user_name}-{ver}'\n",
    "print(model_name)\n",
    "\n",
    "transformer = lgb_estimator.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    assemble_with = 'Line', \n",
    "    accept = 'text/csv',\n",
    "    output_path=batch_output_path,\n",
    "    max_payload=30,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a transform job\n",
    "transformer.transform(\n",
    "    data=s3_input_test, \n",
    "    split_type='Line', \n",
    "    content_type='text/csv',\n",
    "    input_filter='$[1:]', # この例ではIncome列を除く（入力ファイルにID列など推論に不要となる列がある時に利用できます）\n",
    "    join_source='Input', \n",
    "    output_filter='$[0, -1]', # label, predict\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論結果のダウンロード\n",
    "S3Downloader.download(\n",
    "    s3_uri=batch_output_path, \n",
    "    local_path='.', \n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_csv('test.csv.out', sep=\",\", header=None)\n",
    "output_df.head(-10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('Scikit-learn AUC: ', roc_auc_score(output_df[0], output_df[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. モデルの評価、閾値の調整\n",
    "\n",
    "「7.モデルの解釈」でダウンロードした学習済みモデルを使って性能評価値の確認とより良い閾値の探索を行います。\n",
    "\n",
    "テスト用データを読み込んで推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f'{key}/{test_prefix}.csv', header=None).drop(0, axis=1)\n",
    "test_data_y = pd.read_csv(f'{key}/{test_prefix}.csv', header=None)[0]\n",
    "output = booster.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Scikit-learn AUC: ', roc_auc_score(test_data_y, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混同行列を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_data_y, output>0.2)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "閾値を 0.5 に設定して各種メトリクスの値を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_data_y, output>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[こちらのサンプルノートブック](https://github.com/aws-samples/aws-ml-jp/blob/main/sagemaker/xgboost-customer-churn/xgboost_customer_churn.ipynb) の「5-3.最適な閾値を探す」の方法で最適な閾値を探してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "\n",
    "for c in cutoffs:\n",
    "    _predictions = pd.Categorical(np.where(output > c, 1, 0), categories=[0, 1])\n",
    "    matrix_a = np.array([[0, 100], [500, 100]])\n",
    "    matrix_b = pd.crosstab(index=test_data_y, columns=_predictions, dropna=False)\n",
    "    costs.append(np.sum(np.sum(matrix_a * matrix_b)))\n",
    "\n",
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)], 'for a cost of:', np.min(costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
