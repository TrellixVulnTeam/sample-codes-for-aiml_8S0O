{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb660254",
   "metadata": {},
   "source": [
    "# Amazon SageMaker scikit-learn コンテナを使って NearestNeighbors の学習、推論を行う\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "このサンプルノートブックでは、SageMaker が用意した scikit-learn のコンテナを使って NearestNeighbors の学習と、学習したモデルを使った推論を行います。\n",
    "\n",
    "まず初めに、SageMaker Python SDK を使ってジョブを起動する方法を紹介します。その後同様の操作を boto3 を使って行う方法を紹介します。\n",
    "\n",
    "---\n",
    "## セットアップ\n",
    "\n",
    "必要なライブラリやパラメタを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "project_name = 'sklearn-byo-script'\n",
    "user_name = 'demo2'\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = f\"sagemaker/{project_name}\"\n",
    "\n",
    "print(f\"bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce38fb",
   "metadata": {},
   "source": [
    "## データの準備\n",
    "\n",
    "このサンプルノートブックでは scikit-learn が用意したデータセットを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205bf94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191057a",
   "metadata": {},
   "source": [
    "取得したデータには特徴量、ターゲット変数、メタデータなどが含まれるので、特徴量部分のみを抜き出して CSV ファイルにしてから S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data['data']).to_csv('wine.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='wine.csv', bucket=bucket, key_prefix=f'{project_name}/{user_name}')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c33d6",
   "metadata": {},
   "source": [
    "## SageMaker 学習ジョブを使って knn を学習する \n",
    "\n",
    "code/train ディレクトリの中の train.py を使って knn を学習します。code ディレクトリの中に requirements.txt があると、学習インスタンス起動時に SageMaker が自動的にそこに書かれたライブラリを pip install してくれます。\n",
    "\n",
    "SageMaker Job は全てのジョブ名がユニークである必要があります。Scikit-learn 用の Estimator である SKLearn を作成する際に、パラメタ base_job_name にprefix を指定しておくと、SageMaker が自動的にタイムスタンプを付与します。フルでジョブ名を指定したい場合は fit() のパラメタ job_name を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed76081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'train.py'\n",
    "\n",
    "# run the Scikit-Learn script\n",
    "sklearn = SKLearn(\n",
    "    base_job_name=project_name+'-'+user_name,\n",
    "    entry_point=script_path,\n",
    "    source_dir='code/train',\n",
    "    train_instance_type=\"ml.m5.large\",\n",
    "    role=role,\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        'n_neighbors': 2,\n",
    "        'radius': 0.4\n",
    "#         'metric': 'cosine'\n",
    "    })\n",
    "    \n",
    "sklearn.fit({'train':inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ecfcbb",
   "metadata": {},
   "source": [
    "## 学習したモデルを使ってバッチ推論を実行する\n",
    "\n",
    "SageMaker Processing を使ってバッチ推論を実行します。SageMaker Processing は、任意のコンテナとスクリプトを使ってジョブを柔軟に実行できる機能です。実行するスクリプトはオンプレミスなどのローカル環境で動作するものをほぼそのまま利用できます。[SageMaker バッチ変換](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/batch-transform.html) の機能でもバッチ推論は可能ですが、こちらは [SageMaker の仕様](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#serve-a-model) に則って推論スクリプトを書く必要があります。\n",
    "\n",
    "以下のセルを実行して SageMaker が用意している Scikit-learn のコンテナイメージの URI を取得します。SageMaker のビルトインコンテナイメージは AWS が公開している Amazon ECR リポジトリに保存されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "inference_repository_uri = retrieve(\n",
    "    framework='sklearn',\n",
    "    region=region,\n",
    "    version='1.0-1',\n",
    "    py_version='py3',\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    \n",
    ")\n",
    "inference_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d1175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "inference_job_name = project_name + '-' + user_name + '-' + timestamp\n",
    "inference_input_data = inputs\n",
    "inference_model = sklearn.model_data\n",
    "inference_output_data = f's3://{bucket}/{prefix}/inference/{inference_job_name}'\n",
    "code_path = '/opt/ml/processing/input/code'\n",
    "input_dir = '/opt/ml/processing/input/data'\n",
    "model_dir =  '/opt/ml/processing/input/model'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "SCRIPT_LOCATION = \"code/inference\"\n",
    "\n",
    "code_s3_path = sagemaker_session.upload_data(\n",
    "    SCRIPT_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, timestamp),\n",
    ")\n",
    "\n",
    "# 入力データの設定\n",
    "# バッチ推論をするために必要なソースコード、データ、モデルをインスタンスで使用するためのパスの設定\n",
    "# source で指定した S3 パスから destination で指定したローカルパスに自動的にァイルがダウンロードされる\n",
    "inference_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=code_s3_path,\n",
    "        destination=code_path\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "        input_name=\"data\",\n",
    "        source=inference_input_data,\n",
    "        destination=input_dir\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "        input_name=\"model\",\n",
    "        source=inference_model,\n",
    "        destination=model_dir\n",
    "    )\n",
    "]\n",
    "\n",
    "# 出力データの設定\n",
    "# 推論結果を保存するパスに関する設定\n",
    "# source で指定したローカルパスから destination で指定した S3 パスに自動的にファイルがアップロードされる\n",
    "inference_outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=\"result\",\n",
    "        source=output_dir,\n",
    "        destination=inference_output_data,\n",
    "    )\n",
    "]\n",
    "\n",
    "inference_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=inference_repository_uri,\n",
    "        entrypoint=[\"python3\", f\"{code_path}/inference.py\"],\n",
    "        instance_count=1, \n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=16,\n",
    "        volume_kms_key=None,\n",
    "        output_kms_key=None,\n",
    "        max_runtime_in_seconds=86400,  # default is 24 hours(60*60*24)\n",
    "        sagemaker_session=None,\n",
    "        env=None,\n",
    "        network_config=None\n",
    "    )\n",
    "\n",
    "inference_processor.run(\n",
    "    job_name=inference_job_name,\n",
    "    inputs=inference_inputs,\n",
    "     outputs=inference_outputs,\n",
    "    arguments=['--n_neighbors', '2'],\n",
    "    logs=False,\n",
    "    wait=False\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://s3.console.aws.amazon.com/s3/buckets/{bucket}?region={region}&prefix={prefix}/inference/{inference_job_name}/&showversions=false\\\" target=\\\"_blank\\\">推論結果 (S3)</a>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89e883",
   "metadata": {},
   "source": [
    "## boto3 を使ってジョブを実行する\n",
    "\n",
    "ここまでは、SageMaker Python SDK を使って Training Job, Processing Job を実行しました。SageMaker Python SDK は非常に便利ですが、boto3 のみしか使えない環境では boto3 を使って同様のことが可能です。SageMaker Python SDK がよしなにやっていた部分を明示的に記述する必要がありますが、インフラ部分の細かな設定などは boto3 からのみ実行することができます。\n",
    "\n",
    "### モデルの学習\n",
    "\n",
    "学習ジョブで使用するソースコードは sourcedir.tar.gz という名前の tar.gz ファイルに圧縮して S3 にアップロードします。圧縮ファイル直下にファイルが配置されるようにしてください（ソースコードが入ったフォルダごと圧縮しない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_LOCATION = \"code/train\"\n",
    "TRAINNING_SCRIPT_LOCATION = \"sourcedir.tar.gz\"\n",
    "!cd $SCRIPT_LOCATION && tar zcvf ../../$TRAINNING_SCRIPT_LOCATION ./*\n",
    "\n",
    "train_code_s3_path = sagemaker_session.upload_data(\n",
    "    TRAINNING_SCRIPT_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(prefix, SCRIPT_LOCATION, timestamp),\n",
    ")\n",
    "train_code_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a32db",
   "metadata": {},
   "source": [
    "create_training_job API を使って学習ジョブを実行します。ジョブの完了を待たずレスポンスが返ってきます。ジョブの完了は S3 バケットへの model.tar.gz の PutObject イベントで把握できます。ジョブ実行時に設定可能なパラメタについては [こちらのドキュメント](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job) を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "training_job_name = project_name + '-' + user_name + '-' + timestamp\n",
    "output_location = f's3://{bucket}/{prefix}/train'\n",
    "\n",
    "create_training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": inference_repository_uri, \"TrainingInputMode\": \"File\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": output_location},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.c4.xlarge\", \"VolumeSizeInGB\": 50},\n",
    "    \"TrainingJobName\": training_job_name,\n",
    "    \"HyperParameters\": {\n",
    "        'n_neighbors': \"2\",\n",
    "        'radius': \"0.4\",\n",
    "        'sagemaker_program' : \"train.py\",\n",
    "        'sagemaker_submit_directory': train_code_s3_path\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60 * 60},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": inputs,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "sagemaker_client.create_training_job(**create_training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a0d64",
   "metadata": {},
   "source": [
    "### バッチ推論\n",
    "\n",
    "create_processing_job API も create_training_job API と同様に実行後すぐにレスポンスが返ってきます。ジョブ実行時に設定可能なパラメタについては [こちらのドキュメント](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_processing_job) を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "inference_job_name = project_name + '-' + user_name + '-' + timestamp\n",
    "inference_input_data = inputs\n",
    "inference_model = f'{output_location}/{training_job_name}/output/model.tar.gz'\n",
    "inference_output_data = f's3://{bucket}/{prefix}/inference/{inference_job_name}'\n",
    "\n",
    "sagemaker_client.create_processing_job(\n",
    "    ProcessingInputs=[\n",
    "        {\n",
    "            'InputName': 'code',\n",
    "            'S3Input': {\n",
    "                'S3Uri': code_s3_path,\n",
    "                'LocalPath': code_path,\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3InputMode': 'File',\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            'InputName': 'data',\n",
    "            'S3Input': {\n",
    "                'S3Uri': inference_input_data,\n",
    "                'LocalPath': input_dir,\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3InputMode': 'File'\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            'InputName': 'model',\n",
    "            'S3Input': {\n",
    "                'S3Uri': inference_model,\n",
    "                'LocalPath': model_dir,\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3InputMode': 'File'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    ProcessingOutputConfig={\n",
    "        'Outputs': [\n",
    "            {\n",
    "                'OutputName': 'result',\n",
    "                'S3Output': {\n",
    "                    'S3Uri': inference_output_data,\n",
    "                    'LocalPath': output_dir,\n",
    "                    'S3UploadMode': 'EndOfJob'\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    ProcessingJobName=inference_job_name,\n",
    "    ProcessingResources={\n",
    "        'ClusterConfig': {\n",
    "            'InstanceCount': 1,\n",
    "            'InstanceType': 'ml.m5.xlarge',\n",
    "            'VolumeSizeInGB': 16,\n",
    "        }\n",
    "    },\n",
    "    AppSpecification={\n",
    "        'ImageUri': inference_repository_uri,\n",
    "        'ContainerEntrypoint': [\n",
    "            'python3',\n",
    "            f'{code_path}/inference.py',\n",
    "        ],\n",
    "        'ContainerArguments': [\n",
    "            '--n_neighbors', '2'\n",
    "        ]\n",
    "    },\n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a8bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
