{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce003b49",
   "metadata": {},
   "source": [
    "# Semantic Segmentation 自動ラベリング検証\n",
    "\n",
    "このノートブックは、ラベリング済みのデータセットを使って自動ラベリングによってどの程度工数が削減できるかを検証するためのサンプルノートブックです。検証手順は以下の通りです。\n",
    "\n",
    "1. Amazon SageMaker のビルトインアルゴリズムを使って、用意したラベルありデータセットから任意の数を取り出しモデルの学習を実施\n",
    "1. 作成したモデルを使って、残りのデータ（未ラベリングデータ扱い）に対して推論を行う（自動ラベリングに相当）\n",
    "1. 自動ラベリングの結果、評価値が閾値より高ければラベル済みデータとして扱う\n",
    "1. 自動ラベリングの結果、評価値が閾値より高いデータをラベリング済みデータリストに追加。このとき自動ラベリングで作成したラベル画像をモデルの学習に使用するよう変更（あらかじめ用意したラベル画像と差し替える）\n",
    "1. 評価値が高いデータの数が少なければ、未ラベリングデータの中から不足分だけ未ラベリングリストからラベリング済みリストに追加（手動ラベリング相当の処理）\n",
    "1. 自動ラベリングの結果、評価値が低いものは未ラベリングリストに戻す\n",
    "1. 増えたラベリング済みデータセットを使ってモデルを学習\n",
    "1. 以降、未ラベリングのデータがなくなるまで繰り返す\n",
    "\n",
    "Amazon SageMaker の Semantic Segmentation の詳細の使い方については [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html#semantic-segmentation-inputoutput) をご参照ください。\n",
    "\n",
    "## Amazon SageMaker のセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import parse_s3_url\n",
    "\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "ecr_client = boto3.client('ecr', region_name=region)\n",
    "\n",
    "prefix = 'auto-labeling-test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cc93a",
   "metadata": {},
   "source": [
    "### SageMaker ノートブックインスタンスの IAM ロールに権限の追加\n",
    "ノートブックインスタンスで使用している IAM ロールに、以下のポリシーをアタッチしてください。\n",
    "\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "\n",
    "## データセットの準備\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) を使用します。 データセットの詳細は [Pascal VOC Dataset page](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html)をご参照ください。分類クラスは以下の通りです。\n",
    "\n",
    "| Label Id |     Class     |\n",
    "|:--------:|:-------------:|\n",
    "|     0    |   Background  |\n",
    "|     1    |   Aeroplane   |\n",
    "|     2    |    Bicycle    |\n",
    "|     3    |      Bird     |\n",
    "|     4    |      Boat     |\n",
    "|    5     |     Bottle    |\n",
    "|     6    |      Bus      |\n",
    "|     7    |      Car      |\n",
    "|     8    |      Cat      |\n",
    "|     9    |     Chair     |\n",
    "|    10    |      Cow      |\n",
    "|    11    |  Dining Table |\n",
    "|    12    |      Dog      |\n",
    "|    13    |     Horse     |\n",
    "|    14    |   Motorbike   |\n",
    "|    15    |     Person    |\n",
    "|    16    |  Potted Plant |\n",
    "|    17    |     Sheep     |\n",
    "|    18    |      Sofa     |\n",
    "|    19    |     Train     |\n",
    "|    20    |  TV / Monitor |\n",
    "|    255   | Hole / Ignore |\n",
    "\n",
    "入力画像とラベル画像をそれぞれ data/image と data/annotation に保存し、Amazon S3 にアップロードします。また、SageMaker 学習ジョブの入力として使用するため、入力画像とラベル画像の対応を記載した manifest ファイルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Downloading the dataset...\")\n",
    "!wget -P /tmp https://fast-ai-imagelocal.s3.amazonaws.com/pascal-voc.tgz\n",
    "# S3 cp may be even faster on environments where it's available:\n",
    "# !aws s3 cp s3://fast-ai-imagelocal/pascal-voc.tgz /tmp/pascal-voc.tgz\n",
    "\n",
    "print(\"Extracting VOC2012...\")\n",
    "!tar -xf /tmp/pascal-voc.tgz --wildcards pascal-voc/VOC2012*\n",
    "\n",
    "print(\"Deleting /tmp files...\")\n",
    "!rm /tmp/pascal-voc.tgz\n",
    "\n",
    "# Alternatively could consider using the Oxford Uni source:\n",
    "#!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "#!tar -xf /tmp/VOCtrainval_11-May-2012.tar -C pascal-voc/VOC2012\n",
    "#!rm /tmp/VOCtrainval_11-May-2012.tar\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90083d9a",
   "metadata": {},
   "source": [
    "以下のセルでは、画像を data フォルダ以下に集約しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directory structure mimicing the s3 bucket where data is to be dumped.\n",
    "VOC2012 = \"pascal-voc/VOC2012\"\n",
    "os.makedirs(\"data/image\", exist_ok=True)\n",
    "os.makedirs(\"data/annotation\", exist_ok=True)\n",
    "\n",
    "# Create a list of all validation images.\n",
    "# 画像1450枚くらい\n",
    "with open(VOC2012 + \"/ImageSets/Segmentation/val.txt\") as f:\n",
    "    val_list = f.read().splitlines()\n",
    "\n",
    "# Move the jpg images in validation list to validation directory and png images to validation_annotation directory.\n",
    "for i in val_list:\n",
    "    shutil.copy2(VOC2012 + \"/JPEGImages/\" + i + \".jpg\", \"data/image/\")\n",
    "    shutil.copy2(VOC2012 + \"/SegmentationClass/\" + i + \".png\", \"data/annotation/\")\n",
    "    \n",
    "# すべてのデータセットを使用するには以下をコメントアウトを解除してからこのセルを実行する\n",
    "# # Create a list of all validation images.\n",
    "# with open(VOC2012 + \"/ImageSets/Segmentation/train.txt\") as f:\n",
    "#     val_list = f.read().splitlines()\n",
    "\n",
    "# # Move the jpg images in validation list to validation directory and png images to validation_annotation directory.\n",
    "# for i in val_list:\n",
    "#     shutil.copy2(VOC2012 + \"/JPEGImages/\" + i + \".jpg\", \"data/image/\")\n",
    "#     shutil.copy2(VOC2012 + \"/SegmentationClass/\" + i + \".png\", \"data/annotation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400b746",
   "metadata": {},
   "source": [
    "以下のセルでは、manifest ファイルを作成しています。100MB ほどの画像データを S3 にアップロードするため実行が完了するまでに少し時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3232b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "gt_job_name = 'semaseg'\n",
    "train_manifest_filename = 'train.manifest'\n",
    "train_file_list = glob.glob('data/image/*.jpg')\n",
    "num_training_samples = len(train_file_list)\n",
    "\n",
    "train_annotation_channel = sess.upload_data(\n",
    "    path=\"data\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(prefix, \"train-data\"),\n",
    ")\n",
    "\n",
    "with open(train_manifest_filename, 'w') as f:\n",
    "    for file in train_file_list:\n",
    "        file = os.path.basename(file)\n",
    "        train_data_path = os.path.join(train_annotation_channel, 'image', file)\n",
    "        train_annot_data_path = os.path.join(train_annotation_channel, 'annotation', file[:-3] + 'png')\n",
    "        manifest_format = '{\"source-ref\":\"' + train_data_path + '\",\"' + gt_job_name + '-ref\":\"'+ train_annot_data_path +'\",\"'+ gt_job_name +'-ref-metadata\":{\"job-name\":\"'+ gt_job_name +'\"}}'\n",
    "        f.write(manifest_format + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c257f87",
   "metadata": {},
   "source": [
    "## スクリプト実行用コンテナイメージ作成\n",
    "\n",
    "検証用スクリプトの実行は、実行時間が長時間になることを想定してノートブックインスタンス上ではなく、Amazon SageMaker Processing を使って実現します。スクリプトが動作する環境のコンテナイメージを作成し、Amazon ECR に push します。\n",
    "\n",
    "ECR にコンテナイメージをプッシュする権限を得るために、ノートブックインスタンスにアタッチしている IAM role に、`AmazonEC2ContainerRegistryPowerUser` policy を追加してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/requirements.txt\n",
    "\n",
    "boto3==1.20.30\n",
    "mxnet==1.8.0\n",
    "numpy==1.18.5\n",
    "pillow==8.4.0\n",
    "sagemaker==2.72.2\n",
    "scikit-learn==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99655cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join( 'docker/Dockerfile')\n",
    "\n",
    "build_config={f\"\"\"FROM python:3.7-slim-buster\n",
    "    \n",
    "ENV AWS_DEFAULT_REGION={region}\n",
    "\n",
    "RUN apt update \\\n",
    "  && apt install -y libopenblas-dev libgomp1 \\\n",
    "  && apt-get clean \\\n",
    "  && rm -rf /var/lib/apt/lists/*\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install -r requirements.txt\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    f.write('\\n'.join(list(build_config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d91be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag = ':latest'\n",
    "ecr_repository = prefix\n",
    "image_uri = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repository+tag}'\n",
    "\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    " \n",
    "# リポジトリの作成\n",
    "# すでにある場合はこのコマンドは必要ない\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    " \n",
    "!docker build -t $ecr_repository docker\n",
    "!docker tag {ecr_repository + tag} $image_uri\n",
    "!docker push $image_uri\n",
    "\n",
    "print(f'Image has been pushed to {image_uri}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07fdac1",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Processing でスクリプト実行\n",
    "\n",
    "検証スクリプト `auto-labeling-test.py` を SageMaker Processing で実行します。このノートブックと同じパスに `auto-labeling-test.py` がある想定です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_input_dir = '/opt/ml/processing/input'\n",
    "processing_output_dir = '/opt/ml/processing/output'\n",
    "job_name = prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "EVALUATION_SCRIPT_LOCATION = \"auto-labeling-test.py\"\n",
    "SAMPLE_PNG_LOCATION = 'pascal-voc/VOC2012/SegmentationClass/2008_003701.png'\n",
    "MANIFEST_LOCATION = train_manifest_filename\n",
    "\n",
    "sample_label_png = sess.upload_data(\n",
    "    SAMPLE_PNG_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(prefix, \"input\", timestamp),\n",
    ")\n",
    "print('sample_label_png:', sample_label_png)\n",
    "\n",
    "manifest_file = sess.upload_data(\n",
    "    MANIFEST_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(prefix, \"input\", timestamp),\n",
    ")\n",
    "print('manifest_file:', manifest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ScriptProcessor(base_job_name=job_name,\n",
    "           image_uri=image_uri,\n",
    "           command=['python3'],\n",
    "           role=role,\n",
    "           instance_count=1,\n",
    "           instance_type='ml.c5.xlarge'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "timestamp = datetime.now(JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "proc_output_bucket = bucket\n",
    "proc_output_prefix = job_name\n",
    "project_name = 'pascal-dataset'\n",
    "proc_output_s3_path = f's3://{proc_output_bucket}/{proc_output_prefix}/{project_name}/{timestamp}'\n",
    "\n",
    "base_model_path = 's3://xxx/output/model.tar.gz'\n",
    "\n",
    "processor.run(\n",
    "            job_name='autolabel-' + project_name + '-' + timestamp,\n",
    "            code=EVALUATION_SCRIPT_LOCATION, # S3 の URI でも可\n",
    "             inputs=[\n",
    "                     ProcessingInput(input_name='input',\n",
    "                                     source=os.path.dirname(sample_label_png),\n",
    "                                     destination=processing_input_dir)],\n",
    "             outputs=[ProcessingOutput(output_name='output',\n",
    "                                       source=processing_output_dir,\n",
    "                                      destination=proc_output_s3_path)],\n",
    "              arguments=[\n",
    "                  '--sample-png', os.path.basename(sample_label_png),\n",
    "                  '--manifest-file', os.path.basename(manifest_file),\n",
    "                  '--bucket-name', proc_output_bucket,\n",
    "                  '--data-output-path', proc_output_s3_path,\n",
    "                  '--role-arn', role,\n",
    "                  '--class-num', '21',  # ラベルの分類クラス数\n",
    "                  '--background-class-id', '0',  # 背景に割り当てられたラベルID\n",
    "                  '--project-name', project_name,  # 検証結果を保存する S3 パスを作成するために使用\n",
    "                  '--confidence-thresh', '0.98',  # 自動ラベリング結果で最低限満たしてほしい画素単位の確信度\n",
    "                  '--total-confidence-thresh', '0.85',  # confidence-threshを超える画素が何割あれば採用するかを決める閾値\n",
    "                  '--num-manual-target', '1000',  # モデル再学習に必要な追加データセット数\n",
    "                  '--train_ratio', '0.8',  # データセットにおける学習データの割合。残りは検証データ\n",
    "                  '--timestamp', timestamp,\n",
    "#                   '--base-model-path', base_model_path,  # 追加学習をする場合はベースモデルの model.tar.gz が保存されている S3 パスを指定\n",
    "                  '--gt-job-name', gt_job_name,\n",
    "                  '--train-instance-type', 'ml.p3.2xlarge',\n",
    "                  '--input-dir', processing_input_dir,\n",
    "                  '--output-dir', processing_output_dir\n",
    "              ],\n",
    "              wait=False\n",
    "            )\n",
    "print('---------------------------------')\n",
    "print('Results will be saved here:', proc_output_s3_path)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://s3.console.aws.amazon.com/s3/buckets/{bucket}?region={region}&prefix={prefix}/{project_name}/{timestamp}/&showversions=false\\\" target=\\\"_blank\\\">クリックしてデータ保存場所に飛ぶ</a>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be774b17",
   "metadata": {},
   "source": [
    "自動ラベリング検証ジョブが開始しました。このノートブックの構成の場合、ジョブの環境までに 2時間ほどかかります。ジョブの状況は SageMaker コンソールの左側のメニューにある[処理中]->[ジョブの処理]から確認できます。\n",
    "\n",
    "学習ジョブの入力に使用する manifest ファイル、自動ラベリング（バッチ推論）の出力などは上記セル実行時に出力されたパスに出力されます。このパス以下にイテレーション数を示す数字のフォルダが作成され、その中にそのイテレーションで生成されたファイルが保存されます。また以下のパス直下にある png フォルダの中には、自動ラベリングによって作成されたラベル画像が保存されます。\n",
    "\n",
    "```bash\n",
    "root \n",
    "|-0/\n",
    "  |-autolabel: バッチ推論の入出力ファイル\n",
    "  |-train: 学習ジョブの入出力ファイル（学習済みモデルのパスは学習ジョブが生成したパスに保存される）\n",
    "  |-updated-list: イテレーション官僚時点でのラベルあり画像となし画像のリスト\n",
    "|-1/\n",
    "|-2/\n",
    "|-png/: 自動ラベリングによって作成された PNG 画像\n",
    "|-report.txt: 何枚自動ラベリングされたかなどが記載されたレポート\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78852b74",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "このノートブックで作成したリソースを削除します。他に、このノートブックを実行したノートブックインスタンスやデータを保存した Amazon S3 バケットも不要であれば削除してください。\n",
    "\n",
    "### ECR リポジトリの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc44416",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_list = [\n",
    "    ecr_repository\n",
    "]\n",
    "for i in container_image_list:\n",
    "    try:\n",
    "        ecr_client.delete_repository(\n",
    "            repositoryName=i,\n",
    "            force=True\n",
    "        )\n",
    "        print('Delete:', i)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4f66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
