{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Step Functions Data Science SDK で Amazon SageMaker Training Job を並列実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [背景](#1.-背景)\n",
    "1. [セットアップ](#2.-セットアップ)\n",
    "1. [データ](#3.-データ)\n",
    "1. [SageMaker Estimator の準備](#4.-SageMaker-Estimator-の準備)\n",
    "1. [AWS Step Functions の準備](#5.-AWS-Step-Functions-の準備)\n",
    "1. [リソースの削除](#6.-リソースの削除)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 背景\n",
    "\n",
    "AWS Step Functions は、機械学習パイプラインの構築でよく使われます。AWS Step Functions Data Science SDK を使うと、Python でパイプラインを作ることができるため、データサイエンティストが自身のユースケースに最適な構成を簡単に構築できます。Step Functions を使った基本的なパイプライン構築方法については [こちらのサンプルノートブック](https://github.com/aws-samples/aws-ml-jp/blob/main/mlops/step-functions-data-science-sdk/model-train-evaluate-compare/step_functions_mlworkflow_scikit_learn_data_processing_and_model_evaluation_with_experiments.ipynb) をご参照ください。 \n",
    "\n",
    "本ノートブックは、**学習コードは共通で良いが複数の学習データを使ってそれぞれのモデルを学習させたい** ユースケースにピッタリなサンプルノートブックです。モデル学習の並列実行に Step Functions の [Map State](https://docs.aws.amazon.com/ja_jp/step-functions/latest/dg/amazon-states-language-map-state.html) を使用します。サンプルデータとしては MNIST を使用します。MNISTは、手書き文字の分類に広く使用されているデータセットです。 70,000個のラベル付きの28x28ピクセルの手書き数字のグレースケール画像で構成されています。 データセットは、60,000個のトレーニング画像と10,000個のテスト画像に分割されます。 手書きの数字 0から9の合計10のクラスがあります。 \n",
    "\n",
    "<img src=\"workflow.png\" width=\"50%\">\n",
    "\n",
    "学習スクリプトでは PyTorch を使用しています。SageMaker の PyTorch の詳細については、[sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) と [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) のレポジトリをご参照ください。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. セットアップ\n",
    "\n",
    "SageMaker セッションを作成し、設定を開始します。\n",
    "\n",
    "- 学習およびモデルデータに使用する S3 バケットとプレフィックスは、ノートブックインスタンス、トレーニング、およびホスティングと同じリージョン内にある必要があります。\n",
    "- データへの学習およびホスティングアクセスを提供するために使用される IAM ロール arn を用います。 ノートブックインスタンス、学習インスタンス、および/またはホスティングインスタンスに複数のロールが必要な場合は、 `sagemaker.get_execution_role（）` を、適切な IAM ロール arn 文字列に置き換えてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "project_name = 'sagemaker-parallel-training-map'\n",
    "user_name = 'demo'\n",
    "sagemaker_policy_name = project_name + '-' + user_name + '-policy'\n",
    "prefix = f'sagemaker/{project_name}/{user_name}'\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "text = f\"\"\"\n",
    "以下の手順で IAM 関連の設定を実施してください。\n",
    "1. <a href=\\\"policy/sagemaker-policy.json\\\" target=\\\"_blank\\\">policy/sagemaker-policy.json</a> の中身をコピー\n",
    "1. <a href=\\\"https://{region}.console.aws.amazon.com/iam/home#/policies$new?step=edit\\\" target=\\\"_blank\\\">IAM Policy の作成</a>をクリックし、**JSON** タブをクリックしてから手順1でコピーした JSON をペーストして右下の **次のステップ：タグ** ボタンをクリック\n",
    "1. 右下の **次のステップ：確認** ボタンをクリック\n",
    "1. **名前** に **{sagemaker_policy_name}** を記載して、右下の **ポリシーの作成** ボタンをクリック\n",
    "1.  <a href=\\\"https://us-east-1.console.aws.amazon.com/sagemaker/home?region={region}#/notebook-instances\\\" target=\\\"_blank\\\">ノートブックインスタンス一覧</a> を開いてこのノートブックを実行しているノートブックをクリック\n",
    "1. **アクセス許可と暗号化** の部分に表示されている IAM ロールへのリンクをクリック\n",
    "1. **アクセス許可を追加** をクリックして **ポリシーをアタッチ** を選択\n",
    "1. **その他の許可ポリシー** の検索ボックスで手順4 で作成した {sagemaker_policy_name} を検索して横にあるチェックボックスをオンにする\n",
    "1. **ポリシーのアタッチ** をクリック\n",
    "\"\"\"\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックのコードは、以前からのノートブックインスタンスで実行する場合と、SageMaker Studio のノートブックで実行する場合とで挙動が異なります。以下のセルを実行することで、いまの実行環境が以前からのノートブックインスタンスなのか、SageMaker Studio のノートブックなのかを判定して、`on_studio`に記録します。この結果に基づいて、以降のノートブックの実行を次のように変更します。\n",
    "\n",
    "- データセットの展開先を変更します。SageMaker Studio を利用する場合、home のディレクトリは EFS をマウントして実現されており、データセットを展開する際にやや時間を要します。そこで home 以外のところへ展開するようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "NOTEBOOK_METADATA_FILE = \"/opt/ml/metadata/resource-metadata.json\"\n",
    "if os.path.exists(NOTEBOOK_METADATA_FILE):\n",
    "    with open(NOTEBOOK_METADATA_FILE, \"rb\") as f:\n",
    "        metadata = json.loads(f.read())\n",
    "        domain_id = metadata.get(\"DomainId\")\n",
    "        on_studio = True if domain_id is not None else False\n",
    "print(\"Is this notebook runnning on Studio?: {}\".format(on_studio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データ\n",
    "### 3.1. データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://fast-ai-imageclas/mnist_png.tgz . --no-sign-request\n",
    "if on_studio:\n",
    "    !tar -xzf mnist_png.tgz -C /opt/ml --no-same-owner\n",
    "else:\n",
    "    !tar -xvzf  mnist_png.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "\n",
    "root_dir_studio = '/opt/ml'\n",
    "data_dir = os.path.join(root_dir_studio,'data') if on_studio else 'data'\n",
    "training_dir = os.path.join(root_dir_studio,'mnist_png/training') if on_studio else 'mnist_png/training'\n",
    "test_dir = os.path.join(root_dir_studio,'mnist_png/testing') if on_studio else 'mnist_png/testing'\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "training_data = datasets.ImageFolder(root=training_dir,\n",
    "                            transform=transforms.Compose([\n",
    "                            transforms.Grayscale(),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                            transform=transforms.Compose([\n",
    "                            transforms.Grayscale(),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "training_data_loader = DataLoader(training_data, batch_size=len(training_data))\n",
    "training_data_loaded = next(iter(training_data_loader))\n",
    "torch.save(training_data_loaded, os.path.join(data_dir, 'training.pt'))\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "test_data_loaded = next(iter(test_data_loader))\n",
    "torch.save(test_data_loaded, os.path.join(data_dir, 'test.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.データをS3にアップロードする\n",
    "データセットを S3 にアップロードするには、 `sagemaker.Session.upload_data` 関数を使用します。 戻り値として入力した S3 のロケーションは、後で学習ジョブを実行するときに使用します。このサンプルでは、学習データを 2セット用意して並列で学習を実行するため、2ヶ所にデータをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix+'/1')\n",
    "inputs2 = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix+'/2')\n",
    "print(inputs1)\n",
    "print(inputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SageMaker Estimator の準備\n",
    "\n",
    "学習の条件を設定するため、Estimator クラスの子クラスの PyTorch オブジェクトを作成します。 ここでは、PyTorchスクリプト、IAMロール、および（ジョブごとの）ハードウェア構成を渡す PyTorch Estimator を定義しています。また合わせてローカルの `source_dir` を指定することで、依存するスクリプト群をコンテナにコピーして、学習時に使用することが可能です。\n",
    "\n",
    "ハイパーパラメータは Step Functions 実行時に渡すため、ここでは設定しません。（ここで設定したものは後ですべて上書きされて無効になります）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "instance_type = 'ml.m5.xlarge'\n",
    "\n",
    "estimator = PyTorch(\n",
    "                    entry_point=\"mnist.py\",\n",
    "                    role=role,\n",
    "                    framework_version='1.8.0',\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "#                     hyperparameters={\n",
    "#                         'batch-size':128,\n",
    "#                         'lr': 0.01,\n",
    "#                         'epochs': 1,\n",
    "#                         'backend': 'gloo'\n",
    "#                     }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AWS Step Functions の準備\n",
    "\n",
    "前の手順で作成した Estimator を使って Step Functions の TrainingStep を作成し、その後 Workflow を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.inputs import ExecutionInput, StepInput\n",
    "from stepfunctions.steps import (\n",
    "    Chain,\n",
    "    ChoiceRule,\n",
    "    ModelStep,\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    ")\n",
    "from stepfunctions.template import TrainingPipeline\n",
    "from stepfunctions.template.utils import replace_parameters_with_jsonpath\n",
    "from stepfunctions.workflow import Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 IAM Role と Policy の作成\n",
    "\n",
    "Step Functions の Workflow にセットする IAM Role を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "policy_arn_list = []\n",
    "role_name_list = []\n",
    "\n",
    "def get_policy_arn(policy_name):\n",
    "    next_token = ''\n",
    "    while True:\n",
    "        if next_token == '':\n",
    "            response = iam_client.list_policies(Scope='Local')\n",
    "        else:\n",
    "            response = iam_client.list_policies(Scope='Local', Marker=next_token)\n",
    "        for content in response['Policies']:\n",
    "            if policy_name == content['PolicyName']:\n",
    "                return content['Arn']\n",
    "        if 'Marker' in response:\n",
    "            next_token = response['Marker']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "def detach_role_policies(role_name):\n",
    "    try:\n",
    "        response = iam_client.list_attached_role_policies(\n",
    "            RoleName=role_name,\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    policies = response['AttachedPolicies']\n",
    "\n",
    "    for p in policies:\n",
    "        response = iam_client.detach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=p['PolicyArn']\n",
    "        )\n",
    "\n",
    "            \n",
    "def create_role(role_name, assume_role_policy):\n",
    "    try:\n",
    "        response = iam_client.create_role(\n",
    "            Path = '/service-role/',\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps(assume_role_policy),\n",
    "            MaxSessionDuration=3600*12 # 12 hours\n",
    "        )\n",
    "        role_arn = response['Role']['Arn']\n",
    "    except Exception as ex:\n",
    "        if \"EntityAlreadyExists\" in str(ex):\n",
    "            detach_role_policies(role_name)\n",
    "            response = iam_client.delete_role(\n",
    "                RoleName = role_name,\n",
    "            )\n",
    "            response = iam_client.create_role(\n",
    "                Path = '/service-role/',\n",
    "                RoleName = role_name,\n",
    "                AssumeRolePolicyDocument = json.dumps(assume_role_policy),\n",
    "                MaxSessionDuration=3600*12 # 12 hours\n",
    "            )\n",
    "            role_arn = response['Role']['Arn']\n",
    "        else:\n",
    "            print(ex)\n",
    "    sleep(10)\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_policy(policy_name, policy_json_name):\n",
    "    with open('policy/' + policy_json_name, 'r') as f:\n",
    "        policy_json = json.load(f)\n",
    "    try:\n",
    "        response = iam_client.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(policy_json),\n",
    "        )\n",
    "        policy_arn = response['Policy']['Arn']\n",
    "    except Exception as ex:\n",
    "        if \"EntityAlreadyExists\" in str(ex):\n",
    "            response = iam_client.delete_policy(\n",
    "                PolicyArn=get_policy_arn(policy_name)\n",
    "            )\n",
    "            response = iam_client.create_policy(\n",
    "                PolicyName=policy_name,\n",
    "                PolicyDocument=json.dumps(policy_json),\n",
    "            )\n",
    "            policy_arn = response['Policy']['Arn']\n",
    "    policy_arn_list.append(policy_arn)\n",
    "    \n",
    "    sleep(10)\n",
    "    return policy_arn\n",
    "\n",
    "\n",
    "def create_policy_role(policy_name, policy_json_name, role_name, assume_role_policy):\n",
    "\n",
    "    role_arn = create_role(role_name, assume_role_policy)\n",
    "    policy_arn = create_policy(policy_name, policy_json_name)\n",
    "\n",
    "    sleep(5)\n",
    "    response = iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "\n",
    "    role_name_list.append(role_name)\n",
    "    policy_arn_list.append(policy_arn)\n",
    "    sleep(10)\n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step_functions_policy_name = project_name + '-stepfunctions-' + user_name + '-policy'\n",
    "step_functions_role_name = project_name + '-stepfunctions-' + user_name + '-role'\n",
    "step_functions_policy_json_name = 'stepfunctions-policy.json'\n",
    "\n",
    "assume_role_policy = {\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [{\"Sid\": \"\",\"Effect\": \"Allow\",\"Principal\": {\"Service\":\"states.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]\n",
    "    }\n",
    "\n",
    "workflow_execution_role = create_policy_role(step_functions_policy_name, step_functions_policy_json_name,\n",
    "                   step_functions_role_name, assume_role_policy)\n",
    "workflow_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Step Functions Workflow 実行時のパラメータの準備\n",
    "\n",
    "Step Functions Workflow 実行時に指定するパラメータのスキーマを定義します。すべての学習ジョブに共通でセットしたいパラメータは `ExecutionInput` で、学習ジョブごとに変えたいパラメータは `StepInput` で定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "        \"TrainingParameters\": dict,\n",
    "    }\n",
    ")\n",
    "\n",
    "step_input = StepInput(\n",
    "    schema={\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TrainingInput\": str,\n",
    "        \"TrainingOutput\": str,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TrainingStep の作成\n",
    "\n",
    "TrainingStep を作成します。学習データ、学習ジョブ名、学習済みモデルを保存するパスを、先ほど作成した ExecutionInput と StepInput を使って設定します。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = steps.TrainingStep(\n",
    "    \"SageMaker Training Step\",\n",
    "    estimator=estimator,\n",
    "    data={\"training\": sagemaker.TrainingInput(step_input[\"TrainingInput\"])},\n",
    "    job_name=step_input[\"TrainingJobName\"],\n",
    "    hyperparameters=execution_input[\"TrainingParameters\"],\n",
    "    output_data_config_path=step_input[\"TrainingOutput\"],\n",
    "    wait_for_completion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Step Functions Workflow の作成\n",
    "\n",
    "作成した TrainingStep を使って Map State を作成し、続けて Workflow を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "param_name = 'Jobs'\n",
    "\n",
    "training_map = steps.states.Map(\n",
    "    \"SageMaker training Map\",\n",
    "    iterator=training_step,\n",
    "    items_path=f'$.{param_name}',\n",
    ")\n",
    "\n",
    "workflow_graph = Chain([training_map])\n",
    "workflow_name = project_name+\"-\" + user_name\n",
    "\n",
    "branching_workflow = Workflow(\n",
    "    name=workflow_name,\n",
    "    definition=workflow_graph,\n",
    "    role=workflow_execution_role,\n",
    ")\n",
    "\n",
    "branching_workflow.create()\n",
    "branching_workflow.update(workflow_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 学習スクリプトを S3 にアップロード\n",
    "\n",
    "並列実行される学習ジョブで使用する学習スクリプトを tar.gz で固めて S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINNING_SCRIPT_LOCATION = \"source.tar.gz\"\n",
    "!tar zcvf $TRAINNING_SCRIPT_LOCATION mnist.py\n",
    "\n",
    "train_code = sagemaker_session.upload_data(\n",
    "    TRAINNING_SCRIPT_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(user_name, prefix, \"code\"),\n",
    ")\n",
    "train_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Step Functions Workflow 実行時のパラメータの作成\n",
    "\n",
    "Workflow 実行時に指定する、学習データのS3パス、学習ジョブ名、学習済みモデルを保存するS3パス、ハイパーパラメータなどの情報を作成します。Map State に渡す情報（今回は `input_params` ）は dict のリストとして作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "timestamp = datetime.now(tz=JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "input_data_list = [inputs1, inputs2]\n",
    "\n",
    "input_params = []\n",
    "\n",
    "for i in range(2):\n",
    "    id = str(i+1)\n",
    "    job_name_prefix = f'sfn-map-test-{timestamp}'\n",
    "    job_name = f'{job_name_prefix}-{id}'\n",
    "    input_params.append(\n",
    "        {\n",
    "            'TrainingJobName': job_name,\n",
    "            'TrainingInput': input_data_list[i],\n",
    "            'TrainingOutput':  f's3://{bucket}/{job_name_prefix}/output/result/{id}'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "input_params_dict = {}\n",
    "input_params_dict['TrainingParameters'] = {\n",
    "    \"sagemaker_program\": \"mnist.py\",\n",
    "    \"sagemaker_submit_directory\": train_code,\n",
    "    'batch-size':'128',\n",
    "    'lr': '0.01',\n",
    "    'epochs': '1',\n",
    "    'backend': 'gloo'}\n",
    "input_params_dict[param_name] = input_params\n",
    "input_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Step Functions Workflow の実行\n",
    "\n",
    "作成したパラメータを使って Step Functions Workflow を実行します。表示されたリンクから AWS コンソールに移動して今実行した Workflow を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = branching_workflow.execute(\n",
    "    inputs=input_params_dict\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"<a href=\\\"https://{region}.console.aws.amazon.com/states/home?region={region}#/statemachines/view/arn:aws:states:us-east-1:{account_id}:stateMachine:{workflow_name}\\\" target=\\\"_blank\\\">Step Functions のコンソール</a>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. リソースの削除\n",
    "\n",
    "このノートブックで作成したリソースを削除しましょう。\n",
    "\n",
    "### Step Functions Workflow の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_list = Workflow.list_workflows()\n",
    "workflow_arn = [d['stateMachineArn'] for d in workflow_list  if d['name']==workflow_name][0]\n",
    "sfn_workflow = Workflow.attach(workflow_arn)\n",
    "try:\n",
    "    sfn_workflow.delete()\n",
    "    print('Delete:', workflow_name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM Role と Policy の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name_list = list(set(role_name_list))\n",
    "policy_arn_list = list(set(policy_arn_list))\n",
    "\n",
    "for r in role_name_list:\n",
    "    try:\n",
    "        detach_role_policies(r)\n",
    "        iam_client.delete_role(RoleName=r)\n",
    "        print('IAM Role 削除完了:', r)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "for p in policy_arn_list:\n",
    "    try:\n",
    "        iam_client.delete_policy(PolicyArn=p)\n",
    "        print('IAM Policy 削除完了:', p)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# ノートブックインスタンスにアタッチしたポリシーの削除\n",
    "sagemaker_policy_arn = get_policy_arn(sagemaker_policy_name)\n",
    "response = iam_client.detach_role_policy(\n",
    "    RoleName=role.split('/')[2],\n",
    "    PolicyArn=sagemaker_policy_arn\n",
    ")\n",
    "print('\\nこちらの IAM Policy は手動で削除してください。', sagemaker_policy_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
