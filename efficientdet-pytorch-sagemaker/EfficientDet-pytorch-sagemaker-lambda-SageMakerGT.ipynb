{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aa39bc",
   "metadata": {},
   "source": [
    "# Amazon SageMaker で EfficientDet (PyTorch) を転移学習して AWS Lambda にデプロイして推論する\n",
    "\n",
    "このノートブックは、物体検知のための機械学習アルゴリズム [EfficientDet (PyTorch 実装)](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) を Amazon SageMaker の学習ジョブを使って転移学習し、学習したモデルを AWS Lambda にデプロイして推論を実行するサンプルノートブックです。Amazon SageMaker ノートブックインスタンス上でこのノートブックを使用してください。ノートブックインスタンスの起動方法は [こちらの記事](https://qiita.com/mariohcat/items/304b5e9ba20c7761084f) をご参照ください。ノートブックインスタンスタイプはデフォルトの ml.t2.medium で構いませんが、コンテナのビルドに 10分ほどかかるため、時間を短縮したい場合は ml.c5.xlarge などのインスタンスを選択してください。\n",
    "\n",
    "### 注意\n",
    "- **このノートブックでは GPU インスタンスを使ってモデルの学習をします。学習時間が数時間になる場合もありますので、[料金](https://aws.amazon.com/jp/sagemaker/pricing/) にご注意ください。**\n",
    "- このノートブックは、Amazon SageMaker Ground Truth を使ってあらかじめ 100枚程度の画像に対して物体検知（境界ボックス）のラベリングを実施してある想定です。Amazon SageMaker Ground Truth の使い方は [こちらの記事](https://aws.amazon.com/jp/builders-flash/202003/sagemaker-groundtruth-cat/?awsf.filter-name=*all) をご参照ください。\n",
    "\n",
    "このノートブックの全体の流れは、\n",
    "\n",
    "1. 学習に使用するデータの準備\n",
    "1. SageMaker 学習ジョブを使ってモデルの学習\n",
    "1. ノートブックインスタンス上で推論（モデルがうまく学習できているかの確認）\n",
    "1. AWS Lambda で推論\n",
    "\n",
    "## 準備\n",
    "\n",
    "### ロールにポリシーを追加\n",
    "\n",
    "**このサンプルでは、コンテナを Amazon ECR に push および AWS Lambda 関数の実行を行います。**以下の操作でこのノートブックインスタンスで使用している IAM ロールに Amazon ECR にイメージを push するための権限と AWS Lambda 関数を実行するための権限を追加してください。\n",
    "\n",
    "1. Amazon SageMaker コンソールからこのノートブックインスタンスの詳細画面を表示<br>\n",
    "（左側のメニューのインスタンス -> ノートブックインスタンス -> インスタンス名をクリック）\n",
    "1. 「アクセス許可と暗号化」の「IAM ロール ARN」のリンクをクリック（IAM のコンソールに遷移します）\n",
    "1. 「ポリシーをアタッチします」と書いてある青いボタンをクリック\n",
    "1. 検索ボックスに ec2containerregistry と入力し AmazonEC2ContainerRegistryFullAccess のチェックボックスをチェックする\n",
    "1. 「ポリシーのアタッチ」と書いてある青いボタンをクリック\n",
    "1. 「インラインポリシーの追加」をクリック\n",
    "1. 「サービス」で Lambda を選択\n",
    "1. 「アクション」で「InvokeFunction」を検索して選択\n",
    "1. 「リソース」で「ARN の追加」と書かれたリンクをクリックして「Region」に使用しているリージョン名、「Function name」に呼び出したい Lambda 関数名を入力して「追加」ボタンをクリック\n",
    "1. 「ポリシーの確認」ボタンをクリック\n",
    "1. 「名前」に任意の名前を入力して「ポリシーの作成」ボタンをクリック\n",
    "\n",
    "### EfficientDet のソースコードのセットアップ\n",
    "\n",
    "以下のセルを実行して、今回使用する EfficientDet のリポジトリをクローンします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8f89a",
   "metadata": {},
   "source": [
    "以下のセルを実行して、クローンしたソースコードから必要なものを src フォルダにコピーします。src の中のソースコードは、モデルを学習する際に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61004539",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src\n",
    "!mkdir -p docker/train\n",
    "!cp Yet-Another-EfficientDet-Pytorch/backbone.py src\n",
    "!cp -r Yet-Another-EfficientDet-Pytorch/efficientdet src\n",
    "!cp -r Yet-Another-EfficientDet-Pytorch/efficientnet src\n",
    "!cp -r Yet-Another-EfficientDet-Pytorch/utils src\n",
    "!pip install pycocotools tqdm pyyaml webcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0eb15",
   "metadata": {},
   "source": [
    "### SageMaker を使うための準備\n",
    "\n",
    "以下のセルでは、Amazon SageMaker を使うためのセットアップを行います。ロールの情報、ノートブックインスタンスのリージョン、アカウントID などの情報を取得しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6869c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_prefix = 'EfficientDet-PyTorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ff462",
   "metadata": {},
   "source": [
    "### アノテーションデータを COCO形式に変換\n",
    "\n",
    "ここからは、アノテーションデータを COCO形式に変換します。今回使用する EfficientDet の実装は COCO形式のアノテーションファイルを前提としているため、COCO形式以外のアノテーションデータしかない場合は、COCO形式への変換が必要です。\n",
    "\n",
    "このサンプルでは、Amazon SageMaker Ground Truth を使ってアノテーションした結果のファイルである manifest ファイルを COCO 形式に変換します。<b>このサンプルでは物体検知モデルを学習するため、SageMaker Ground Truth でラベリングする際に「境界ボックス」のラベリングタイプを指定してラベリングしてあることを前提としています。</b>\n",
    "\n",
    "以下のセルに Amazon SageMaker Ground Truth の output.manifest ファイルが保存されている S3 パスを入力して実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_s3_path = 's3://gt_output_path/manifests/output/output.manifest'\n",
    "label_s3_path = 's3://gt_output_path/annotation-tool/data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1948edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $manifest_s3_path ./output.manifest\n",
    "!aws s3 cp $label_s3_path ./data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59502d81",
   "metadata": {},
   "source": [
    "以下のセルを実行して、output.manifest を COCO形式に変換して、さらに学習用と検証用に分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベリングしたデータを学習用と検証用に分ける際の学習用データの割合\n",
    "train_data_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcefca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import ntpath\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import boto3\n",
    "\n",
    "# 学習データ出力フォルダ名\n",
    "coco_output_dir = 'datasets-gt-cats'\n",
    "\n",
    "s3 = boto3.resource('s3') #S3オブジェクトを取得\n",
    "\n",
    "from argparse import Namespace, ArgumentParser\n",
    "\n",
    "\n",
    "def find_category_name(dic, class_id):\n",
    "\n",
    "    for k, v in dic.items():\n",
    "        if int(k)== class_id:\n",
    "            return v\n",
    "        \n",
    "    return 'none'\n",
    "\n",
    "def load_annotation_bbox(line):\n",
    "    info = Namespace()\n",
    "    metadata = re.search(r'\"([\\w-]+-metadata)\"', line).group()[1:-1]\n",
    "    json_load = json.loads(line)\n",
    "    job_name = metadata[:-9]\n",
    "\n",
    "    # Image information\n",
    "    info.image_fn = json_load['source-ref']\n",
    "    info.width = json_load[job_name]['image_size'][0]['width']\n",
    "    info.height =  json_load[job_name]['image_size'][0]['height']\n",
    "    \n",
    "    # Bounding boxes\n",
    "    objects = []\n",
    "    for obj in json_load[job_name]['annotations']:\n",
    "        label = find_category_name(json_load[metadata]['class-map'], obj['class_id'])\n",
    "        class_id = obj['class_id']\n",
    "        xmin = obj['left']\n",
    "        ymin = obj['top']\n",
    "        width = obj['width']\n",
    "        height = obj['height']\n",
    "\n",
    "        objects.append(Namespace(label=label, class_id=class_id, xmin=xmin, ymin=ymin, width=width, height=height))\n",
    "    info.objects = objects\n",
    "\n",
    "    return info\n",
    "\n",
    "root_dir = './'\n",
    "\n",
    "label_exists = False\n",
    "\n",
    "coco_file_name_train =os.path.join(root_dir, coco_output_dir, 'annotations/instances_train.json')\n",
    "coco_file_name_validation =os.path.join(root_dir, coco_output_dir, 'annotations/instances_validation.json')\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(root_dir, coco_output_dir, 'annotations')):\n",
    "    os.makedirs(os.path.join(root_dir, coco_output_dir, 'annotations'))\n",
    "    os.makedirs(os.path.join(root_dir, coco_output_dir, 'annotations/train'))\n",
    "    os.makedirs(os.path.join(root_dir, coco_output_dir, 'annotations/validation'))\n",
    "\n",
    "images_train = []\n",
    "images_validation = []\n",
    "annotations_train = []\n",
    "annotations_validation = []\n",
    "image_suffix = [\"jpg\", \"png\", \"jpeg\"]\n",
    "\n",
    "with open('output.manifest', 'r') as f:\n",
    "    line = f.readline()\n",
    "\n",
    "    while line:\n",
    "        line = line.strip()\n",
    "        info = load_annotation_bbox(line)\n",
    "        \n",
    "        s3path = json.loads(line)['source-ref']\n",
    "        bucket_name = s3path.split('/')[2]\n",
    "\n",
    "        bucket_boto3 = s3.Bucket(bucket_name)\n",
    "        path = s3path[6+len(bucket_name):]\n",
    "#         print(bucket_name, path)\n",
    "        \n",
    "        rd = random.random()\n",
    "        \n",
    "        dirname = 'train'\n",
    "        \n",
    "        if rd < train_data_ratio:\n",
    "            images = images_train\n",
    "            annotations = annotations_train\n",
    "        else:\n",
    "            images = images_validation\n",
    "            annotations = annotations_validation\n",
    "            dirname = 'validation'\n",
    "    \n",
    "        image_id = len(images)\n",
    "        image_base_fn = os.path.basename(info.image_fn)\n",
    "        bucket_boto3.download_file(path, os.path.join(root_dir, coco_output_dir, 'annotations', dirname, image_base_fn))\n",
    "        \n",
    "        images.append({\n",
    "            \"file_name\": image_base_fn,\n",
    "            \"height\": info.height,\n",
    "            \"width\": info.width,\n",
    "            \"id\": image_id\n",
    "        })\n",
    "        \n",
    "        # Annotation information\n",
    "        for obj in info.objects:\n",
    "\n",
    "            annotations.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"bbox\": [obj.xmin, obj.ymin, obj.width, obj.height],\n",
    "                \"category_id\": obj.class_id  + 1, # Category ID is zero indexed!!\n",
    "                \"id\": len(annotations),\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": (obj.width) * (obj.height)\n",
    "            })\n",
    "\n",
    "        line = f.readline()\n",
    "\n",
    "labels = []\n",
    "with open('data.json', 'r') as f:\n",
    "    line = f.read()\n",
    "    line_json = json.loads(line)\n",
    "    labels_list = line_json['labels']\n",
    "    \n",
    "    for l in labels_list:\n",
    "        labels.append(l['label'])\n",
    "        \n",
    "# Categories\n",
    "categories = []\n",
    "for i, c in enumerate(labels):\n",
    "    categories.append({'supercategory': c, 'name': c, 'id': i+1})\n",
    "\n",
    "\n",
    "# Save annotation file\n",
    "json.dump(\n",
    "    {\"images\": images_train, \"annotations\": annotations_train, \"categories\": categories},\n",
    "    open(coco_file_name_train, \"w\")\n",
    ")\n",
    "json.dump(\n",
    "    {\"images\": images_validation, \"annotations\": annotations_validation, \"categories\": categories},\n",
    "    open(coco_file_name_validation, \"w\")\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33981ad",
   "metadata": {},
   "source": [
    "### モデルの学習に必要なデータを Amazon S3 にアップロード\n",
    "\n",
    "学習データ、検証データ、アノテーションデータを 、あとで学習ジョブで使用できるように Amazon S3 にアップロードします。\n",
    "\n",
    "以下のセルに、アノテーションデータをアップロードしたい S3 パスを入力して実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_s3_path = 's3://bucket/path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $coco_output_dir/annotations $train_data_s3_path/annotations --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed3ac3",
   "metadata": {},
   "source": [
    "## モデルの学習\n",
    "\n",
    "ここからは、EfficientDet のモデルを SageMaker の学習ジョブの機能を使って学習するための手順です。まずは学習を行うスクリプト `train.py` を作成します。このスクリプトは、初めにクローンしたリポジトリのコードをベースに、SageMaker の学習ジョブ用にカスタマイズしたものです。このサンプルノートブックでは、以下のセルを実行することで src/train.py が作成されますが、別途作成した train.py を src フォルダにコピーしても問題ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6340b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "# original author: signatrix\n",
    "# adapted from https://github.com/signatrix/efficientdet/blob/master/train.py\n",
    "# modified by Zylo117\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.dataset import CocoDataset, Resizer, Normalizer, Augmenter, collater\n",
    "from efficientdet.loss import FocalLoss\n",
    "from utils.sync_batchnorm import patch_replication_callback\n",
    "from utils.utils import replace_w_sync_bn, CustomDataParallel, get_last_weights, init_weights, boolean_string\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "class Params:\n",
    "    def __init__(self, project_file):\n",
    "        self.params = yaml.safe_load(open(project_file).read())\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return self.params.get(item, None)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser('Yet Another EfficientDet Pytorch: SOTA object detection network - Zylo117')\n",
    "    parser.add_argument('-c', '--compound_coef', type=int, default=0, help='coefficients of efficientdet')\n",
    "    parser.add_argument('-n', '--num_workers', type=int, default=12, help='num_workers of dataloader')\n",
    "    parser.add_argument('--batch_size', type=int, default=12, help='The number of images per batch among all devices')\n",
    "    parser.add_argument('--head_only', type=boolean_string, default=False,\n",
    "                        help='whether finetunes only the regressor and the classifier, '\n",
    "                             'useful in early stage convergence or small/easy dataset')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--optim', type=str, default='adamw', help='select optimizer for training, '\n",
    "                                                                   'suggest using \\'admaw\\' until the'\n",
    "                                                                   ' very final stage then switch to \\'sgd\\'')\n",
    "    parser.add_argument('--num_epochs', type=int, default=500)\n",
    "    parser.add_argument('--val_interval', type=int, default=1, help='Number of epoches between valing phases')\n",
    "    parser.add_argument('--save_interval', type=int, default=500, help='Number of steps between saving')\n",
    "    parser.add_argument('--es_min_delta', type=float, default=0.0,\n",
    "                        help='Early stopping\\'s parameter: minimum change loss to qualify as an improvement')\n",
    "    parser.add_argument('--es_patience', type=int, default=0,\n",
    "                        help='Early stopping\\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.')\n",
    "    parser.add_argument('--data_path', type=str, default='datasets/', help='the root folder of dataset')\n",
    "    parser.add_argument('--log_path', type=str, default='logs/')\n",
    "    parser.add_argument('-w', '--load_weights', type=str, default=None,\n",
    "                        help='whether to load weights from a checkpoint, set None to initialize, set \\'last\\' to load last checkpoint')\n",
    "    parser.add_argument('--saved_path', type=str, default='logs/')\n",
    "    parser.add_argument('--debug', type=boolean_string, default=False,\n",
    "                        help='whether visualize the predicted boxes of training, '\n",
    "                             'the output images will be in test/')\n",
    "    parser.add_argument('--init', type=boolean_string, default=False,\n",
    "                        help='whether start training from epoch 0')\n",
    "    \n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation-dir', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    parser.add_argument('--annotation-dir', type=str, default=os.environ['SM_CHANNEL_ANNOTATIONS'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument(\"--checkpoint-path\",type=str,default=\"/opt/ml/checkpoints\")\n",
    "    parser.add_argument('--output-dir', type=str, default=os.environ['SM_OUTPUT_DIR'])\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "class ModelWithLoss(nn.Module):\n",
    "    def __init__(self, model, debug=False):\n",
    "        super().__init__()\n",
    "        self.criterion = FocalLoss()\n",
    "        self.model = model\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, imgs, annotations, obj_list=None):\n",
    "        _, regression, classification, anchors = self.model(imgs)\n",
    "        if self.debug:\n",
    "            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations,\n",
    "                                                imgs=imgs, obj_list=obj_list)\n",
    "        else:\n",
    "            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations)\n",
    "        return cls_loss, reg_loss\n",
    "\n",
    "\n",
    "def train(opt):\n",
    "    params = Params(f'settings.yml')\n",
    "    \n",
    "\n",
    "    if opt.num_gpus == 0:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    else:\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "    opt.saved_path = opt.checkpoint_path + f'/'\n",
    "    opt.log_path = opt.output_dir + f'/tensorboard/'\n",
    "    os.makedirs(opt.log_path, exist_ok=True)\n",
    "    os.makedirs(opt.saved_path, exist_ok=True)\n",
    "\n",
    "    training_params = {'batch_size': opt.batch_size,\n",
    "                       'shuffle': True,\n",
    "                       'drop_last': True,\n",
    "                       'collate_fn': collater,\n",
    "                       'num_workers': opt.num_workers}\n",
    "\n",
    "    val_params = {'batch_size': opt.batch_size,\n",
    "                  'shuffle': False,\n",
    "                  'drop_last': True,\n",
    "                  'collate_fn': collater,\n",
    "                  'num_workers': opt.num_workers}\n",
    "\n",
    "    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "    training_set = CocoDataset(root_dir='/opt/ml/input/data', set=params.train_set,\n",
    "                               transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "                                                             Augmenter(),\n",
    "                                                             Resizer(input_sizes[opt.compound_coef])]))\n",
    "\n",
    "    training_generator = DataLoader(training_set, **training_params)\n",
    "\n",
    "    val_set = CocoDataset(root_dir='/opt/ml/input/data', set=params.val_set,\n",
    "                          transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "                                                        Resizer(input_sizes[opt.compound_coef])]))\n",
    "    val_generator = DataLoader(val_set, **val_params)\n",
    "\n",
    "    model = EfficientDetBackbone(num_classes=len(params.obj_list), compound_coef=opt.compound_coef,\n",
    "                                 ratios=eval(params.anchors_ratios), scales=eval(params.anchors_scales))\n",
    "\n",
    "    # load last weights\n",
    "    if opt.load_weights is not None:\n",
    "        if opt.load_weights.endswith('.pth'):\n",
    "            weights_path = opt.load_weights\n",
    "        else:\n",
    "            weights_path = get_last_weights(opt.saved_path)\n",
    "        try:\n",
    "            last_step = int(os.path.basename(weights_path).split('_')[-1].split('.')[0])\n",
    "        except:\n",
    "            last_step = 0\n",
    "\n",
    "        try:\n",
    "            ret = model.load_state_dict(torch.load(weights_path), strict=False)\n",
    "        except RuntimeError as e:\n",
    "            print(f'[Warning] Ignoring {e}')\n",
    "            print(\n",
    "                '[Warning] Don\\'t panic if you see this, this might be because you load a pretrained weights with different number of classes. The rest of the weights should be loaded already.')\n",
    "\n",
    "        print(f'[Info] loaded weights: {os.path.basename(weights_path)}, resuming checkpoint from step: {last_step}')\n",
    "    else:\n",
    "        last_step = 0\n",
    "        print('[Info] initializing weights...')\n",
    "        init_weights(model)\n",
    "        \n",
    "    if opt.init:\n",
    "        last_step = 0\n",
    "        \n",
    "\n",
    "    # freeze backbone if train head_only\n",
    "    if opt.head_only:\n",
    "        def freeze_backbone(m):\n",
    "            classname = m.__class__.__name__\n",
    "            for ntl in ['EfficientNet', 'BiFPN']:\n",
    "                if ntl in classname:\n",
    "                    for param in m.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        model.apply(freeze_backbone)\n",
    "        print('[Info] freezed backbone')\n",
    "\n",
    "    # https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
    "    # apply sync_bn when using multiple gpu and batch_size per gpu is lower than 4\n",
    "    #  useful when gpu memory is limited.\n",
    "    # because when bn is disable, the training will be very unstable or slow to converge,\n",
    "    # apply sync_bn can solve it,\n",
    "    # by packing all mini-batch across all gpus as one batch and normalize, then send it back to all gpus.\n",
    "    # but it would also slow down the training by a little bit.\n",
    "    if opt.num_gpus > 1 and opt.batch_size // opt.num_gpus < 4:\n",
    "        model.apply(replace_w_sync_bn)\n",
    "        use_sync_bn = True\n",
    "    else:\n",
    "        use_sync_bn = False\n",
    "\n",
    "    writer = SummaryWriter(opt.log_path + f'/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}/')\n",
    "\n",
    "    # warp the model with loss function, to reduce the memory usage on gpu0 and speedup\n",
    "    model = ModelWithLoss(model, debug=opt.debug)\n",
    "\n",
    "    if opt.num_gpus > 0:\n",
    "        model = model.cuda()\n",
    "        if opt.num_gpus > 1:\n",
    "            model = CustomDataParallel(model, opt.num_gpus)\n",
    "            if use_sync_bn:\n",
    "                patch_replication_callback(model)\n",
    "\n",
    "    if opt.optim == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), opt.lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), opt.lr, momentum=0.9, nesterov=True)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "    epoch = 0\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    step = max(0, last_step)\n",
    "    model.train()\n",
    "\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(opt.num_epochs):\n",
    "            last_epoch = step // num_iter_per_epoch\n",
    "            if epoch < last_epoch:\n",
    "                continue\n",
    "\n",
    "            epoch_loss = []\n",
    "            progress_bar = tqdm(training_generator)\n",
    "            for iter, data in enumerate(progress_bar):\n",
    "                if iter < step - last_epoch * num_iter_per_epoch:\n",
    "                    progress_bar.update()\n",
    "                    continue\n",
    "                try:\n",
    "                    imgs = data['img']\n",
    "                    annot = data['annot']\n",
    "\n",
    "                    if opt.num_gpus == 1:\n",
    "                        # if only one gpu, just send it to cuda:0\n",
    "                        # elif multiple gpus, send it to multiple gpus in CustomDataParallel, not here\n",
    "                        imgs = imgs.cuda()\n",
    "                        annot = annot.cuda()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n",
    "                    cls_loss = cls_loss.mean()\n",
    "                    reg_loss = reg_loss.mean()\n",
    "\n",
    "                    loss = cls_loss + reg_loss\n",
    "                    if loss == 0 or not torch.isfinite(loss):\n",
    "                        continue\n",
    "\n",
    "                    loss.backward()\n",
    "                    # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss.append(float(loss))\n",
    "\n",
    "                    logger.info(\n",
    "                        'Step: {}. Epoch: {}/{}. Iteration: {}/{}. Train Cls loss: {:.5f}, Train Reg loss: {:.5f}, Train Total loss: {:.5f}'.format(\n",
    "                            step, epoch, opt.num_epochs, iter + 1, num_iter_per_epoch, cls_loss.item(),\n",
    "                            reg_loss.item(), loss.item()))\n",
    "                    writer.add_scalars('Loss', {'train': loss}, step)\n",
    "                    writer.add_scalars('Regression_loss', {'train': reg_loss}, step)\n",
    "                    writer.add_scalars('Classfication_loss', {'train': cls_loss}, step)\n",
    "\n",
    "                    # log learning_rate\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    writer.add_scalar('learning_rate', current_lr, step)\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "                    if step % opt.save_interval == 0 and step > 0:\n",
    "                        save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "                        print('checkpoint...')\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('[Error]', traceback.format_exc())\n",
    "                    print(e)\n",
    "                    continue\n",
    "            scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "            if epoch % opt.val_interval == 0:\n",
    "                model.eval()\n",
    "                loss_regression_ls = []\n",
    "                loss_classification_ls = []\n",
    "                for iter, data in enumerate(val_generator):\n",
    "                    with torch.no_grad():\n",
    "                        imgs = data['img']\n",
    "                        annot = data['annot']\n",
    "\n",
    "                        if opt.num_gpus == 1:\n",
    "                            imgs = imgs.cuda()\n",
    "                            annot = annot.cuda()\n",
    "\n",
    "                        cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n",
    "                        cls_loss = cls_loss.mean()\n",
    "                        reg_loss = reg_loss.mean()\n",
    "\n",
    "                        loss = cls_loss + reg_loss\n",
    "                        if loss == 0 or not torch.isfinite(loss):\n",
    "                            continue\n",
    "\n",
    "                        loss_classification_ls.append(cls_loss.item())\n",
    "                        loss_regression_ls.append(reg_loss.item())\n",
    "\n",
    "                cls_loss = np.mean(loss_classification_ls)\n",
    "                reg_loss = np.mean(loss_regression_ls)\n",
    "                loss = cls_loss + reg_loss\n",
    "\n",
    "                logger.info(\n",
    "                    'Val. Epoch: {}/{}. Classification loss: {:1.5f}, Regression loss: {:1.5f}, Val Total loss: {:1.5f}'.format(\n",
    "                        epoch, opt.num_epochs, cls_loss, reg_loss, loss))\n",
    "                writer.add_scalars('Loss', {'val': loss}, step)\n",
    "                writer.add_scalars('Regression_loss', {'val': reg_loss}, step)\n",
    "                writer.add_scalars('Classfication_loss', {'val': cls_loss}, step)\n",
    "\n",
    "                if loss + opt.es_min_delta < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                    save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                # Early stopping\n",
    "                if epoch - best_epoch > opt.es_patience > 0:\n",
    "                    print('[Info] Stop training at epoch {}. The lowest loss achieved is {}'.format(epoch, best_loss))\n",
    "                    break\n",
    "    except KeyboardInterrupt:\n",
    "        save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "        writer.close()\n",
    "    writer.close()\n",
    "                           \n",
    "    if isinstance(model, CustomDataParallel):\n",
    "        torch.save(model.module.model.state_dict(), os.path.join(opt.model_dir, 'model.pth'))\n",
    "    else:\n",
    "        torch.save(model.model.state_dict(), os.path.join(opt.model_dir, 'model.pth'))\n",
    "\n",
    "\n",
    "def save_checkpoint(model, name):\n",
    "    if isinstance(model, CustomDataParallel):\n",
    "        torch.save(model.module.model.state_dict(), os.path.join(opt.saved_path, name))\n",
    "    else:\n",
    "        torch.save(model.model.state_dict(), os.path.join(opt.saved_path, name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt = get_args()\n",
    "    train(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f2f72",
   "metadata": {},
   "source": [
    "次に、学習スクリプトが読むこむ設定ファイルを作成します。こちらも EfficientDet のリポジトリのものをベースにカスタマイズしています。一番下の行の `obj_list` には、分類したいクラス名をリストとしてセットします。各パラメタ名の後のコロンの後にスペースが必要です。<br>たとえば、`obj_list:<半角スペース>['class1', 'class2']` とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a29b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232ba1d",
   "metadata": {},
   "source": [
    "上記セルを実行した出力 `['class1', 'class2'...]` を以下のセルの `obj_list` にコピー＆ペーストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab228073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/settings.yml\n",
    "\n",
    "# project_name: coco_adaptor  # also the folder name of the dataset that under data_path folder\n",
    "train_set: train\n",
    "val_set: validation\n",
    "\n",
    "# mean and std in RGB order, actually this part should remain unchanged as long as your dataset is similar to coco.\n",
    "mean: [0.485, 0.456, 0.406]\n",
    "std: [0.229, 0.224, 0.225]\n",
    "\n",
    "# this is coco anchors, change it if necessary\n",
    "anchors_scales: '[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]'\n",
    "anchors_ratios: '[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]'\n",
    "\n",
    "# must match your dataset's category_id.\n",
    "# category_id is one_indexed\n",
    "obj_list: ['front_face', 'other_face']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db1231",
   "metadata": {},
   "source": [
    "次に、学習ジョブで使用するコンテナイメージを作るための Dockerfile を作成します。SageMaker が提供している PyTorch 1.4.0 のコンテナイメージをベースに、必要あライブラリのインストールと、転移学習に必要な学習済みモデルのダウンロードを行っています。このノートブックでは、EfficientDet の coefficient 0 のモデルを使用するため、`efficientdet-d0.pth` をダウンロードします。\n",
    "\n",
    "**以下のセルの FROM の行の 'us-east-1' の部分をお使いのリージョンに合わせて変更してください。（東京リージョンの場合 `ap-northeast-1`）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/train/Dockerfile\n",
    "FROM  763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.4.0-gpu-py36-cu101-ubuntu16.04\n",
    "\n",
    "WORKDIR /opt/app\n",
    "\n",
    "RUN pip3 install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors torchvision==0.5.0\n",
    "RUN mkdir -p /data/models\n",
    "\n",
    "RUN wget -O /data/models/efficientdet-d0.pth https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d0.pth\n",
    "# RUN wget -O /data/models/efficientdet-d2.pth https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d2.pth\n",
    "\n",
    "WORKDIR /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303931d",
   "metadata": {},
   "source": [
    "ここからは、Amazon ECR に関するセットアップをしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = 'efficientdet-pytorch-train'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "train_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89693e1",
   "metadata": {},
   "source": [
    "ベースイメージは Amazon SageMaker が用意している Amazon ECR リポジトリに保存されているため、そこへのアクセス権が必要です。以下のコマンドを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids 763104351884 --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c3b2d",
   "metadata": {},
   "source": [
    "それでは、以下のセルを実行してコンテナイメージをビルドして ECR にプッシュしましょう。t2.medium のノートブックインスタンスの場合、この処理には 10分程度かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5818a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker/train\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $train_repository_uri\n",
    "!docker push $train_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9176a6",
   "metadata": {},
   "source": [
    "ここまでで、SageMaker の学習ジョブを起動するために必要なコンテナイメージと学習スクリプトの準備が完了しました。それでは学習ジョブを実行しましょう。PyTorch の estimator を作成する際に引数で hyperparameters を設定しています。ここで EfficientDet のハイパーパラメタや転移学習のベースとなる学習済みモデルのパスを指定しています。学習済みモデルのパスは前の手順で Dockerfile で指定したパスと対応しています。`metric_definitions` を設定することで、学習ログから各種メトリクスの数値を抽出し、学習ジョブと紐付けを行います。これらのメトリクスは AWS コンソールの学習ジョブの詳細画面で確認することができます。\n",
    "\n",
    "以下のセルの `local_mode = False` を `local_mode = True` にすると、ノートブックインスタンス上で学習ジョブを動かすことができます。これをローカルモードを言います。新しいアルゴリズムを試す場合などは、ハイパーパラメタの `num_epochs` を 1 などの小さい値にしてローカルモードで学習ジョブを動かすことで素早くデバッグを行うことができます。ノートブックインスタンスのインスタンスタイプをモデルの学習に十分なスペックのものにして大きなエポック数でローカルモードで学習ジョブを動かすことも可能です。\n",
    "\n",
    "estimator.fit() で学習ジョブが開始します。引数で学習データ、検証データ、教師データが格納されている Amazon S3 パスを指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "import uuid\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    ss = LocalSession()\n",
    "    instance_type = 'local_gpu' # or 'local'\n",
    "else:\n",
    "    ss = session\n",
    "    instance_type = 'ml.p3.8xlarge'\n",
    "\n",
    "# Spot training をする場合は、チェックポイントの設定を推奨\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_path = 's3://{}/checkpoint-{}'.format(bucket, checkpoint_suffix)\n",
    "checkpoint_local_path='/opt/ml/checkpoints'\n",
    "\n",
    "job_name = ecr_repository + datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "                        entry_point='train.py',\n",
    "                        source_dir='src',\n",
    "                        image_uri=train_repository_uri,\n",
    "                        job_name_base=job_name,\n",
    "                        role=role, \n",
    "                        instance_count=1,\n",
    "                        sagemaker_session=ss,\n",
    "                        instance_type=instance_type,\n",
    "#                         max_run = 5000,\n",
    "#                         use_spot_instances = 'True',\n",
    "#                         max_wait = 10000,\n",
    "                        checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                        checkpoint_local_path=checkpoint_local_path,\n",
    "                        output_path=\"s3://{}/{}/output\".format(bucket, job_name),\n",
    "                        hyperparameters = {'num_epochs': 100, 'batch_size':32, 'head_only':True, 'lr':0.01, 'compound_coef': 0,\n",
    "                                                       'load_weights': '/data/models/efficientdet-d0.pth'},\n",
    "                        enable_sagemaker_metrics=True,\n",
    "#                         profiler_config=profiler_config,\n",
    "                        metric_definitions = [dict(\n",
    "                                                                Name = 'Val Cls Loss',\n",
    "                                                                Regex = 'Classification loss: (.*?),'\n",
    "                                                            ),\n",
    "                                                              dict(\n",
    "                                                                Name = 'Val Reg Loss',\n",
    "                                                                Regex = 'Regression loss: (.*?),'\n",
    "                                                            ),\n",
    "                                                              dict(\n",
    "                                                                Name = 'Val Total Loss',\n",
    "                                                                Regex = 'Val Total loss: (.*?)$'\n",
    "                                                            ),\n",
    "                                                              dict(\n",
    "                                                                Name = 'Train Cls Loss',\n",
    "                                                                Regex = 'Train Cls loss: (.*?),'\n",
    "                                                            ),\n",
    "                                                              dict(\n",
    "                                                                Name = 'Train Reg Loss',\n",
    "                                                                Regex = 'Train Reg loss: (.*?),'\n",
    "                                                            ),\n",
    "                                                              dict(\n",
    "                                                                Name = 'Train Total Loss',\n",
    "                                                                Regex = 'Train Total loss: (.*?)$'\n",
    "                                                            )\n",
    "                                             ]\n",
    "                          \n",
    ")\n",
    "\n",
    "pytorch_estimator.fit({'train': train_data_s3_path+'/annotations/train/', \n",
    "                       'validation': train_data_s3_path+'/annotations/validation/', \n",
    "                       'annotations': train_data_s3_path+'/annotations/'}, wait=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816827c",
   "metadata": {},
   "source": [
    "## ノートブックインスタンス上で推論\n",
    "\n",
    "先ほど学習したモデルを使って、ノートブックインスタンス上で推論を試してみます。この方法では、モデルをうまく学習できたかどうかをクイックに確認することができます。\n",
    "\n",
    "まずは、S3 に保存されている学習済みモデルをノートブック インスタンスにダウンロードして解凍します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_s3 = pytorch_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b78129",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $model_path_s3 ./\n",
    "!tar zvxf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f1043",
   "metadata": {},
   "source": [
    "以下のセルを実行して、ノートブックインスタンス上で推論を行うスクリプト `efficientdet_pred.py` を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bc936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./efficientdet_pred.py\n",
    "\n",
    "# Author: Zylo117\n",
    "\n",
    "\"\"\"\n",
    "Simple Inference Script of EfficientDet-Pytorch\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import time\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, STANDARD_COLORS, standard_to_bgr, get_index_label, plot_one_box\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "def display(preds, imgs, cnt, fname):\n",
    "    for i in range(len(imgs)):\n",
    "        if len(preds[i]['rois']) == 0:\n",
    "            continue\n",
    "\n",
    "        items = []\n",
    "        for j in range(len(preds[i]['rois'])):\n",
    "            x1, y1, x2, y2 = preds[i]['rois'][j].astype(np.int)\n",
    "            obj = obj_list[preds[i]['class_ids'][j]]\n",
    "            score = float(preds[i]['scores'][j])\n",
    "            items.append([obj, score])\n",
    "            plot_one_box(imgs[i], [x1, y1, x2, y2], label=obj,score=score,color=color_list[get_index_label(obj, obj_list)])\n",
    "\n",
    "        ofilename = os.path.join(args.output_path, fname)\n",
    "            \n",
    "        print(items)\n",
    "        print('result image is saved to ' + ofilename)\n",
    "        cv2.imwrite(ofilename, imgs[i])\n",
    "\n",
    "        \n",
    "parser = argparse.ArgumentParser('EfficientDet inference options')\n",
    "parser.add_argument('-c', '--compound-coef', type=int, default=0, help='0-8')\n",
    "parser.add_argument('-im', '--image-path', type=str, default='', help='folder name that has images for inference')\n",
    "parser.add_argument('-lb', '--label-name', type=str, default='', help='annotation data file name')\n",
    "parser.add_argument('-cl', '--class-labels', type=str, nargs=\"*\", default='', help='list of class labels')\n",
    "parser.add_argument('-t', '--conf-thresh', type=float, default=0.2, help='threshold for confidence value')\n",
    "# parser.add_argument('-w', '--weight-path', type=str, default='', help='weight file path')\n",
    "parser.add_argument('-o', '--output-path', type=str, default='result', help='output path')\n",
    "# parser.add_argument('-g', '--use-cuda', type=bool, default=False, help='whether use CUDA')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if len(args.class_labels) == 0:\n",
    "    print('[ERROR] --class-labels is required.')\n",
    "    sys.exit()\n",
    "else:\n",
    "    obj_list = args.class_labels\n",
    "    obj_list = list(map(lambda x:re.sub('[\\[\\],]','', x), obj_list))\n",
    "    \n",
    "\n",
    "os.makedirs(args.output_path, exist_ok=True)\n",
    "    \n",
    "compound_coef = args.compound_coef\n",
    "force_input_size = None  # set None to use default size\n",
    "\n",
    "if args.image_path[-3:] == 'jpg':\n",
    "    img_path_list = [args.image_path]\n",
    "else:\n",
    "    img_path_list = glob.glob(args.image_path+'/**/*', recursive=True)\n",
    "\n",
    "# print(img_path_list)\n",
    "\n",
    "# replace this part with your project's anchor config\n",
    "anchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n",
    "anchor_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "\n",
    "threshold = args.conf_thresh\n",
    "iou_threshold = 0.2\n",
    "\n",
    "use_cuda = False\n",
    "use_float16 = False\n",
    "cudnn.fastest = False\n",
    "cudnn.benchmark = False\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n",
    "                             ratios=anchor_ratios, scales=anchor_scales)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=torch.device(device) ))\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "if use_float16:\n",
    "    model = model.half()\n",
    "\n",
    "\n",
    "color_list = standard_to_bgr(STANDARD_COLORS)\n",
    "# tf bilinear interpolation is different from any other's, just make do\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "\n",
    "for i, ip in enumerate(img_path_list):\n",
    "    print('-------------------------------')\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    img_path = tuple([ip])\n",
    "\n",
    "    ori_imgs, framed_imgs, framed_metas = preprocess(*img_path, max_size=input_size)\n",
    "\n",
    "    if use_cuda:\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "    else:\n",
    "        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features, regression, classification, anchors = model(x)\n",
    "\n",
    "        regressBoxes = BBoxTransform()\n",
    "        clipBoxes = ClipBoxes()\n",
    "\n",
    "        out = postprocess(x,\n",
    "                          anchors, regression, classification,\n",
    "                          regressBoxes, clipBoxes,\n",
    "                          threshold, iou_threshold)\n",
    "        \n",
    "        \n",
    "    out = invert_affine(framed_metas, out)\n",
    "\n",
    "    print(str(time.time()-start), ' sec.')\n",
    "    if len(args.label_name) > 0:\n",
    "        pass\n",
    "    else:\n",
    "        display(out, ori_imgs, i, os.path.basename(ip))\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(result, columns=['filename', 'ground truth', 'prediction', 'Top', 'Includes'])\n",
    "result_df.to_csv('result.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df7b4f",
   "metadata": {},
   "source": [
    "作成した `efficientdet_pred.py` を実行して推論を行います。`-im` オプションには入力となる画像が保存されたフォルダのパスを指定します。このサンプルでは、モデルの検証用画像を指定しています。`-o` オプションで指定したフォルダに結果の画像が保存されます。`-cl` オプションには、分類クラスのリストを指定します。モデル学習時と同じ順番で指定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed9a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python efficientdet_pred.py -t 0.3 -im $coco_output_dir\"/annotations/validation/\"  -o \"result-gt\" -cl $labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3346413",
   "metadata": {},
   "source": [
    "上記セルの実行ログに表示されたいずれかの出力ファイル名を以下のセルにセットして実行すると、画像が表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(mpimg.imread(\"result-gt/img_00609.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acec7f6",
   "metadata": {},
   "source": [
    "## AWS Lambda で推論\n",
    "\n",
    "モデルがうまく学習できていることが確認できたら、モデルを Lambda にデプロイして推論してみましょう。EfficientDet に必要なライブラリや学習したモデルを含むコンテナイメージを作成し、それを Lambda にデプロイします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49582fb9",
   "metadata": {},
   "source": [
    "### コンテナイメージとソースコードの準備\n",
    "\n",
    "ここからは、コンテナイメージを作成するための準備をします。まずは EfficientDet の推論に必要なソースコードと学習済みモデルをコンテナイメージ用にコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f542b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./docker/lambda-inference/app\n",
    "!cp -r ./src/* ./docker/lambda-inference/app\n",
    "!cp model.pth ./docker/lambda-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f41a1",
   "metadata": {},
   "source": [
    "次に、Dockerfile を作成します。ノートブックインスタンスでデバッグする際に使用するサンプル画像もイメージの中に埋め込んでいますが、Lambda で実行する際は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./docker/lambda-inference/Dockerfile\n",
    "\n",
    "# Define custom function directory\n",
    "ARG FUNCTION_DIR=\"/function\"\n",
    "\n",
    "FROM python:3.6.10-slim-buster\n",
    "# FROM public.ecr.aws/lambda/python:3.8\n",
    "    \n",
    "# Include global arg in this stage of the build\n",
    "ARG FUNCTION_DIR\n",
    "\n",
    "# Install aws-lambda-cpp build dependencies\n",
    "RUN apt-get update && \\\n",
    "  apt-get install -y \\\n",
    "  g++ \\\n",
    "  make \\\n",
    "  cmake \\\n",
    "  unzip \\\n",
    "  libcurl4-openssl-dev \\\n",
    "  libopencv-dev\n",
    "#   libgl1-mesa-dev\n",
    "  \n",
    "RUN pip install \\\n",
    "    torch==1.4.0+cpu torchvision==0.5.0+cpu \\\n",
    "    opencv-python webcolors boto3 \\\n",
    "    -f https://download.pytorch.org/whl/torch_stable.html \\\n",
    "  && rm -rf /root/.cache/pip\n",
    "\n",
    "\n",
    "# Copy function code\n",
    "RUN mkdir -p ${FUNCTION_DIR}\n",
    "COPY app/ ${FUNCTION_DIR}/\n",
    "\n",
    "COPY model.pth ${FUNCTION_DIR}\n",
    "\n",
    "# Install the function's dependencies\n",
    "RUN pip install \\\n",
    "    --target ${FUNCTION_DIR} \\\n",
    "        awslambdaric\n",
    "\n",
    "\n",
    "# Set working directory to function root directory\n",
    "WORKDIR ${FUNCTION_DIR}\n",
    "\n",
    "\n",
    "ENTRYPOINT [ \"/usr/local/bin/python\", \"-m\", \"awslambdaric\" ]\n",
    "CMD [ \"app.handler\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636a21f",
   "metadata": {},
   "source": [
    "次のセルを実行して、Lambda が実行するスクリプト `app.py` を作成します。`class_num` に、モデルを学習した際のクラス数を設定してください。結果の画像にバウンディングボックスが表示されすぎる場合は、`threshold = 0.2` の数値を大きくしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8eddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./docker/lambda-inference/app/app.py\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, STANDARD_COLORS, standard_to_bgr, get_index_label, plot_one_box\n",
    "\n",
    "import boto3\n",
    "\n",
    "version = '1.0'\n",
    "modelname = 'model.pth'\n",
    "s3 = boto3.resource('s3')\n",
    "device = 'cpu'\n",
    "\n",
    "# replace this part with your project's anchor config\n",
    "anchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n",
    "anchor_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "compound_coef = 0\n",
    "class_num = 2\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=class_num,\n",
    "                             ratios=anchor_ratios, scales=anchor_scales)\n",
    "model.load_state_dict(torch.load(modelname, map_location=torch.device(device) ))\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "# def from_s3_to_tmp(event):\n",
    "def from_s3_to_tmp(bucket, path):\n",
    "#     bucket = event['bucket']\n",
    "#     path = event['s3_path']\n",
    "    \n",
    "    if 'png' in path:\n",
    "        filename = '/tmp/input.png'\n",
    "    elif 'jpg' in path or 'jpeg' in path:\n",
    "        filename =  '/tmp/input.jpg'\n",
    "    elif 'pth' in path:\n",
    "        filename = '/tmp/model.pth'\n",
    "    else:\n",
    "        print('[ERROR] wrong file type!')\n",
    "        return ''\n",
    "\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    bucket.download_file(path, filename)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def gen_result(preds):\n",
    "    if len(preds[0]['rois']) == 0:\n",
    "        return {'bbox':[]}\n",
    "\n",
    "    bbox = []\n",
    "    for j in range(len(preds[0]['rois'])):\n",
    "        x1, y1, x2, y2 = preds[0]['rois'][j].astype(np.int)\n",
    "        cls = preds[0]['class_ids'][j]\n",
    "        score = float(preds[0]['scores'][j])\n",
    "        bbox.append({'x1':x1, 'y1':y1, 'x2':x2, 'y2':y2, 'score':score, 'classid':cls})\n",
    "\n",
    "    return bbox\n",
    "\n",
    "def handler(event, context):\n",
    "    \n",
    "    \n",
    "    threshold = 0.2\n",
    "    \n",
    "    global version\n",
    "    global modelname\n",
    "    global model\n",
    "    global class_num\n",
    "    global device\n",
    "\n",
    "    if 's3_path' not in event or 'bucket' not in event:\n",
    "        return {'result': False}\n",
    "    \n",
    "    filename = from_s3_to_tmp(event['bucket'], event['s3_path'])\n",
    "    \n",
    "    if filename == '':\n",
    "        return {'result': False}\n",
    "    \n",
    "    \n",
    "    if 'model_bucket' in event and 'model_s3_path' in event:\n",
    "        if 'class_num' in event:\n",
    "            class_num = int(event['class_num'])\n",
    "        if 'version' in event:\n",
    "            if version != event['version']:\n",
    "                print('[INFO] update model: ', event['model_bucket'], event['model_s3_path'])\n",
    "                modelname = from_s3_to_tmp(event['model_bucket'], event['model_s3_path'])\n",
    "                version = event['version']\n",
    "                model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=class_num,\n",
    "                             ratios=anchor_ratios, scales=anchor_scales)\n",
    "                model.load_state_dict(torch.load(modelname, map_location=torch.device(device) ))\n",
    "                model.requires_grad_(False)\n",
    "                model.eval()\n",
    "            \n",
    "    if 'threshold' in event:\n",
    "        threshold = float(event['threshold'])\n",
    "    \n",
    "    \n",
    "    force_input_size = None  # set None to use default size\n",
    "\n",
    "    img_path = tuple([filename])\n",
    "\n",
    "    iou_threshold = 0.2\n",
    "\n",
    "    use_cuda = False\n",
    "    use_float16 = False\n",
    "    cudnn.fastest = False\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "    color_list = standard_to_bgr(STANDARD_COLORS)\n",
    "    # tf bilinear interpolation is different from any other's, just make do\n",
    "    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "    input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "    ori_imgs, framed_imgs, framed_metas = preprocess(*img_path, max_size=input_size)\n",
    "\n",
    "    x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "#     pt1 = time.time()\n",
    "#     model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=class_num,\n",
    "#                                  ratios=anchor_ratios, scales=anchor_scales)\n",
    "#     model.load_state_dict(torch.load(modelname, map_location=torch.device(device) ))\n",
    "#     pt2 = time.time()\n",
    "#     model.requires_grad_(False)\n",
    "#     model.eval()\n",
    "\n",
    "    if use_float16:\n",
    "        model = model.half()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pt3 = time.time()\n",
    "        features, regression, classification, anchors = model(x)\n",
    "        pt4 = time.time()\n",
    "        \n",
    "        regressBoxes = BBoxTransform()\n",
    "        clipBoxes = ClipBoxes()\n",
    "\n",
    "        out = postprocess(x,\n",
    "                      anchors, regression, classification,\n",
    "                      regressBoxes, clipBoxes,\n",
    "                      threshold, iou_threshold)\n",
    "        \n",
    "    out = invert_affine(framed_metas, out)\n",
    "    out = gen_result(out)\n",
    "    \n",
    "    result = json.dumps(out, cls=MyEncoder)\n",
    "    pt5 = time.time()\n",
    "\n",
    "    print(result)\n",
    "#     print('pt2-pt1 (load model): ', str(pt2-pt1))\n",
    "    print('pt4-pt3 (inference): ', str(pt4-pt3))\n",
    "    print('pt5-pt4 (post process): ', str(pt5-pt4))\n",
    "    \n",
    "    return {\n",
    "        'statusCode'        : 200,\n",
    "        'result':result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e229f1",
   "metadata": {},
   "source": [
    "## AWS Lambda で推論\n",
    "\n",
    "ここからは、AWS Lambda で推論を実行するための手順を行なってきます。\n",
    "\n",
    "### コンテナイメージのビルドと Amazon ECR へのプッシュ\n",
    "\n",
    "コンテナやソースコードの動作確認ができたら、次はそれらを Lambda にデプロイします。以下のセルを実行して、Amazon ECR のリポジトリ名などを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd181e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository_lambda_inference = 'efficientdet-lambda-inference'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "inference_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_lambda_inference + tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e04ea",
   "metadata": {},
   "source": [
    "以下のセルを実行して、コンテナイメージをビルドして ECR にプッシュします。ノートブックインスタンスで Lambda 用コンテナの動作確認をした際にイメージのビルドは完了しているので、この処理はイメージを　ECR　にプッシュするための 1分ほどで完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537615c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_lambda_inference docker/lambda-inference\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_lambda_inference\n",
    "!docker tag {ecr_repository_lambda_inference + tag} $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b036f",
   "metadata": {},
   "source": [
    "### コンテナイメージを AWS Lambda にデプロイ\n",
    "\n",
    "ここからは、ノートブックインスタンスを離れて、AWS Lambda のコンソール操作になります。AWS コンソールから AWS Lambda のコンソールにアクセスしてください。その後、以下の手順を実施して先ほど作成したコンテナイメージが動作する Lambda 関数を作成してください。\n",
    "\n",
    "1. AWS Lambda コンソールで、「関数の作成」をクリック\n",
    "1. 「以下のいずれかのオプションを選択して、関数を作成します。」と書かれた部分で「コンテナイメージ」を選択\n",
    "1. 「関数名」に任意の関数名を入力\n",
    "1. 「イメージを参照」ボタンをクリックして先ほど作成した Lambda 用のコンテナイメージ（efficientdet-lambda-inference）を選択<br>\n",
    "「Amazon ECR イメージリポジトリ」のプルダウンメニューに作成したはずのリポジトリがなかったり、コンテナイメージがない場合は一つ上のセルの実行時に何らかのエラーが出ている可能性があるので確認してください。**モデルの学習の際に使用したコンテナを選ばないようにご注意ください**\n",
    "1. 「関数の作成」をクリック\n",
    "1. 作成した関数名をクリックしてから「設定」タブをクリック\n",
    "1. 左側のメニューで「一般設定」を選択し、「編集」ボタンをクリック\n",
    "1. 「メモリ」を512 MB に変更、「タイムアウト」を 1分 に変更して「保存」ボタンをクリック\n",
    "1. 左側のメニューで「アクセス権限」をクリックして、「ロール名」のリンクをクリック（IAM のコンソールが別タブで開く）\n",
    "1. 表示されたロールに「AmazonS3FullAccess」ポリシーをアタッチ\n",
    "\n",
    "一度関数を作成したあとでコンテナイメージを更新したい場合は、作成した関数のコンソール画面で「イメージ」タブを選択し、「新しいイメージをデプロイ」をクリックして更新したいコンテナイメージを選択してください。\n",
    "\n",
    "### 作成した Lambda 関数を実行\n",
    "\n",
    "推論したい画像を S3 にアップロードしてから以下のセルを実行して、Lambda を使った推論を行います。以下のセルの 'bucket_name', 'image_path', lambda_function_name' をご自身の環境に合わせて書き換えてください。Lambda のメモリが不足することがあるので推論の入力として指定する画像サイズは短辺が 1000画素程度になるようリサイズしてください。\n",
    "\n",
    "実行結果のログに`{'statusCode': 200, 'result': '[{\"x1\": 366, \"y1\": 853, \"x2\": 627, \"y2\": 974, \"score\": 0.2866290807723999, \"classid\": 0}]'}` のようなテキストが表示されていれば OK です。推論結果を反映させた画像が、`img_inferred_dn.jpg` ようなファイル名で保存されます。\n",
    "\n",
    "#### モデルの更新\n",
    "推論に使用するモデルを更新したい場合は、あらかじめ更新したいモデルファイル（model.pth） を S3 にアップロードし、Lambda 関数を Invoke する際に、引数で version（モデルのバージョン）、model_bucket（モデルを保存している S3 バケット名）、model_s3_path（モデルを保存しているバケット名以下のパス）を指定すると、指定されたモデルを使った推論結果を取得できるようになります。不要なダウンロードを回避するために、関数が記憶している version と異なる version が指定された場合のみ指定されたモデルを S3 からダウンロードするようにしているので、モデルを更新したい場合は必ず適切な version を指定してください。version、model_bucket、model_s3_path を指定しなくても関数を実行することは可能ですが、更新されたモデルを使って推論されるかデフォルトのモデルを使って推論されるかはタイミングに依存します（コールドスタートになった場合はデフォルトのモデルが使用されます）のでご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./access.py\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import cv2\n",
    "from utils.utils import plot_one_box, standard_to_bgr, STANDARD_COLORS, get_index_label\n",
    "\n",
    "#--------------------------------\n",
    "# 推論したい画像が格納された S3 バケット名\n",
    "bucket_name = 'bucket'\n",
    "\n",
    "# 推論したい画像が格納された S3 バケット名以下のパス\n",
    "image_path = 'dir/xxx.jpg'\n",
    "\n",
    "# Lambda 関数名\n",
    "lambda_function_name = 'efficientdet-inference'\n",
    "#--------------------------------\n",
    "# 分類クラスのリスト（今まで同じ順番で）\n",
    "obj_list = labels\n",
    "# coefficient（今までと同じ値）\n",
    "compound_coef = 0\n",
    "#--------------------------------\n",
    "\n",
    "\n",
    "def save_image(bboxes, img):\n",
    "    color_list = standard_to_bgr(STANDARD_COLORS)\n",
    "    \n",
    "    for b in bboxes:\n",
    "        x1= b['x1']\n",
    "        y1= b['y1']\n",
    "        x2= b['x2']\n",
    "        y2= b['y2']\n",
    "        obj = obj_list[b['classid']]\n",
    "        score = b['score']\n",
    "        plot_one_box(img, [x1, y1, x2, y2], label=obj,score=score,color=color_list[get_index_label(obj, obj_list)])\n",
    "\n",
    "    ofilename = 'img_inferred_d'+str(compound_coef)+'.jpg'\n",
    "    print('Image was saved as ', ofilename)\n",
    "    cv2.imwrite(ofilename, img)\n",
    "\n",
    "start = time.time()\n",
    "l = boto3.Session().client('lambda').invoke(\n",
    "    FunctionName=lambda_function_name,\n",
    "    InvocationType='RequestResponse', # Event or RequestResponse\n",
    "#     Payload=json.dumps({'bucket': bucket_name, 's3_path': image_path, 'model_bucket': bucket_name, 'version': '1.1', 'model_s3_path': 'data/efficientdet/awscats/model.pth'})\n",
    "    Payload=json.dumps({'bucket': bucket_name, 's3_path': image_path})\n",
    ")\n",
    "j = json.loads(l['Payload'].read())\n",
    "print('Time for inference: ', round(time.time()-start, 2), ' sec.')\n",
    "\n",
    "print(j)\n",
    "bbox = json.loads(j['result'])\n",
    "\n",
    "tmp_img = os.path.basename(image_path)\n",
    "!aws s3 cp 's3://'$bucket_name/$image_path ./\n",
    "\n",
    "img = cv2.imread(tmp_img)\n",
    "save_image(bbox, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cf286",
   "metadata": {},
   "source": [
    "以下のセルを実行して、出力された画像を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ee37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(mpimg.imread(\"img_inferred_d0.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b64099",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "\n",
    "このノートブックを実行するのに使用したノートブックインスタンスや、ノートブックで作成したものを削除して不要な課金を回避してください。このノートブックで作成したものは以下のとおりです。今後使用しないことを確認の上削除してください。\n",
    "\n",
    "1. `sagemaker-<region>-<account ID>` という名前の S3 バケット\n",
    "1. 学習データをアップロードした S3 バケット\n",
    "1. Amazon ECR のリポジトリとイメージ\n",
    "1. AWS Lambda 関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d190abfa",
   "metadata": {},
   "source": [
    "## [Option] Lambdaにデプロイする前にローカルで動作確認\n",
    "\n",
    "モデルを Lambda にデプロイする前にローカルで動作確認をしておきたい場合、こちらのツールキットのリポジトリをクローンします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/aws/aws-lambda-python-runtime-interface-client.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f1e88",
   "metadata": {},
   "source": [
    "### ノートブックインスタンス上でコンテナとソースコードの動作確認\n",
    "\n",
    "以下のコマンドを実行して、ノートブックインスタンス上で関数のデバッグに使用するツールをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.aws-lambda-rie && \\\n",
    "    curl -Lo ~/.aws-lambda-rie/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie && \\\n",
    "    chmod +x ~/.aws-lambda-rie/aws-lambda-rie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981af1fb",
   "metadata": {},
   "source": [
    "以下のセルを実行して、Lambda 推論用のコンテナイメージをビルドして実行するスクリプトを作成します。コマンドで指定している各種パスはご自身の環境に合わせて書き換えてください。以下のセルはこのサンプル用の記述になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./build_and_run.sh\n",
    "\n",
    "docker build -t efficientdet-lambda-inference docker/lambda-inference\n",
    "docker run --name lambda --rm -p 9000:8080 \\\n",
    "          -v ~/.aws-lambda-rie:/aws-lambda \\\n",
    "            --entrypoint /aws-lambda/aws-lambda-rie \\\n",
    "             efficientdet-lambda-inference \\\n",
    "                /usr/local/bin/python -m awslambdaric app.handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a3342",
   "metadata": {},
   "source": [
    "Terminal で、作成したスクリプトを実行します。\n",
    "**以下のセルを実行する前に以下の手順を実施してください。**\n",
    "\n",
    "1. Jupyter のファイルブラウザのブラウザタブ（このノートブックのタブの左側にあることが多い）を表示し、右上にある New -> Terminal をクリック\n",
    "1. Terminal が表示されたら以下のコマンドを実行<br>\n",
    "```\n",
    "$ cd SageMaker\n",
    "$ sh build_and_run.sh\n",
    "```\n",
    "\n",
    "すると、コンテナイメージが実行されます。ログの表示が停止したら推論リクエストの待機状態なので、以下のセルを実行します。`{\"statusCode\": 200, \"result\": \"[{\\\"x1\\\": 905, \\\"y1\\\": 1458, \\\"x2\\\": 2004, \\\"y2\\\": 1845, \\\"score\\\": 0.5928193926811218, \\\"classid\\\": 0}]\"}` のようなログが表示されたら成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47914db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"bucket\": \"bucket_name\", \"s3_path\": \"path/frame_0003.jpeg\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b34e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
