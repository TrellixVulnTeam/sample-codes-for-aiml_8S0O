{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker で PyTorch の GNN を使ったノード分類を行う\n",
    "このサンプルノートブックは、[PyTorch geometric のサンプルコード](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)を参考にしました。\n",
    "\n",
    "## Node Classification with Graph Neural Networks\n",
    "\n",
    "[Previous: Introduction: Hands-on Graph Neural Networks](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8)\n",
    "\n",
    "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
    "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
    "\n",
    "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
    "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
    "Two documents are connected if there exists a citation link between them.\n",
    "The task is to infer the category of each document (7 in total).\n",
    "\n",
    "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
    "We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):\n",
    "\n",
    "\n",
    "## 準備\n",
    "\n",
    "**このサンプルでは、カスタムコンテナを Amazon ECR に push する必要があります。**以下の操作でこのノートブックインスタンスで使用している IAM ロールに Amazon ECR にイメージを push するための権限を追加してください。\n",
    "\n",
    "1. Amazon SageMaker コンソールからこのノートブックインスタンスの詳細画面を表示<br>（左側のメニューのインスタンス -> ノートブックインスタンス -> インスタンス名をクリック）\n",
    "1. 「アクセス許可と暗号化」の「IAM ロール ARN」のリンクをクリック（IAM のコンソールに遷移します）\n",
    "1. 「ポリシーをアタッチします」と書いてある青いボタンをクリック\n",
    "1. 検索ボックスに ec2containerregistry と入力し  AmazonEC2ContainerRegistryFullAccess のチェックボックスをチェックする\n",
    "1. 「ポリシーのアタッチ」と書いてある青いボタンをクリック\n",
    "\n",
    "以下のセルでは、Amazon SageMaker を使うためのセットアップを行います。ロールの情報、ノートブックインスタンスのリージョン、アカウントID などの情報を取得しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_output = session.default_bucket()\n",
    "s3_prefix = 'gnn-byo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker\n",
    "!mkdir docker/processing\n",
    "!mkdir docker/train\n",
    "!mkdir docker/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/processing/requirements.txt\n",
    "\n",
    "boto3==1.17.35\n",
    "torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cpu.html\n",
    "torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cpu.html\n",
    "torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cpu.html\n",
    "torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cpu.html\n",
    "torch-geometric==1.6.3\n",
    "matplotlib==3.3.4\n",
    "scikit-learn==0.24.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp docker/processing/requirements.txt docker/train/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Experiments のセットアップ\n",
    "Amazon SageMaker Experiments のライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理用、学習用の Expetiments を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "import time\n",
    "\n",
    "gnn_experiment_preprocess = Experiment.create(\n",
    "    experiment_name=f\"gnn-byo-preprocess-{int(time.time())}\", \n",
    "    description=\"node classification using gnn (preprocess)\", \n",
    "    sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "print(gnn_experiment_preprocess)\n",
    "\n",
    "gnn_experiment_train = Experiment.create(\n",
    "    experiment_name=f\"gnn-byo-train-{int(time.time())}\", \n",
    "    description=\"node classification using gnn (train)\", \n",
    "    sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "print(gnn_experiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このサンプルノートブックでは、データの前処理、前処理したデータを使ってモデルの学習、学習済みモデルを使ってバッチ推論、の順でおこないます。\n",
    "\n",
    "これから 2種類のコンテナイメージを作成して Amazon ECR に push します。1つめのコンテナイメージはデータの前処理とバッチ推論で使用し、2つめのコンテナイメージはモデルの学習で使用します。\n",
    "\n",
    "## データの前処理\n",
    "データの前処理は Amazon SageMaker Processing の仕組みを使って行います。まずは前処理用のコンテナイメージを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = 'gnn-byo-proc'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/processing/Dockerfile\n",
    "\n",
    "FROM python:3.8-buster\n",
    "\n",
    "WORKDIR /opt/app\n",
    "\n",
    "RUN pip3 install torch==1.8.0\n",
    "\n",
    "COPY requirements.txt /opt/app\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "RUN pip3 install -U torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cpu.html\n",
    "RUN pip3 install jupyter\n",
    "\n",
    "COPY . /opt/app\n",
    "\n",
    "EXPOSE 8888\n",
    "\n",
    "# jupyter notebook --allow-root --ip=* --no-browser -NotebookApp.token=''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記 Dockerfile を使ってコンテナイメージをビルドし、Amazon ECR に push します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker/processing\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したイメージを使って ScriptProcessor を作成します。このとき、`instance_type` に　`local` を設定するとローカルモードになり、ノートブックインスタンス上で Processing Job が実行されます。作成したコンテナイメージやスクリプトのデバッグをする際は、ローカルモードの利用がおすすめです。デバッグが完了したら、`instance_type` に　インスタンスタイプを設定して Processing Job を実施します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=processing_repository_uri,\n",
    "                                   role=role,\n",
    "                                   sagemaker_session=session,\n",
    "                                   instance_count=1,\n",
    "#                                    instance_type='local')\n",
    "                                   instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Job で使用するスクリプトを作成します。前処理の内容を変更した場合は、前処理スクリプトを更新してから 2つしたのセル（script_processor.run）を再度実行すれば OK です。コンテナイメージの再作成は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/app')\n",
    "\n",
    "import boto3\n",
    "\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    aws_session = boto3.Session(profile_name=None)\n",
    "\n",
    "    dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "    \n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('======================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Number of features: {dataset.num_features}')\n",
    "    print(f'Number of classes: {dataset.num_classes}')\n",
    "    \n",
    "    data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    # Gather some statistics about the graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "    print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "    \n",
    "    # save to container directory for uploading to S3\n",
    "    \n",
    "    import os\n",
    "\n",
    "    path = \"./\"\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    \n",
    "    \n",
    "    \n",
    "    src = 'data/Planetoid/Cora'\n",
    "    dist = '/opt/ml/processing/output/Cora'\n",
    "    \n",
    "    print(os.path.getsize(src))\n",
    "    \n",
    "    import tarfile\n",
    "\n",
    "    # 圧縮\n",
    "    with tarfile.open('sample.tar.gz', 'w:gz') as t:\n",
    "        t.add(src)\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    shutil.copytree(src, dist)\n",
    "    \n",
    "    from torch_geometric.io import read_planetoid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したスクリプトを使って `run` を実行して  Processing Job を起動します。`run` の引数には以下を設定しています。\n",
    "\n",
    "- code: 処理スクリプトのファイル名\n",
    "- inputs: （入力データがある場合）入力データが保存されている Amazon S3 パスを `source` に、Processing 用インスタンスのどこに入力データをダウンロードするかを `destination` に設定します。今回はインターネット経由でデータをダウンロードするため使用しません。\n",
    "- outputs: 出力データを保存する Processing 用インスタンスのパスを `source` で指定し、そこに処理済みのデータなどを保存しておくと、`destination` に設定した S3 パスにそれらのデータが自動的にアップロードされます。\n",
    "- experiment_config: Processing Job を登録する Experiments があれば、その情報を指定します。\n",
    "\n",
    "**以下をローカルモードで実行すると、最後に  `PermissionError: [Errno 13] Permission denied: 'ind.cora.tx'` というエラーが出ますが、これはジョブがうまく動いていても出るので無視して構いません。インスタンスを使用した場合はこのエラーは出ません。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime \n",
    "\n",
    "processing_job_name = \"gnn-byo-process-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/data'.format(s3_output, s3_prefix)\n",
    "\n",
    "script_processor.run(code='preprocessing.py',\n",
    "                      job_name=processing_job_name,\n",
    "#                       inputs=[ProcessingInput(\n",
    "#                         source=raw_s3,\n",
    "#                         destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='output',\n",
    "                                                destination='{}/output'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/output')],\n",
    "                      experiment_config={\n",
    "                            \"ExperimentName\": gnn_experiment_preprocess.experiment_name,\n",
    "                            \"TrialComponentDisplayName\": \"Processing\",\n",
    "                      }\n",
    "                               )\n",
    "\n",
    "preprocessing_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習\n",
    "\n",
    "ここまでで、データの前処理と、前処理済みデータの Amazon S3 へのアップロードが完了しました。次は、前処理済みのデータを使って GNN を学習します。\n",
    "\n",
    "まずは学習用コンテナイメージを作成します。ベースイメージに、Amazon SageMaker が用意している PyTorch 1.8.0 のイメージを使用しました。\n",
    "\n",
    "**この Dockerfile はノートブックインスタンスが `us-east-1 (バージニア北部)` の想定なので、他のリージョンをお使いの場合は FROM に書かれている Amazon ECR の URI の `us-east-1` の部分をお使いのリージョンに合わせて書き換えてください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/train/Dockerfile\n",
    "# FROM python:3.8-buster\n",
    "FROM  763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.8.0-cpu-py36-ubuntu18.04\n",
    "\n",
    "WORKDIR /opt/app\n",
    "\n",
    "RUN pip3 install torch==1.8.0\n",
    "\n",
    "COPY requirements.txt /opt/app\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "RUN pip3 install -U torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cpu.html\n",
    "RUN pip3 install jupyter\n",
    "\n",
    "\n",
    "RUN pip3 install sagemaker-training\n",
    "\n",
    "WORKDIR /\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = 'gnn-byo-train'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "train_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベースイメージは Amazon SageMaker が用意している Amazon ECR リポジトリに保存されているため、そこへのアクセス権が必要です。以下のコマンドを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids 763104351884 --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習スクリプトを作成します。学習スクリプトの内容を変更した場合は、`pytorch_estimator.fit()` を再度実行すれば OK です。学習スクリプトをコンテナイメージの中に入れておらず、Estimator 経由でコンテナに渡すようにしているため、コンテナイメージの再作成は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "def _save_checkpoint(model, optimizer, epoch, loss, args):\n",
    "#     print(\"epoch: {} - loss: {}\".format(epoch+1, loss))\n",
    "    checkpointing_path = args.checkpoint_path + '/checkpoint.pth'\n",
    "    print(\"Saving the Checkpoint: {}\".format(checkpointing_path))\n",
    "    torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        }, checkpointing_path)\n",
    "    \n",
    "def _load_checkpoint(model, optimizer, args):\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Checkpoint file found!\")\n",
    "    print(\"Loading Checkpoint From: {}\".format(args.checkpoint_path + '/checkpoint.pth'))\n",
    "    checkpoint = torch.load(args.checkpoint_path + '/checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch_number = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"Checkpoint File Loaded - epoch_number: {} - loss: {}\".format(epoch_number, loss))\n",
    "    print('Resuming training from epoch: {}'.format(epoch_number+1))\n",
    "    print(\"--------------------------------------------\")\n",
    "    return model, optimizer, epoch_number\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--features-num', type=int, default=64, metavar='N',\n",
    "                        help='input feature size (default: 64)')\n",
    "    parser.add_argument('--classes-num', type=int, default=1, metavar='N',\n",
    "                        help='input class size (default: 1)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument(\"--checkpoint-path\",type=str,default=\"/opt/ml/checkpoints\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = GCN(hidden_channels=16, num_features=args.features_num, num_classes=args.classes_num)\n",
    "    print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    path = args.data_dir\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    \n",
    "    from torch_geometric.io import read_planetoid_data\n",
    "    data = read_planetoid_data(args.data_dir, 'Cora')\n",
    "    \n",
    "    # Check if checkpoints exists\n",
    "    if not os.path.isfile(args.checkpoint_path + '/checkpoint.pth'):\n",
    "        epoch_number = 0\n",
    "    else:    \n",
    "        model, optimizer, epoch_number = _load_checkpoint(model, optimizer, args) \n",
    "\n",
    "    for epoch in range(epoch_number, int(args.epochs)+1):\n",
    "        loss = train()\n",
    "        acc = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Acc: {acc:.4f}')\n",
    "        \n",
    "        if (epoch %100 == 0):\n",
    "            _save_checkpoint(model, optimizer, epoch, loss, args)\n",
    "        \n",
    "    torch.save(model.state_dict(), args.model_dir+'/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker/train\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $train_repository_uri\n",
    "!docker push $train_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし、上記コマンドでコンテナイメージをビルドする際に no space left というエラーが出ていたら、以下のコマンドのコメントアウトを解除して実行し、不要なファイルを削除してから再度コンテナイメージのビルドを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !docker system prune -a -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator を作成して `fit` で学習ジョブを起動します。ハイパーパラメタの設定や取得したいメトリクスの情報を指定することができます。Processing Job と同様にローカルモードを使用することができます。`fit` の引数には、学習データが保存されている S3 のパスを指定します。PyTorch の Estimator については [こちらのドキュメント](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html#sagemaker.pytorch.estimator.PyTorch) をご参照ください。今回 PyTorch という名前の Estimator を使用しましたが、コンテナイメージの中に学習スクリプトを含めた状態で使用する場合は、Estimator という名前の Estimator を使用してください。\n",
    "\n",
    "Estimator の `metric_definitions` に記録したいメトリクスの情報を指定することができます。`Regex` には、学習スクリプトが出力するログから取得したい数値を抽出するための正規表現を指定します。つまりメトリクスを記録したい場合は、学習スクリプトがメトリクスに関する情報をログに出力する必要があります。今回は Loss と Acc をメトリクスとして取得するよう設定しています。\n",
    "\n",
    "Spot Instanceを用いて実行する場合は、下記のコードを Estimator の `instance_type`の次の行あたりに追加します。なお、`max_wait` は、`max_run` 以上の値である必要があります。\n",
    "\n",
    "```python\n",
    "max_run = 5000,\n",
    "use_spot_instances = 'True',\n",
    "max_wait = 10000,\n",
    "```\n",
    "\n",
    "チェックポイントの利用は必須ではありませんが、Spot Instance を使う場合は中断に備えてチェックポイントを有効にすることが推奨されています。チェックポイントの学習インスタンス上の保存パス（checkpoint_local_path）と、それらをアップロードする先のパス（checkpoint_s3_path）を設定し、学習スクリプトにチェックポイントを checkpoint_local_path に保存する記述を追加します。\n",
    "\n",
    "保存したチェックポイントから学習を再開する場合は、新しく Estimator 定義して引数にチェックポイントが保存してある checkpoint_s3_path と チェックポイントをダウンロードしたいパス checkpoint_local_path を設定して fit を実行します。\n",
    "\n",
    "チェックポイントの詳細については [こちらのドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html#model-checkpoints-enable) をご参照ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "# Spot training をする場合は、チェックポイントの設定を推奨\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_path = 's3://{}/checkpoint-{}'.format(bucket, checkpoint_suffix)\n",
    "checkpoint_local_path=\"/opt/ml/checkpoints\"\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "                        entry_point='train.py',\n",
    "                        image_uri=train_repository_uri,\n",
    "                        role=role, \n",
    "                        instance_count=1,\n",
    "#                         instance_type='local',\n",
    "                        instance_type='ml.c4.2xlarge',\n",
    "                        max_run = 5000,\n",
    "                        use_spot_instances = 'True',\n",
    "                        max_wait = 10000,\n",
    "                        checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                        checkpoint_local_path=checkpoint_local_path,\n",
    "                        output_path=\"s3://{}/output\".format(bucket),\n",
    "                        sagemaker_session=session,\n",
    "                        hyperparameters = {'epochs': 200, 'features-num':1433, 'classes-num':7, 'lr':0.01},\n",
    "                        enable_sagemaker_metrics=True,\n",
    "                        metric_definitions = [dict(\n",
    "                                                                Name = 'Loss',\n",
    "                                                                Regex = 'Loss: ([0-9.]+)'\n",
    "                                                            ),\n",
    "                                                              dict(\n",
    "                                                                Name = 'Acc',\n",
    "                                                                Regex = 'Acc: ([0-9.]+)'\n",
    "                                                            )\n",
    "                                             ]\n",
    "                          \n",
    ")\n",
    "\n",
    "pytorch_estimator.fit({'train': os.path.join(output_destination, 'output/Cora/raw/')},\n",
    "                     experiment_config={\n",
    "                            \"ExperimentName\": gnn_experiment_train.experiment_name,\n",
    "                            \"TrialComponentDisplayName\": \"Training\",\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Experiments でモデルを比較\n",
    "\n",
    "SageMaker Experiments を使うと複数のモデルのメトリクスなどを比較することができます。上のセルの Estimator の引数で epochs や lr などのハイパーパラメタを変えて何度か学習を実行してから次のセル以降を実行してみましょう。Experiments 内の Trial のフィルタやソートなど方法については [ExperimentAnalytics のドキュメント](https://sagemaker.readthedocs.io/en/stable/api/training/analytics.html#sagemaker.analytics.ExperimentAnalytics) をご参照ください。\n",
    "\n",
    "メトリクスに関して、DataFrame の列名は Loss - Min などと書かれていますが、ExperimentAnalytics の sort_by で Loss - Min を指定する場合は、metrics.loss.min となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=session, \n",
    "    experiment_name=gnn_experiment_train.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.acc.max\",\n",
    "    sort_order=\"Ascending\",# Ascending or Descending\n",
    "    metric_names=['Loss', 'Acc'],\n",
    "    parameter_names=['epochs', 'lr'],\n",
    "    input_artifact_names=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = trial_component_analytics.dataframe()\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Job を使ったバッチ推論\n",
    "\n",
    "学習したモデルを使ってバッチ推論を行います。今回は、前処理で使用したコンテナイメージを流用してバッチ推論用 Processing Job を起動します。\n",
    "\n",
    "まずは推論用スクリプトを作成します。<br>\n",
    "推論結果をグラフにプロットし、その画像を Amazon S3 にアップロードするようにしました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color, path):\n",
    "    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "#     plt.show()\n",
    "    fig.savefig(os.path.join(path, \"img.png\"))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--features-num', type=str, default='1', metavar='N',\n",
    "                        help='input feature size (default: 1)')\n",
    "    parser.add_argument('--classes-num', type=str, default='1', metavar='N',\n",
    "                        help='input class size (default: 1)')\n",
    "    parser.add_argument('--model-dir', type=str, default='/opt/ml/model', metavar='N',\n",
    "                        help='model data path (default: /opt/ml/model)')\n",
    "    parser.add_argument('--input-dir', type=str, default='/opt/ml/input', metavar='N',\n",
    "                        help='input data path (default: /opt/ml/input)')\n",
    "    parser.add_argument('--output-dir', type=str, default='/opt/ml/output', metavar='N',\n",
    "                        help='output data path (default: /opt/ml/output)')\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    from torch_geometric.io import read_planetoid_data\n",
    "    data = read_planetoid_data(args.input_dir, 'Cora')\n",
    "  \n",
    "    with tarfile.open(os.path.join(args.model_dir, 'model.tar.gz'), 'r:gz') as t:\n",
    "        t.extractall()\n",
    "\n",
    "    model = GCN(hidden_channels=16, num_features=int(args.features_num), num_classes=int(args.classes_num))\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "#     print(model)\n",
    "    \n",
    "    test_acc = test()\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    visualize(out, color=data.y,  path=args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "batch_inference_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=processing_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "#                                    instance_type='local')\n",
    "                                   instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime \n",
    "\n",
    "processing_job_name = \"gnn-byo-batch-inference-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination_inference = 's3://{}/{}/batch-inference'.format(s3_output, s3_prefix)\n",
    "input_dir = '/opt/ml/processing/input'\n",
    "model_dir = '/opt/ml/processing/model'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "model_s3 = pytorch_estimator.model_data\n",
    "raw_s3 = os.path.join(output_destination, 'output/Cora/raw/')\n",
    "\n",
    "batch_inference_processor.run(code='inference.py',\n",
    "                      job_name=processing_job_name,\n",
    "                      inputs=[ProcessingInput(\n",
    "                                        source=model_s3,\n",
    "                                        destination=model_dir),\n",
    "                                     ProcessingInput(\n",
    "                                        source=raw_s3,\n",
    "                                        destination=input_dir)],\n",
    "                      outputs=[ProcessingOutput(output_name='output',\n",
    "                                                destination='{}/output'.format(output_destination_inference),\n",
    "                                                source=output_dir)],\n",
    "                              arguments=['--model-dir', model_dir, '--input-dir', input_dir, '--output-dir', output_dir , '--features-num', '1433', '--classes-num', '7']\n",
    "#                       experiment_config={\n",
    "#                             \"ExperimentName\": gnn_experiment.experiment_name,\n",
    "#                             \"TrialComponentDisplayName\": \"Processing\",\n",
    "#                       }\n",
    "                               )\n",
    "\n",
    "preprocessing_job_description = batch_inference_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ推論で出力したプロットの画像をダウンロードして表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output_destination_inference/output/img.png ./\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"./img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "\n",
    "利用が終わったら、このノートブックを実行したノートブックインスタンスの停止および削除を実施してください。ノートブックインスタンスを停止させると、ノートブックインスタンスの課金は止まりますがアタッチされている EBS ボリュームへの課金が継続しますので、完全に課金を止めるにはノートブックインスタンスの停止だけでなく削除まで実施してください。\n",
    "\n",
    "また、Amazon S3 にアップロードした各種ファイルに対しても課金が発生するため、不要であれば削除してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.Session().client('sagemaker')\n",
    "def cleanup(experiment):\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(sagemaker_boto_client=sm, trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                sagemaker_boto_client=sm,\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # tc is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "    experiment.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(gnn_experiment_preprocess)\n",
    "cleanup(gnn_experiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
