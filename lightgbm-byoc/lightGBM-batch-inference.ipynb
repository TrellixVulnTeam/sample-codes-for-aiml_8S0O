{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "piano-reunion",
   "metadata": {},
   "source": [
    "# LightGBM でバッチ推論を行う\n",
    "## 0. はじめに\n",
    "カスタムコンテナで LightGBM のバッチ推論をするサンプルノートブックです。モデルの学習とリアルタイム推論については [こちらのノートブック（英語）](https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/lightgbm-byo/lightgbm-byo.ipynb) をご参照ください。\n",
    "\n",
    "このノートブックでは、Transform API を使ったバッチ推論と、Amazon SageMaker Processing を使ったバッチ推論の方法をご紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for inference sources\n",
    "!mkdir -p docker-inference-transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "session = sagemaker.Session()\n",
    "s3_output = session.default_bucket()\n",
    "s3_prefix = 'lightGBM-BYO'\n",
    "tag = ':latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-edward",
   "metadata": {},
   "source": [
    "## 1. Transform API を使用してバッチ推論する\n",
    "### 1-1. 推論用スクリプトの準備\n",
    "\n",
    "[Multi Model Server (MMS)](https://github.com/awslabs/multi-model-server) を使ってバッチ推論を行います。MMS は ModelHandler のなかでデータの前処理を行い、学習済みモデルを使って推論を行い、出力の後処理を行います。以下は inferennce script です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference-transform/model_script.py\n",
    "\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sagemaker_inference import content_types, encoder\n",
    "\n",
    "NUM_FEATURES = 12\n",
    "\n",
    "class ModelHandler(object):\n",
    "    \"\"\"\n",
    "    A lightGBM Model handler implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.model = None\n",
    "\n",
    "    def initialize(self, context):\n",
    "        \"\"\"\n",
    "        Initialize model. This will be called during model loading time\n",
    "        :param context: Initial context contains model server system properties.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.initialized = True\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\") \n",
    "        self.model = lgb.Booster(model_file=os.path.join(model_dir,'online_shoppers_model.txt'))\n",
    "\n",
    "    def preprocess(self, request):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        :param request: list of raw requests\n",
    "        :return: list of preprocessed model input data\n",
    "        \"\"\"        \n",
    "        payload = request[0]['body']\n",
    "        data = payload.decode('utf-8').splitlines()\n",
    "        data = np.array(list(map(lambda a:list(map(float,a.split(','))), data)))\n",
    "        return data\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        \"\"\"\n",
    "        Internal inference methods\n",
    "        :param model_input: transformed model input data list\n",
    "        :return: list of inference output in numpy array\n",
    "        \"\"\"\n",
    "        prediction = self.model.predict(model_input)\n",
    "        return prediction\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"\n",
    "        Post processing step - converts predictions to str\n",
    "        :param inference_output: predictions as numpy\n",
    "        :return: list of inference output as string\n",
    "        \"\"\"\n",
    "\n",
    "        return [str(inference_output.tolist())]\n",
    "        \n",
    "    def handle(self, data, context):\n",
    "        \"\"\"\n",
    "        Call preprocess, inference and post-process functions\n",
    "        :param data: input data\n",
    "        :param context: mms context\n",
    "        \"\"\"\n",
    "        \n",
    "        model_input = self.preprocess(data)\n",
    "        model_out = self.inference(model_input)\n",
    "        return self.postprocess(model_out)\n",
    "\n",
    "_service = ModelHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(context)\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    return _service.handle(data, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-southeast",
   "metadata": {},
   "source": [
    "## 1-2. MMS を開始するためのスクリプトの準備\n",
    "\n",
    "以下のスクリプトでは inference toolkit を import し、`model_server.start_model_server` 関数を呼び出して MMS を開始します。この関数は先ほど作成した inference script をモデルサーバに渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference-transform/dockerd-entrypoint.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "import os\n",
    "from retrying import retry\n",
    "from subprocess import CalledProcessError\n",
    "from sagemaker_inference import model_server\n",
    "\n",
    "def _retry_if_error(exception):\n",
    "    return isinstance(exception, CalledProcessError or OSError)\n",
    "\n",
    "@retry(stop_max_delay=1000 * 50,\n",
    "       retry_on_exception=_retry_if_error)\n",
    "def _start_mms():\n",
    "    # by default the number of workers per model is 1, but we can configure it through the\n",
    "    # environment variable below if desired.\n",
    "    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n",
    "    model_server.start_model_server(handler_service='/home/model-server/model_script.py:handle')\n",
    "\n",
    "def main():\n",
    "    if sys.argv[1] == 'serve':\n",
    "        _start_mms()\n",
    "    else:\n",
    "        subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n",
    "\n",
    "    # prevent docker exit\n",
    "    subprocess.call(['tail', '-f', '/dev/null'])\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-condition",
   "metadata": {},
   "source": [
    "## 1-3. 推論用コンテナの作成\n",
    "\n",
    "必要なライブラリと先ほど作成した 2つのスクリプトを含むコンテナを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference-transform/Dockerfile\n",
    "\n",
    "FROM ubuntu:18.04\n",
    "    \n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "# Install necessary dependencies for MMS and SageMaker Inference Toolkit\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    openjdk-8-jdk-headless \\\n",
    "    python3-dev \\\n",
    "    curl \\\n",
    "    vim \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && curl -O https://bootstrap.pypa.io/get-pip.py \\\n",
    "    && python3 get-pip.py\n",
    "\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
    "RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
    "    \n",
    "RUN pip install lightgbm numpy pandas \\ \n",
    "                scikit-learn multi-model-server \\\n",
    "                sagemaker-inference retrying\n",
    "\n",
    "# Copy entrypoint script to the image\n",
    "COPY dockerd-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
    "RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
    "\n",
    "RUN mkdir -p /home/model-server/\n",
    "\n",
    "# Copy the default custom service file to handle incoming data and inference requests\n",
    "COPY model_script.py /home/model-server/model_script.py\n",
    "\n",
    "# Define an entrypoint script for the docker image\n",
    "ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
    "\n",
    "# Define command to be passed to the entrypoint\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-excitement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository_inference = 'lightgbm-byo-inference'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "inference_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_inference + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_inference docker-inference-transform\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_inference\n",
    "!docker tag {ecr_repository_inference + tag} $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-space",
   "metadata": {},
   "source": [
    "### 1-4. バッチ推論の実行\n",
    "\n",
    "ここでは、学習済みモデルの `model.tar.gz` を使ってバッチ推論を行います。以下の `model_artifact` に、学習済みモデルの S3 パスを設定してください。`input_data` に入力データが格納された S3 パスを設定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact =  's3://bucket/lightgbm/model.tar.gz'\n",
    "input_data = 's3://bucket/lightGBM-BYO/data/test/csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker import Model, Predictor\n",
    "\n",
    "model_name = 'lightgbm-byo-deployment'\n",
    "\n",
    "lgbm_model = Model(model_data=model_artifact,\n",
    "                   role=role,\n",
    "                   image_uri=inference_repository_uri,\n",
    "                   name=model_name)\n",
    "\n",
    "lgb_transformer = Transformer(model_name=model_name, instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-maldives",
   "metadata": {},
   "source": [
    "Transform API を実行してバッチ推論を開始します。SageMaker コンソールの左側のメニューから 推論 -> バッチ変換ジョブ を選択すると、実行したジョブの詳細を確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_transformer.transform(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-encyclopedia",
   "metadata": {},
   "source": [
    "## 2. Amazon SageMaker Processing を使用してバッチ推論する\n",
    "SageMaker Processing の機能を使ってバッチ推論を実現することもできます。既存の推論用カスタムコンテナがある場合はこちらの方法の方がコードの変更が少ないことが多いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker-inference-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-lawyer",
   "metadata": {},
   "source": [
    "### 2-1. カスタムコンテナの作成\n",
    "Dockerfile を作成し、カスタムコンテナをビルドして Amazon ECR に push します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference-processing/Dockerfile\n",
    "\n",
    "FROM ubuntu:18.04\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    python3-dev \\\n",
    "    curl \\\n",
    "    vim \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && curl -O https://bootstrap.pypa.io/get-pip.py \\\n",
    "    && python3 get-pip.py\n",
    "\n",
    "    \n",
    "RUN pip install lightgbm numpy pandas scikit-learn\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-yield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository_inference = 'lightgbm-byo-batch-inference'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "inference_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_inference + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_inference docker-inference-processing\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_inference\n",
    "!docker tag {ecr_repository_inference + tag} $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-consequence",
   "metadata": {},
   "source": [
    "### 2-1. SageMaker Processing の準備\n",
    "SageMaker Processing のインスタンスのどこに各ファイルを置くかを設定するためのパスを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_input_dir = '/opt/ml/processing/input'\n",
    "processing_model_dir = '/opt/ml/processing/model'\n",
    "processing_output_dir = '/opt/ml/processing/output'\n",
    "job_name = f'sagemaker-lightgbm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "processor = ScriptProcessor(base_job_name=job_name,\n",
    "                                   image_uri=inference_repository_uri,\n",
    "                                   command=['python3'],\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c5.xlarge'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-conclusion",
   "metadata": {},
   "source": [
    "### 2-3. バッチ推論用スクリプトの作成\n",
    "学習済みモデルを使ってバッチ推論するスクリプトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch-inference.py\n",
    "\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-dir', type=str, default=None)\n",
    "    parser.add_argument('--model-dir', type=str, default=None)\n",
    "    parser.add_argument('--output-dir', type=str, default=None)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    \n",
    "    filelist = glob.glob(os.path.join(args.input_dir, '*.csv'))\n",
    "    print(filelist)\n",
    "    \n",
    "    model_dir = args.model_dir\n",
    "    model = lgb.Booster(model_file=os.path.join(model_dir,'online_shoppers_model.txt'))\n",
    "    \n",
    "    for f in filelist:\n",
    "        data = np.loadtxt(f, delimiter=',')\n",
    "        prediction = model.predict(data)\n",
    "        np.savetxt(os.path.join(args.output_dir, os.path.basename(f)+'.out'), prediction, fmt='%f')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-distribution",
   "metadata": {},
   "source": [
    "### 2-4. SageMaker Processing ジョブの実行\n",
    "`model_artifact` に学習済みモデルが保存されている S3 パスを。`inference_data_s3_path` に入力データが保存されている S3 パスを設定して Processing Job を開始します。SageMaker Processing を使ってバッチ推論をする場合は、学習済みモデルを `model.tar.gz` に固める必要はありません。処理結果は `processing_output_dir` に保存されます。\n",
    "\n",
    "SageMaker コンソールの左側のメニューから 処理中 -> ジョブの処理 を選択すると、実行したジョブの詳細を確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = 's3://bucket/lightgbm/online_shoppers_model.txt'\n",
    "inference_data_s3_path = 's3://bucket/lightGBM-BYO/data/test/csv/'\n",
    "\n",
    "processor.run(code='./batch-inference.py', # S3 の URI でも可\n",
    "                     inputs=[ProcessingInput(source=inference_data_s3_path,\n",
    "                                        destination=processing_input_dir),\n",
    "                                    ProcessingInput(source=model_artifact,\n",
    "                                        destination=processing_model_dir)],\n",
    "                     outputs=[\n",
    "                         ProcessingOutput(output_name='batch',source=processing_output_dir)],\n",
    "                      arguments=[\n",
    "                          '--input-dir',processing_input_dir,\n",
    "                          '--model-dir',processing_model_dir,\n",
    "                          '--output-dir',processing_output_dir\n",
    "                      ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-italy",
   "metadata": {},
   "source": [
    "## 3. おわりに\n",
    "以上、2種類のバッチ推論方法をご紹介しました。ご紹介したどちらの方法もすべての入力データに対する推論が完了したら自動的に使用したインスタンスは停止されるため、明示的にインスタンスの停止や削除を行う必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-supplier",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
