{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precious-tuner",
   "metadata": {},
   "source": [
    "# Amazon SageMaker で LightGBM を使う\n",
    "\n",
    "このノートブックでは、LightGBM のカスタムコンテナを作成し、それを使って Amazon SageMaker でモデルを学習、デプロイします。カスタムコンテナを作る方法や、作ったコンテナを使ってモデルを学習する方法の参考になればと思います。また、Amazon SageMaker Experiments を使って複数の学習ジョブのメトリクスを比較する方法も紹介しています。\n",
    "\n",
    "このノートブックは、[こちら](https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/lightgbm-byo/lightgbm-byo.ipynb) を参考に作成しました。使用するデータセットは [こちら](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset) で、オンラインショッピングの顧客が商品を購入するかどうかを予測するモデルを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-crisis",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "session = sagemaker.Session()\n",
    "s3_output = session.default_bucket()\n",
    "s3_prefix = 'lightGBM-BYO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-germany",
   "metadata": {},
   "source": [
    "このノートブックでは Amazon SageMaker Experiments を利用するため、必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "import time\n",
    "\n",
    "lightgbm_experiment = Experiment.create(\n",
    "    experiment_name=f\"lightgbm-{int(time.time())}\", \n",
    "    description=\"Purchase intent prediction with lightGBM\", \n",
    "    sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "print(lightgbm_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-cricket",
   "metadata": {},
   "source": [
    "このサンプルでは、カスタムコンテナを Amazon ECR に push する必要があります。以下の操作でこのノートブックインスタンスで使用している IAM ロールに Amazon ECR にイメージを push するための権限を追加してください。\n",
    "\n",
    "1. Amazon SageMaker コンソールからこのノートブックインスタンスの詳細画面を表示<br>（左側のメニューのインスタンス -> ノートブックインスタンス -> インスタンス名をクリック）\n",
    "1. 「アクセス許可と暗号化」の「IAM ロール ARN」のリンクをクリック（IAM のコンソールに遷移します）\n",
    "1. 「ポリシーをアタッチします」と書いてある青いボタンをクリック\n",
    "1. 検索ボックスに ec2containerregistry と入力し  AmazonEC2ContainerRegistryFullAccess のチェックボックスをチェックする\n",
    "1. 「ポリシーのアタッチ」と書いてある青いボタンをクリック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p raw\n",
    "!wget -P ./raw https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./raw/online_shoppers_intention.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.countplot(x=df['Revenue'])\n",
    "plt.ylim(0,12000)\n",
    "plt.xlabel('Transactions Completed', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.text(x=-.175, y=11000 ,s='10,422', fontsize=16)\n",
    "plt.text(x=.875, y=2500, s='1908', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-arbor",
   "metadata": {},
   "source": [
    "ダウンロードしたデータを Amazon S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_s3_prefix = '{}/raw'.format(s3_prefix)\n",
    "raw_s3 = session.upload_data(path='./raw/', key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-missile",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Processing でデータの前処理\n",
    "\n",
    "Amazon SageMaker Processing は、前処理などをジョブとして実行する際に便利です。任意のコンテナとスクリプトを指定することで、様々な処理をジョブとして実行することができます。ジョブの情報は Amazon SageMaker によって記録・管理されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docker-proc-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-proc-evaluate/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "RUN apt -y update && apt install -y --no-install-recommends \\\n",
    "    libgomp1 \\\n",
    "    && apt clean    \n",
    "RUN pip3 install lightgbm numpy pandas scikit-learn \n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = 'lightgbm-byo-proc-eval'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-louisville",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker-proc-evaluate\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=processing_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    input_file = glob.glob('{}/*.csv'.format('/opt/ml/processing/input'))\n",
    "    print('\\nINPUT FILE: \\n{}\\n'.format(input_file))   \n",
    "    df = pd.read_csv(input_file[0])\n",
    "    \n",
    "    # minor preprocessing (drop some uninformative columns etc.)\n",
    "    print('Preprocessing the dataset . . . .')   \n",
    "    df_clean = df.drop(['Month','Browser','OperatingSystems','Region','TrafficType','Weekend'], axis=1)\n",
    "    visitor_encoded = pd.get_dummies(df_clean['VisitorType'], prefix='Visitor_Type', drop_first = True)\n",
    "    df_clean_merged = pd.concat([df_clean, visitor_encoded], axis=1).drop(['VisitorType'], axis=1)\n",
    "    X = df_clean_merged.drop('Revenue', axis=1)\n",
    "    y = df_clean_merged['Revenue']\n",
    "    \n",
    "    # split the preprocessed data with stratified sampling for class imbalance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=2, test_size=.2)\n",
    "\n",
    "    # save to container directory for uploading to S3\n",
    "    print('Saving the preprocessed dataset . . . .')   \n",
    "    train_data_output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "    np.save(train_data_output_path, X_train.to_numpy())\n",
    "    train_labels_output_path = os.path.join('/opt/ml/processing/train', 'y_train.npy')\n",
    "    np.save(train_labels_output_path, y_train.to_numpy())    \n",
    "    test_data_output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "    np.save(test_data_output_path, X_test.to_numpy())\n",
    "    test_labels_output_path = os.path.join('/opt/ml/processing/test', 'y_test.npy')\n",
    "    np.save(test_labels_output_path, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime \n",
    "\n",
    "processing_job_name = \"lightgbm-byo-process-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/data'.format(s3_output, s3_prefix)\n",
    "\n",
    "script_processor.run(code='preprocessing.py',\n",
    "                      job_name=processing_job_name,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_s3,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train',\n",
    "                                                destination='{}/train'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test',\n",
    "                                                destination='{}/test'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/test')]\n",
    "                    )\n",
    "\n",
    "preprocessing_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "        print(preprocessed_training_data)\n",
    "    if output['OutputName'] == 'test':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "        print(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-ground",
   "metadata": {},
   "source": [
    "あとでモデルの評価をノートブックインスタンス上でするために、前処理済みのデータをノートブックインスタンスにダウンロードしておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.download_data(path='.', bucket=s3_output, key_prefix=s3_prefix+'/data/test/x_test.npy')\n",
    "session.download_data(path='.', bucket=s3_output, key_prefix=s3_prefix+'/data/test/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-glass",
   "metadata": {},
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docker-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-black",
   "metadata": {},
   "source": [
    "argparse を使ってハイパーパラメタの設定を受け取ってモデルの学習で使用します。学習したモデルは、Estimator から受け取った args.model_dir に保存します。ここに学習済みモデルを保存しておくと、Amazon SageMaker が自動的に Amazon S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-train/train.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    # extract training data S3 location and hyperparameter values\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--num_leaves', type=int, default=28)\n",
    "    parser.add_argument('--max_depth', type=int, default=5)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print('Loading training data from {}\\n'.format(args.train))\n",
    "    input_files = glob.glob('{}/*.npy'.format(args.train))\n",
    "    print('\\nTRAINING INPUT FILE LIST: \\n{}\\n'.format(input_files)) \n",
    "    for file in input_files:\n",
    "        if 'x_' in file:\n",
    "            x_train = np.load(file)\n",
    "        else:\n",
    "            y_train = np.load(file)      \n",
    "    print('\\nx_train shape: \\n{}\\n'.format(x_train.shape))\n",
    "    print('\\ny_train shape: \\n{}\\n'.format(y_train.shape))\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Loading validation data from {}\\n'.format(args.validation))\n",
    "    eval_input_files = glob.glob('{}/*.npy'.format(args.validation))\n",
    "    print('\\nVALIDATION INPUT FILE LIST: \\n{}\\n'.format(eval_input_files)) \n",
    "    for file in eval_input_files:\n",
    "        if 'x_' in file:\n",
    "            x_val = np.load(file)\n",
    "        else:\n",
    "            y_val = np.load(file)      \n",
    "    print('\\nx_val shape: \\n{}\\n'.format(x_val.shape))\n",
    "    print('\\ny_val shape: \\n{}\\n'.format(y_val.shape))\n",
    "    eval_data = lgb.Dataset(x_val, label=y_val)\n",
    "    \n",
    "    print('Training model with hyperparameters:\\n\\t num_leaves: {}\\n\\t max_depth: {}\\n\\t learning_rate: {}\\n'\n",
    "          .format(args.num_leaves, args.max_depth, args.learning_rate))\n",
    "    parameters = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'is_unbalance': 'true',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_leaves': args.num_leaves,\n",
    "        'max_depth': args.max_depth,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    num_round = 10\n",
    "    bst = lgb.train(parameters, train_data, num_round, eval_data, verbose_eval=1)\n",
    "    \n",
    "    print('Saving model . . . .')\n",
    "    bst.save_model(os.path.join(args.model_dir,'online_shoppers_model.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-train/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "RUN apt -y update && apt install -y --no-install-recommends \\\n",
    "    libgomp1 build-essential \\\n",
    "    && apt clean    \n",
    "RUN pip install lightgbm numpy pandas scikit-learn sagemaker-training\n",
    "COPY train.py /train.py\n",
    "ENV SAGEMAKER_PROGRAM /train.py\n",
    "ENV PYTHONUNBUFFERED=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository_train = 'lightgbm-byo-train'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "train_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_train + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_train docker-train\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_train\n",
    "!docker tag {ecr_repository_train + tag} $train_repository_uri\n",
    "!docker push $train_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-payday",
   "metadata": {},
   "source": [
    "まずはローカルモードでモデルを学習します。ローカルモード使うと、ローカル（今回はノートブックインスタンス）で擬似的に学習ジョブを実行することができます。学習用インスタンスを別途起動する必要がないため、すぐに学習ジョブを実行できるのが特徴です。ローカルモードを使用するには、Estimator の引数に `instance_type='local'` を設定するだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "import json\n",
    "\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "hyperparameters = json_encode_hyperparameters({\n",
    "    'num_leaves': 32,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.08})\n",
    "\n",
    "estimator = Estimator(image_uri=train_repository_uri,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='local', # training job will run locally\n",
    "                      hyperparameters=hyperparameters,\n",
    "                      base_job_name='lightgbm-byo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train': preprocessed_training_data, 'validation': preprocessed_test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-detroit",
   "metadata": {},
   "source": [
    "学習したモデルは Amaozn S3 に model.tar.gz として保存されます。ダウンロードして展開することで学習済みモデルを取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz\n",
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-cinema",
   "metadata": {},
   "source": [
    "## ノートブック上でモデルの評価\n",
    "\n",
    "先ほどダウンロードした学習済みモデルを使ってモデルの評価を行なってみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "input_files = glob.glob('./*.npy')\n",
    "for file in input_files:\n",
    "    if 'x_' in file:\n",
    "        x_test = np.load(file)\n",
    "    else:\n",
    "        y_test = np.load(file)\n",
    "\n",
    "def eval(model_path):\n",
    "            \n",
    "    bst_loaded = lgb.Booster(model_file=model_path+'online_shoppers_model.txt')\n",
    "    y_pred = bst_loaded.predict(x_test)\n",
    "    \n",
    "#     print('Evaluating model . . . .\\n')    \n",
    "    acc = accuracy_score(y_test.astype(int), y_pred.round(0).astype(int))\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print('==== result ====')\n",
    "    print(model_path)\n",
    "    print('Accuracy:  {:.2f}'.format(acc))\n",
    "    print('AUC Score: {:.2f}'.format(auc))\n",
    "    \n",
    "eval('./model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-wednesday",
   "metadata": {},
   "source": [
    "問題なく学習されていそうなことが確認できました。\n",
    "\n",
    "## インスタンスを使ってハイパーパラメタチューニング\n",
    "\n",
    "作成したコンテナやスクリプトの動作確認ができたので、学習インスタンスを使ってハイパーパラメタチューニングをやってみましょう。ここでは、Amazon SageMaker Experiments の機能をも使って、各学習ジョブの結果を関連づけて記録し、あとで結果を比較します。\n",
    "\n",
    "Estimator の引数に metric_definitions を設定することで、メトリクスの値を取得することができます。ここでは validation loss を取得するよう設定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name_map = {}\n",
    "\n",
    "for i, max_depth in enumerate([3, 6, 9, 12]):\n",
    "    # create trial\n",
    "    trial_name = f\"lightgbm-training-depth-{max_depth}-{int(time.time())}\"\n",
    "    trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "        experiment_name=lightgbm_experiment.experiment_name,\n",
    "        sagemaker_boto_client=boto3.client('sagemaker'),\n",
    "    )\n",
    "    trial_name_map[max_depth] = trial_name\n",
    "    \n",
    "    hyperparameters = json_encode_hyperparameters({ \n",
    "                                                    'num_leaves': 32,\n",
    "                                                    'max_depth': max_depth,\n",
    "                                                    'learning_rate': 0.08 })\n",
    "\n",
    "    estimator = Estimator(image_uri=train_repository_uri,\n",
    "                          role=role,\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.c5.xlarge',\n",
    "                          hyperparameters=hyperparameters,\n",
    "                          enable_sagemaker_metrics=True,\n",
    "                          metric_definitions=[\n",
    "                            {'Name':'validation:loss', 'Regex':'.*loss: ([0-9\\\\.]+)'}\n",
    "                           ]\n",
    "                          )\n",
    "    \n",
    "    training_job_name = f\"lightgbm-training-depth-{max_depth}-{int(time.time())}\"\n",
    "    # Now associate the estimator with the Experiment and Trial\n",
    "    estimator.fit(\n",
    "        inputs={'train': preprocessed_training_data, 'validation': preprocessed_test_data}, \n",
    "        job_name=training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-field",
   "metadata": {},
   "source": [
    "## チューニング結果の確認\n",
    "\n",
    "**以降のセルは、上記セルで実行した全ての学習ジョブが完了してから実行してください。**学習ジョブの状況は、Amazon SageMaker のコンソールの左側のメニューから トレーニング -> トレーニングジョブ とクリックすると確認できます。全てジョブが Completed になっていることを確認してください。\n",
    "\n",
    "以下では、Amazon SageMaker Experiments を使って記録された各学習ジョブの情報を確認しています。Experiments にはたくさんの情報が記録されているため、以下ではパラメタ情報は max_depth に絞って表示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(sess, sm), \n",
    "    experiment_name=lightgbm_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.validation:loss.Min\",\n",
    "    sort_order=\"Ascending\",\n",
    "    parameter_names=['max_depth']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-blame",
   "metadata": {},
   "source": [
    "今回は、`metrics.validation:loss.Min` が最も小さいモデルを採用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = trial_component_analytics.dataframe().loc[0]['SageMaker.ModelArtifact - Value']\n",
    "print(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $model_artifact ./model2/model.tar.gz\n",
    "!tar -xvzf ./model2/model.tar.gz -C ./model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-arabic",
   "metadata": {},
   "source": [
    "ハイパーパラメタチューニングの前後でモデルの性能は変わったでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval('./model/')\n",
    "eval('./model2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-tactics",
   "metadata": {},
   "source": [
    "## モデルのデプロイと推論\n",
    "\n",
    "ハイパーパラメタチューニングで最も Loss が小さかったモデルをデプロイして推論します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for inference sources\n",
    "!mkdir -p docker-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference/model_script.py\n",
    "\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sagemaker_inference import content_types, encoder\n",
    "\n",
    "NUM_FEATURES = 12\n",
    "\n",
    "class ModelHandler(object):\n",
    "    \"\"\"\n",
    "    A lightGBM Model handler implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.model = None\n",
    "\n",
    "    def initialize(self, context):\n",
    "        \"\"\"\n",
    "        Initialize model. This will be called during model loading time\n",
    "        :param context: Initial context contains model server system properties.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.initialized = True\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\") \n",
    "        self.model = lgb.Booster(model_file=os.path.join(model_dir,'online_shoppers_model.txt'))\n",
    "       \n",
    "\n",
    "    def preprocess(self, request):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        :param request: list of raw requests\n",
    "        :return: list of preprocessed model input data\n",
    "        \"\"\"        \n",
    "        payload = request[0]['body']\n",
    "        data = np.frombuffer(payload, dtype=np.float64)\n",
    "        data = data.reshape((data.size // NUM_FEATURES, NUM_FEATURES))\n",
    "        return data\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        \"\"\"\n",
    "        Internal inference methods\n",
    "        :param model_input: transformed model input data list\n",
    "        :return: list of inference output in numpy array\n",
    "        \"\"\"\n",
    "        prediction = self.model.predict(model_input)\n",
    "        return prediction\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"\n",
    "        Post processing step - converts predictions to str\n",
    "        :param inference_output: predictions as numpy\n",
    "        :return: list of inference output as string\n",
    "        \"\"\"\n",
    "\n",
    "        return [str(inference_output.tolist())]\n",
    "        \n",
    "    def handle(self, data, context):\n",
    "        \"\"\"\n",
    "        Call preprocess, inference and post-process functions\n",
    "        :param data: input data\n",
    "        :param context: mms context\n",
    "        \"\"\"\n",
    "        \n",
    "        model_input = self.preprocess(data)\n",
    "        model_out = self.inference(model_input)\n",
    "        return self.postprocess(model_out)\n",
    "\n",
    "_service = ModelHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(context)\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    return _service.handle(data, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference/dockerd-entrypoint.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "import os\n",
    "from retrying import retry\n",
    "from subprocess import CalledProcessError\n",
    "from sagemaker_inference import model_server\n",
    "\n",
    "def _retry_if_error(exception):\n",
    "    return isinstance(exception, CalledProcessError or OSError)\n",
    "\n",
    "@retry(stop_max_delay=1000 * 50,\n",
    "       retry_on_exception=_retry_if_error)\n",
    "def _start_mms():\n",
    "    # by default the number of workers per model is 1, but we can configure it through the\n",
    "    # environment variable below if desired.\n",
    "    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n",
    "    model_server.start_model_server(handler_service='/home/model-server/model_script.py:handle')\n",
    "\n",
    "def main():\n",
    "    if sys.argv[1] == 'serve':\n",
    "        _start_mms()\n",
    "    else:\n",
    "        subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n",
    "\n",
    "    # prevent docker exit\n",
    "    subprocess.call(['tail', '-f', '/dev/null'])\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-inference/Dockerfile\n",
    "\n",
    "FROM ubuntu:18.04\n",
    "    \n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "# Install necessary dependencies for MMS and SageMaker Inference Toolkit\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    openjdk-8-jdk-headless \\\n",
    "    python3-dev \\\n",
    "    curl \\\n",
    "    vim \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && curl -O https://bootstrap.pypa.io/get-pip.py \\\n",
    "    && python3 get-pip.py\n",
    "\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
    "RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
    "    \n",
    "RUN pip install lightgbm numpy pandas \\ \n",
    "                scikit-learn multi-model-server \\\n",
    "                sagemaker-inference retrying\n",
    "\n",
    "# Copy entrypoint script to the image\n",
    "COPY dockerd-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
    "RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
    "\n",
    "RUN mkdir -p /home/model-server/\n",
    "\n",
    "# Copy the default custom service file to handle incoming data and inference requests\n",
    "COPY model_script.py /home/model-server/model_script.py\n",
    "\n",
    "# Define an entrypoint script for the docker image\n",
    "ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
    "\n",
    "# Define command to be passed to the entrypoint\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-creativity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 以下のセルを実行して no space left のエラーが出たら以下のコマンドのコメントアウトを外して実行してください。\n",
    "# !docker system prune -a -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository_inference = 'lightgbm-byo-inference'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "inference_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_inference + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_inference docker-inference\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_inference\n",
    "!docker tag {ecr_repository_inference + tag} $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Model, Predictor\n",
    "\n",
    "lgbm_model = Model(model_data=model_artifact,\n",
    "                   role=role,\n",
    "                   image_uri=inference_repository_uri,\n",
    "                   name='lightgbm-byo-deployment')\n",
    "\n",
    "lgbm_model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictor = Predictor(endpoint_name=lgbm_model.endpoint_name)\n",
    "payload = x_test.tobytes()\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "y_pred_str = response.decode('utf-8')\n",
    "y_pred = np.asarray(y_pred_str.strip('][').split(', '), dtype=np.float64)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-middle",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "\n",
    "利用が終わったら、このノートブックを実行したノートブックインスタンスの停止および削除を実施してください。ノートブックインスタンスを停止させると、ノートブックインスタンスの課金は止まりますがアタッチされている EBS ボリュームへの課金が継続しますので、完全に課金を止めるにはノートブックインスタンスの停止だけでなく削除まで実施してください。\n",
    "\n",
    "また、Amazon S3 にアップロードした各種ファイルに対しても課金が発生するため、不要であれば削除してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-rough",
   "metadata": {},
   "source": [
    "作成した Amazon SageMaker Experiments の情報も削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(experiment):\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(sagemaker_boto_client=sm, trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                sagemaker_boto_client=sm,\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # tc is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "    experiment.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(lightgbm_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-switch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
