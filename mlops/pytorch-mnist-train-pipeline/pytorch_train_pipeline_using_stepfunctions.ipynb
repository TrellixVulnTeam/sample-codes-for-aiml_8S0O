{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Step Functions Data Science SDK で機械学習ワークフローを構築する\n",
    "\n",
    "このノートブックは、Amazon SageMaker ノートブックインスタンスでの実行を想定しています。\n",
    "\n",
    "機械学習プロジェクトにおいて、モデル開発の試行錯誤を効率的に行うことは非常に重要です。毎回手作業で同じような処理を行うのではなく、パラメタやソースコードを変えてパイプラインを実行するだけで試行錯誤できるようにすると、ヒューマンエラーを削減したり試行錯誤の記録を残しやすくすることができます。\n",
    "\n",
    "AWS Step Functions Data Science SDK を使うと、AWS Step Function と Amazon SageMaker を使って、データサイエンティストが機械学習ワークフローを簡単に作成して実行することができます。詳しい情報は以下のドキュメントをご参照ください。\n",
    "\n",
    "* [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "* [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n",
    "* [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io)\n",
    "\n",
    "このノートブックでは、以下の処理を行うパイプラインを作成します。\n",
    "- Amazon SageMaker Processing を使ったデータの準備\n",
    "- Amazon SageMaker Training を使ったモデルの学習\n",
    "- Amazon SageMaker Processing を使ったモデルの評価\n",
    "- AWS Lambda を使った評価結果に応じた後処理\n",
    "\n",
    "大まかな流れは以下の通りです。\n",
    "\n",
    "1. AWS Step Functions Data Science SDK の `ProcessingStep` を使ってデータの前処理、特徴量エンジニアリング、学習用とテスト用への分割を行う scikit-learn スクリプトを実行する SageMaker Processing Job を実行\n",
    "1. AWS Step Functions Data Science SDK の `TrainingStep` を使って前処理された学習データを使ったモデルの学習を実行\n",
    "1. AWS Step Functions Data Science SDK の `ProcessingStep` を使って前処理したテスト用データを使った学習済モデルの評価を実行\n",
    "1. AWS Step Functions Data Science SDK の `LambdaStep`を使って最新のモデルと過去のモデルの評価指標の比較を実行\n",
    "\n",
    "\n",
    "このノートブックで使用するデータは手書き数字のデータセット MNIST です。\n",
    "\n",
    "コンテナイメージのビルドを円滑にするために、ノートブックインスタンスのストレージ容量を 30GB 程度に設定することをおすすめします。ストレージ容量は、ノートブックインスタンス作成時の追加設定の領域で設定可能です（デフォルトは 5GB）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "このノートブックを実行するのに必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the latest sagemaker, stepfunctions and boto3 SDKs\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\"\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.3.0\"\n",
    "!{sys.executable} -m pip install sagemaker-experiments\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテナイメージビルドの際の容量不足を回避するために以下のセルを実行して docker 関連のファイルの保存場所を変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sudo service docker stop\n",
    "sudo mv /var/lib/docker /home/ec2-user/SageMaker/docker\n",
    "sudo ln -s /home/ec2-user/SageMaker/docker /var/lib/docker\n",
    "sudo service docker start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents を作成する拡張機能をインストールすると便利です。以下のセルを実行したあと、このノートブックを開いているブラウザのタブをリロードすると Table of Contents の拡張機能が使えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --user\n",
    "jupyter nbextension enable toc2/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なモジュールのインポート\n",
    "\n",
    "同一アカウントで複数の方が同時にこのノートブックを実行する場合は、以下のセルの一番下の行の `user_name` に各自のお名前を設定してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.steps import (\n",
    "    Chain,\n",
    "    ChoiceRule,\n",
    "    ModelStep,\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    ")\n",
    "from stepfunctions.template import TrainingPipeline\n",
    "from stepfunctions.template.utils import replace_parameters_with_jsonpath\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from time import sleep\n",
    "\n",
    "# SageMaker Session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# SageMaker Execution Role\n",
    "# You can use sagemaker.get_execution_role() if running inside sagemaker's notebook instance\n",
    "role = get_execution_role()\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "ecr_client = boto3.client('ecr', region_name=region)\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "project_name = 'pytorch-mnist'\n",
    "user_name = 'sample'\n",
    "container_image_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、ノートブックから Step Functions を実行するための IAM ロール設定を行います。\n",
    "\n",
    "### ノートブックインスタンスの IAM ロールに権限を追加\n",
    "\n",
    "以下の手順を実行して、ノートブックインスタンスに紐づけられた IAM ロールに、AWS Step Functions のワークフローを作成して実行するための権限と Amazon ECR にイメージを push するための権限を追加してください。\n",
    "\n",
    "\n",
    "1. [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/) を開く\n",
    "1. **ノートブックインスタンス** を開いて現在使用しているノートブックインスタンスを選択する\n",
    "1. **アクセス許可と暗号化** の部分に表示されている IAM ロールへのリンクをクリックする\n",
    "1. IAM ロールの ARN は後で使用するのでメモ帳などにコピーしておく\n",
    "1. **ポリシーをアタッチします** をクリックして `IAMFullAccess` を検索する\n",
    "1. `IAMFullAccess` の横のチェックボックスをオンにする\n",
    "1. **ポリシーのアタッチ** をクリックする\n",
    "\n",
    "\n",
    "もしこのノートブックを SageMaker のノートブックインスタンス以外で実行している場合、その環境で AWS CLI 設定を行ってください。詳細は [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) をご参照ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_list = ['AWSStepFunctionsFullAccess', 'AmazonEC2ContainerRegistryFullAccess', 'AWSLambda_FullAccess']\n",
    "\n",
    "for l in policy_list:\n",
    "    response = iam_client.attach_role_policy(\n",
    "        RoleName=role.split('/')[2],\n",
    "        PolicyArn='arn:aws:iam::aws:policy/' + l\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に Step Functions で使用する実行ロールを作成します。\n",
    "\n",
    "### Step Functions の実行ロールの作成\n",
    "\n",
    "作成した Step Functions ワークフローは、AWS の他のサービスと連携するための IAM ロールを必要とします。以下のセルを実行して、必要な権限を持つ IAM Policy を作成し、それを新たに作成した IAM Role にアタッチします。\n",
    "\n",
    "なお、今回は広めの権限を持つ IAM Policy を作成しますが、ベストプラクティスとしては必要なリソースのアクセス権限と必要なアクションのみを有効にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step_functions_policy_name = 'AmazonSageMaker-StepFunctionsWorkflowExecutionPolicy-' + user_name\n",
    "inline_policy ={\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:PutTargets\",\n",
    "                \"events:DescribeRule\",\n",
    "                \"events:PutRule\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTransformJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTuningJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForECSTaskRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForBatchJobsRule\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": role,\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor2\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecs:DescribeTasks\",\n",
    "                \"ecs:RunTask\",\n",
    "                \"ecs:StopTask\",\n",
    "                \"lambda:InvokeFunction\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:CreateHyperParameterTuningJob\",\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:CreateProcessingJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:CreateTransformJob\",\n",
    "                \"sagemaker:DeleteEndpoint\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DescribeHyperParameterTuningJob\",\n",
    "                \"sagemaker:DescribeProcessingJob\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:DescribeTransformJob\",\n",
    "                \"sagemaker:ListProcessingJobs\",\n",
    "                \"sagemaker:ListTags\",\n",
    "                \"sagemaker:StopHyperParameterTuningJob\",\n",
    "                \"sagemaker:StopProcessingJob\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:StopTransformJob\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sns:Publish\",\n",
    "                \"sqs:SendMessage\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = iam_client.create_policy(\n",
    "    PolicyName=step_functions_policy_name,\n",
    "    PolicyDocument=json.dumps(inline_policy),\n",
    ")\n",
    "\n",
    "step_functions_policy_arn = response['Policy']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_functions_role_name = 'AmazonSageMaker-StepFunctionsWorkflowExecutionRole-' + user_name\n",
    "assume_role_policy = {\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [{\"Sid\": \"\",\"Effect\": \"Allow\",\"Principal\": {\"Service\":\"states.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]\n",
    "    }\n",
    "response = iam_client.create_role(\n",
    "    Path = '/service-role/',\n",
    "    RoleName = step_functions_role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy),\n",
    "    MaxSessionDuration=3600*12 # 12 hours\n",
    ")\n",
    "\n",
    "workflow_execution_role = response['Role']['Arn']\n",
    "\n",
    "response = iam_client.attach_role_policy(\n",
    "    RoleName=step_functions_role_name,\n",
    "    PolicyArn=step_functions_policy_arn\n",
    ")\n",
    "\n",
    "response = iam_client.attach_role_policy(\n",
    "    RoleName=step_functions_role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/CloudWatchEventsFullAccess'\n",
    ")\n",
    "workflow_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Functions ワークフロー実行時の入力スキーマ作成\n",
    "\n",
    "Step Functions ワークフローを実行する際に、パラメタなどを引数として渡すことができます。これにより、パラメタのみを変えてワークフローを実行する際は、ワークフローを作り直さなくてもすみます。ここではそれらの引数のスキーマを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker expects unique names for each job, model and endpoint.\n",
    "# If these names are not unique the execution will fail. Pass these\n",
    "# dynamically for each execution using placeholders.\n",
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "        \"PreprocessingJobName\": str,\n",
    "#         \"PreprocessingInputData\": str,\n",
    "        \"PreprocessingCode\": str,\n",
    "        \"PreprocessingOutputData\": str,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TrainingParameters\": dict,\n",
    "        \"TrainingOutputModel\": str,\n",
    "        \"ExperimentName\": str,\n",
    "        \"EvaluationScriptPath\": str,\n",
    "        \"EvaluationProcessingJobName\": str,\n",
    "        \"EvaluationProcessingOutput\": str,\n",
    "        \"EvaluationExperimentArgs\": list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Experiments のセットアップ\n",
    "\n",
    "このノートブックでは、モデルの評価メトリクスを記録するために Amazon SageMaker Experiments を使用します。以下のセルを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = project_name + '-' + user_name\n",
    "\n",
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment_evaluate = Experiment.load(experiment_name=experiment_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        experiment_evaluate = Experiment.create(\n",
    "            experiment_name=experiment_name, \n",
    "            description=\"model evaluation\", \n",
    "            sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "print(experiment_evaluate.experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースイメージを Amazon ECR に push\n",
    "\n",
    "試行錯誤の中で何度もコンテナイメージのビルドを繰り返していると、Docker Hub からベースイメージを pull できなくなることがあります。そこで、はじめにベースイメージをビルドして Amazon ECR に push しておき、そちらを今後ベースイメージとして使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repository = project_name + '-base-' + user_name\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "base_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, base_repository + tag)\n",
    "\n",
    "!docker build -t $base_repository .\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $base_repository\n",
    "!docker tag {base_repository + tag} $base_repository_uri\n",
    "!docker push $base_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS が Amazon S3 上に用意した MNIST データセットを学習ジョブで使用できる形に加工します。データ準備スクリプトを実行するために `ScriptProcessor`を作成します。これは、任意のコンテナイメージを使って Processing ジョブを実行するためのクラスです。\n",
    "\n",
    "データ準備スクリプトでは以下の処理を実行します。\n",
    "\n",
    "* Amazon S3 からデータをダウンロード\n",
    "* ダウンロードしたデータを解凍\n",
    "* データを pt 形式にして S3 にアップロード\n",
    "\n",
    "学習スクリプトでは、準備した学習用データとラベル情報を使用してモデルを学習します。また、モデル評価スクリプトでは学習済みモデルと準備したデータとラベル情報を使用してモデルを評価します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ準備用コンテナイメージの作成\n",
    "\n",
    "まずは MNIST データセットを準備してモデルの学習に使える状態にします。このサンプルノートブックでは、データセットを作成するために torchvision などを使用するため、それらのライブラリを使えるコンテナイメージを作成する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_name = 'docker/data-preparation/Dockerfile'\n",
    "\n",
    "dockerfile_code = f\"\"\"FROM {base_repository_uri}\n",
    "    \n",
    "ENV AWS_DEFAULT_REGION {region}\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN pip3 install -qU -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"]\n",
    "\"\"\"\n",
    "with open(dockerfile_name, 'w') as f:\n",
    "    f.write(dockerfile_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを実行してコンテナイメージのビルドと Amazon ECR への push を行います。ログの最後に Pushed と書かれていることを確認してください。Pushed というログがない場合、イメージのビルドや push に失敗している可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository = project_name + '-preparation-' + user_name\n",
    "container_image_list.append(ecr_repository)\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker/data-preparation\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを実行して、前処理のための Processing Job を実行するための `ScriptProcessor` を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "data_preparation_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=processing_repository_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ準備スクリプトを S3 にアップロードします。パイプライン（Step Functions ワークフロー）実行時にデータ準備用スクリプトのパスを指定することで、ジョブ起動後にそのパスからデータ準備スクリプトが Processing 用コンテナにダウンロードされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(tz=JST).strftime('%Y%m%-d%H%M')\n",
    "\n",
    "PREPROCESSING_SCRIPT_LOCATION = \"preprocessing.py\"\n",
    "\n",
    "preparation_input_code = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(project_name, user_name, \"data_preparation/code\", timestamp),\n",
    ")\n",
    "print('preparation_input_code: ', preparation_input_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ProcessingStep` の作成\n",
    "\n",
    "それでは、Step Functions ワークフローから SageMaker Processing ジョブを起動するための [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) を作成しましょう。\n",
    "\n",
    "このステップは、前の手順で定義した ScriptProcessor に入力と出力の情報を追加して使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ProcessingInputs](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput) と [ProcessingOutputs](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput)  オブジェクトを作成して SageMaker Processing ジョブに入力と出力の情報を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "#     ProcessingInput(\n",
    "#         source=execution_input[\"PreprocessingInputData\"], destination=\"/opt/ml/processing/input\", input_name=\"input-1\"\n",
    "#     ),\n",
    "    ProcessingInput(\n",
    "#         source=input_code,\n",
    "        source=execution_input[\"PreprocessingCode\"],\n",
    "        destination=\"/opt/ml/processing/input/code\",\n",
    "        input_name=\"code\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "processing_output_path = '/opt/ml/processing/output'\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=processing_output_path,\n",
    "        destination=execution_input[\"PreprocessingOutputData\"],\n",
    "        output_name=\"prepared_data\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `ProcessingStep` の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"Data Preparation Step\",\n",
    "    processor=data_preparation_processor,\n",
    "    job_name=execution_input[\"PreprocessingJobName\"],\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    container_arguments=[\"--output-dir\", processing_output_path],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備したデータを使ったモデルの学習\n",
    "\n",
    "\n",
    "### 学習用コンテナイメージの作成\n",
    "まずは、モデルの学習で使用するコンテナイメージを作成します。以下のセルと実行して Dockerfile を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'docker/train/Dockerfile'\n",
    "\n",
    "train_dockerfile={f\"\"\"FROM 763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n",
    " \n",
    "COPY requirements.txt ./\n",
    "RUN pip uninstall torch torchvision -y && pip install --no-cache-dir -U -r requirements.txt\n",
    "\n",
    "ENV SAGEMAKER_PROGRAM train.py\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    f.write('\\n'.join(list(train_dockerfile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテナイメージをビルドして Amazon ECR に push します。このセルの実行が完了するまでに 5分ほどかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository = project_name + '-train-' + user_name\n",
    "container_image_list.append(ecr_repository)\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "train_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker/train\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $train_repository_uri\n",
    "!docker push $train_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習スクリプト `train.py` を使って学習ジョブを実行するための `PyTorch` インスタンスを作成します。これはあとで `TrainingStep` を作成する際に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "metric_definitions = [{'Name': 'average test loss',\n",
    "                       'Regex': 'Test set: Average loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    image_uri=train_repository_uri,\n",
    "                    role=role,\n",
    "#                     framework_version='1.8.0',  # image_uri ではなく framework_version を指定すると組み込みコンテナを利用可能\n",
    "#                     py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.xlarge',\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    hyperparameters={\n",
    "                        'batch-size':128,\n",
    "                        'lr': 0.01,\n",
    "                        'epochs': 1,\n",
    "                        'backend': 'gloo'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習スクリプト `train.py` は、ロジスティック回帰モデルを学習し、学習済みモデルを `/opt/ml/model` に保存します。Amazon SageMaker は、学習ジョブの最後に `/opt/ml/model` に保存されているモデルを `model.tar.gz` に圧縮して S3 にアップロードします。学習スクリプトは基本的に、ローカル環境で使用する際のものを流用できますが、SageMaker で使えるように argparse を使って各種変数を受け取る記述を追加してください。\n",
    "\n",
    "学習用スクリプトを source.tar.gz に固めて S3 にアップロードします。既存のファイルを上書きしてしまわないよう、タイムスタンプを使ってアップロード先のパスを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(tz=JST).strftime('%Y%m%d-%H%M')\n",
    "\n",
    "TRAINNING_SCRIPT_LOCATION = \"source.tar.gz\"\n",
    "!tar zcvf $TRAINNING_SCRIPT_LOCATION train.py\n",
    "\n",
    "train_code = sagemaker_session.upload_data(\n",
    "    TRAINNING_SCRIPT_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(project_name, user_name, \"train/code\", timestamp),\n",
    ")\n",
    "train_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TrainingStep` の作成\n",
    "\n",
    "Step Function ワークフローから学習ジョブを起動するために `TrainingStep` を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = steps.TrainingStep(\n",
    "    \"SageMaker Training Step\",\n",
    "    estimator=estimator,\n",
    "    data={\"training\": sagemaker.TrainingInput(execution_input[\"PreprocessingOutputData\"])},\n",
    "    job_name=execution_input[\"TrainingJobName\"],\n",
    "    hyperparameters=execution_input[\"TrainingParameters\"],\n",
    "#     experiment_config={\n",
    "#              \"ExperimentName\":execution_input[\"ExperimentName\"]},\n",
    "    wait_for_completion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価\n",
    "\n",
    "`evaluation.py` はモデル評価用のスクリプトです。このスクリプトは scikit-learn と Amazon SageMake Experiments を用いるため、カスタムコンテナを利用できる`ScriptProcessor` を使用します。このスクリプトは学習済みモデルとテスト用データセットを入力として受け取り、loss と accuracy が記載された JSON ファイルを出力します。\n",
    "\n",
    "パラメタや学習データを変えて複数のモデルを学習させ、それらの精度を比較する際に評価メトリクスの情報を Amazon SageMaker Experiments で管理しておくと、作成した複数のモデルを比較しやすくなります。以下のモデル評価用のスクリプトでは、モデルの評価メトリクスを算出してそれらを Experiment に登録し、また、最新の学習済みモデルが既存のモデルと比べて良いかどうかを評価メトリクスを使って判定し、その結果も Experiment に記録しています。学習ジョブや Processing ジョブの中で Experiments の Tracker を使う際は、`Tracker.load() ` を使用することでジョブが使用している Trial を自動的に読み出して tracking を開始します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価用スクリプトを S3 にアップロードします。既存のファイルを上書きしてしまわないよう、タイムスタンプを使ってアップロード先のパスを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(tz=JST).strftime('%Y%m%d-%H%M')\n",
    "\n",
    "MODELEVALUATION_SCRIPT_LOCATION = \"evaluation.py\"\n",
    "\n",
    "input_evaluation_code = sagemaker_session.upload_data(\n",
    "    MODELEVALUATION_SCRIPT_LOCATION,\n",
    "    bucket=bucket,\n",
    "    key_prefix=os.path.join(project_name, user_name, \"evaluation/code\", timestamp),\n",
    ")\n",
    "print('input_evaluation_code:', input_evaluation_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル評価用の ProcessingStep の入力と出力オブジェクトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_dir = \"/opt/ml/processing/model\"\n",
    "eval_data_dir = \"/opt/ml/processing/data\"\n",
    "inputs_evaluation = [\n",
    "    # データ準備 steo で準備したデータの設定（評価データ）\n",
    "    ProcessingInput(\n",
    "        source=execution_input[\"PreprocessingOutputData\"],\n",
    "        destination=eval_data_dir,\n",
    "        input_name=\"input-1\",\n",
    "    ),\n",
    "    # 評価対象の学習済みモデルの設定\n",
    "    ProcessingInput(\n",
    "        source=execution_input[\"TrainingOutputModel\"],\n",
    "        destination=eval_model_dir,\n",
    "        input_name=\"input-2\",\n",
    "    ),\n",
    "    # 評価スクリプトの設定\n",
    "    ProcessingInput(\n",
    "        source=execution_input['EvaluationScriptPath'],\n",
    "        destination=\"/opt/ml/processing/input/code\",\n",
    "        input_name=\"code\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "outputs_evaluation = [\n",
    "    # 評価結果の設定\n",
    "    ProcessingOutput(\n",
    "        source=\"/opt/ml/processing/evaluation\",\n",
    "        destination=execution_input[\"EvaluationProcessingOutput\"],\n",
    "        output_name=\"evaluation\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの評価結果を SageMaker Experiments で管理するため、Experiments のライブラリを含んだコンテナイメージを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_name = 'docker/model-evaluation/Dockerfile'\n",
    "\n",
    "dockerfile_code = f\"\"\"FROM {base_repository_uri}\n",
    "    \n",
    "ENV AWS_DEFAULT_REGION {region}\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN pip3 install -qU -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/opt/ml/processing/input/code/evaluation.py\"]\n",
    "\"\"\"\n",
    "with open(dockerfile_name, 'w') as f:\n",
    "    f.write(dockerfile_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository = project_name + '-evaluate-' + user_name\n",
    "container_image_list.append(ecr_repository)\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "evaluate_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker/model-evaluation\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $evaluate_repository_uri\n",
    "!docker push $evaluate_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの前処理の際は SKProcessor を使用しましたが、モデルの評価にはカスタムコンテナを使用するため ScriptProcessor を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "model_evaluation_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=evaluate_repository_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ProcessingStep` の作成\n",
    "\n",
    "ProcessingStep の引数 experiment_config に Experiment 名をセットすることで、起動した Processing ジョブの中で自動的に Trial が作成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_evaluation_step = ProcessingStep(\n",
    "    \"SageMaker Processing Model Evaluation step\",\n",
    "    processor=model_evaluation_processor,\n",
    "    job_name=execution_input[\"EvaluationProcessingJobName\"],\n",
    "    inputs=inputs_evaluation,\n",
    "    outputs=outputs_evaluation,\n",
    "    experiment_config={\n",
    "             \"ExperimentName\":execution_input[\"ExperimentName\"]},\n",
    "    container_arguments=execution_input[\"EvaluationExperimentArgs\"],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/evaluation.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価結果に応じた処理\n",
    "\n",
    "モデルの評価ジョブでの評価結果をもとに、Lambda 関数で後処理を行います。後処理としては例えば、Slack にモデルの評価結果をポストしたり、別の Workflow を起動したりなどが考えられます。このノートブックでは、評価結果に応じて表示するテキストの内容を変えています。\n",
    "\n",
    "実行環境やソースコードが入ったコンテナイメージを Lambda にデプロイします。このノートブックでは、Experiment で管理されている情報を取得するために `sagemaker.analytics.ExperimentAnalytics` を使用するため、必要なライブラリが入ったコンテナを使って Lambda 関数を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_name = 'docker/postprocess-lambda/Dockerfile'\n",
    "\n",
    "dockerfile_code = f\"\"\"FROM {base_repository_uri}\n",
    "\n",
    "# Include global arg in this stage of the build\n",
    "ARG FUNCTION_DIR=\"/function\"\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN pip3 install -qU -r requirements.txt\n",
    "\n",
    "# Copy function code\n",
    "RUN mkdir -p $FUNCTION_DIR\n",
    "COPY app/ $FUNCTION_DIR/\n",
    "\n",
    "# Set working directory to function root directory\n",
    "WORKDIR $FUNCTION_DIR\n",
    "\n",
    "\n",
    "ENTRYPOINT [ \"/usr/local/bin/python\", \"-m\", \"awslambdaric\" ]\n",
    "CMD [ \"app.handler\" ]\n",
    "\"\"\"\n",
    "\n",
    "with open(dockerfile_name, 'w') as f:\n",
    "    f.write(dockerfile_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository_lambda = project_name + '-lambda-' + user_name\n",
    "container_image_list.append(ecr_repository_lambda)\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "lambda_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_lambda + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_lambda docker/postprocess-lambda\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_lambda\n",
    "!docker tag {ecr_repository_lambda + tag} $lambda_repository_uri\n",
    "!docker push $lambda_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コンテナイメージを使って AWS Lambda 関数を作成\n",
    "\n",
    "Lambda で使用するコンテナイメージができたら、コンテナイメージを使って Lambda 関数を作成します。ここからは、必要な権限を持つ IAM Role/Policy の作成と Lambda 関数の作成を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def create_container_lambda_function(function_name, image_uri, policy_list=[], trust_service_list=[]):\n",
    "\n",
    "    timestamp = datetime.now(tz=JST).strftime('%Y%m%d%H%M')\n",
    "    lambda_function_name = function_name\n",
    "    lambda_inference_policy_name = lambda_function_name + '-policy-'+timestamp\n",
    "    lambda_inference_role_name = lambda_function_name + '-role-'+timestamp\n",
    "\n",
    "    inline_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"VisualEditor0\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"sagemaker:Search\",\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                'Effect': 'Allow',\n",
    "                'Action': 'logs:CreateLogGroup',\n",
    "                'Resource': f'arn:aws:logs:{region}:{account_id}:*'\n",
    "            },\n",
    "            {\n",
    "                'Effect': 'Allow',\n",
    "                'Action': [\n",
    "                    'logs:CreateLogStream',\n",
    "                    'logs:PutLogEvents'\n",
    "                ],\n",
    "                'Resource': [\n",
    "                    f'arn:aws:logs:{region}:{account_id}:log-group:/aws/lambda/{lambda_function_name}:*'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = iam_client.create_policy(\n",
    "        PolicyName=lambda_inference_policy_name,\n",
    "        PolicyDocument=json.dumps(inline_policy),\n",
    "    )\n",
    "\n",
    "    policy_arn = response['Policy']['Arn']\n",
    "\n",
    "    service_list = [\"lambda.amazonaws.com\"]\n",
    "    for t in trust_service_list:\n",
    "        service_list.append(t)\n",
    "    \n",
    "    assume_role_policy = {\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [{\"Sid\": \"\",\"Effect\": \"Allow\",\"Principal\": {\"Service\":service_list},\"Action\": \"sts:AssumeRole\"}]\n",
    "    }\n",
    "    response = iam_client.create_role(\n",
    "        Path = '/service-role/',\n",
    "        RoleName = lambda_inference_role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy),\n",
    "        MaxSessionDuration=3600*12 # 12 hours\n",
    "    )\n",
    "    \n",
    "    lambda_role_arn = response['Role']['Arn']\n",
    "    lambda_role_name = response['Role']['RoleName']\n",
    "    \n",
    "    response = iam_client.attach_role_policy(\n",
    "        RoleName=lambda_inference_role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "    \n",
    "    for p in policy_list:\n",
    "        arn = 'arn:aws:iam::aws:policy/' + p\n",
    "        response = iam_client.attach_role_policy(\n",
    "            RoleName=lambda_inference_role_name,\n",
    "            PolicyArn=arn\n",
    "        )\n",
    "    \n",
    "    sleep(20) # wait until IAM is created\n",
    "    \n",
    "    response = lambda_client.create_function(\n",
    "        FunctionName=function_name,\n",
    "        Role=lambda_role_arn,\n",
    "        Code={\n",
    "            'ImageUri':image_uri\n",
    "        },\n",
    "        Timeout=60*5, # 5 minutes\n",
    "        MemorySize=128, # 128 MB\n",
    "        Publish=True,\n",
    "        PackageType='Image',\n",
    "    )\n",
    "    \n",
    "    return lambda_role_name, policy_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_function_name_eval = project_name + '-evaluate-model-' + user_name\n",
    "lambda_role, lambda_policy = create_container_lambda_function(lambda_function_name_eval, lambda_repository_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既存の Lambda 関数で使用するコンテナイメージを更新する場合は、以下のセルのコメントアウトを外して実行してください。新しいコンテナイメージが反映\n",
    "されるまで2分ほどお待ちください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = lambda_client.update_function_code(\n",
    "#         FunctionName=lambda_function_name_eval,\n",
    "#         ImageUri=lambda_repository_uri\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaStep の作成\n",
    "\n",
    "作成した Lambda 関数を使って `LambdaStep` を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps.states import Retry\n",
    "lambda_step = stepfunctions.steps.compute.LambdaStep(\n",
    "    \"Query Evaluation Results\",\n",
    "    parameters={\n",
    "        \"FunctionName\": lambda_function_name_eval,\n",
    "        \"Payload\": {\n",
    "            \"experiment-name\": execution_input[\"ExperimentName\"],\n",
    "             \"evaluation-job-name\": execution_input[\"EvaluationProcessingJobName\"],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "lambda_step.add_retry(\n",
    "    Retry(error_equals=[\"States.TaskFailed\"], interval_seconds=15, max_attempts=2, backoff_rate=4.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fail 状態の作成\n",
    "いずれかのステップが失敗したときにワークフローが失敗だとわかるように `Fail` 状態を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_processing_failure = stepfunctions.steps.states.Fail(\n",
    "    \"ML Workflow failed\", cause=\"SageMakerProcessingJobFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ワークフローの中のエラーハンドリングを追加\n",
    "\n",
    "エラーハンドリングのために [Catch Block](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/states.html#stepfunctions.steps.states.Catch) を使用します。もし いずれかの Step が失敗したら、`Fail` 状態に遷移します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = stepfunctions.steps.states.Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=failed_state_sagemaker_processing_failure,\n",
    ")\n",
    "\n",
    "processing_step.add_catch(catch_state_processing)\n",
    "processing_evaluation_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)\n",
    "lambda_step.add_catch(catch_state_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Workflow` の作成と実行\n",
    "\n",
    "ここまでで Step Functions のワークフローを作成する準備が完了しました。それでは、ワークフローを作成して実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain を使って各 Step を連結してワークフローを作成します。既存のワークフローを変更する場合は、update() を実行します。ログに ERROR が表示された場合は、以下のセルを再度実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = Chain([processing_step, training_step, processing_evaluation_step, lambda_step])\n",
    "# workflow_graph = Chain([processing_step, training_step, processing_evaluation_step])\n",
    "\n",
    "branching_workflow = Workflow(\n",
    "    name=project_name + '-' + user_name,\n",
    "    definition=workflow_graph,\n",
    "    role=workflow_execution_role,\n",
    ")\n",
    "\n",
    "branching_workflow.create()\n",
    "branching_workflow.update(workflow_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker ジョブ名の作成\n",
    "\n",
    "ワークフローを繰り返し実行したい場合は、以下のセルから `branching_workflow.execute()` のセルまでの 3つのセルを 1セットとして実行してください。SageMaker のジョブ名は重複させることができないため、timestamp を使ってジョブ名を生成してからワークフローを実行する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now(tz=JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Generate unique names for Pre-Processing Job, Training Job, and Model Evaluation Job for the Step Functions Workflow\n",
    "training_job_name = f\"{project_name}-training-{user_name}-{timestamp}\"  # Each Training Job requires a unique name\n",
    "preprocessing_job_name = f\"{project_name}-preparation-{user_name}-{timestamp}\"  # Each Preprocessing job requires a unique name,\n",
    "evaluation_job_name = f\"{project_name}-evaluation-{user_name}-{timestamp}\"  # Each Evaluation Job requires a unique name\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing ジョブの出力パスの作成\n",
    "\n",
    "Processing ジョブの出力を保存する S3 パスを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理ジョブ用\n",
    "s3_bucket_base_uri = \"{}{}\".format(\"s3://\", bucket)\n",
    "# output_data = \"{}/{}/{}-{}\".format(s3_bucket_base_uri, user_name, \"data/sklearn_processing/output\", timestamp)\n",
    "output_data = \"{}/{}\".format(s3_bucket_base_uri, preprocessing_job_name)\n",
    "prepared_training_data = 0\n",
    "\n",
    "# モデル評価ジョブ用\n",
    "preprocessed_testing_data = \"{}/{}\".format(output_data, \"test_data\")\n",
    "model_data_s3_uri = \"{}/{}/{}\".format(s3_bucket_base_uri, training_job_name, \"output/model.tar.gz\")\n",
    "output_model_evaluation_s3_uri = \"{}/{}/{}\".format(\n",
    "    s3_bucket_base_uri, training_job_name, \"evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ワークフローの実行\n",
    "\n",
    "パラメタを指定して、ワークフローを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute workflow\n",
    "execution = branching_workflow.execute(\n",
    "    inputs={\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,  # Each pre processing job (SageMaker processing job) requires a unique name,\n",
    "#         \"PreprocessingInputData\": input_data,\n",
    "        \"PreprocessingCode\": preparation_input_code,\n",
    "        \"PreprocessingOutputData\": output_data,\n",
    "        \"TrainingJobName\": training_job_name,  # Each Sagemaker Training job requires a unique name,\n",
    "        \"TrainingParameters\": {\n",
    "                                     \"sagemaker_program\": \"train.py\",\n",
    "                                     \"sagemaker_submit_directory\":  train_code,\n",
    "                                      \"epochs\": '1'\n",
    "        },\n",
    "        \"TrainingOutputModel\": model_data_s3_uri,\n",
    "        \"ExperimentName\": experiment_evaluate.experiment_name,\n",
    "        \"EvaluationScriptPath\": input_evaluation_code,\n",
    "        \"EvaluationProcessingJobName\": evaluation_job_name,  # Each SageMaker processing job requires a unique name,\n",
    "        \"EvaluationProcessingOutput\": output_model_evaluation_s3_uri,\n",
    "        \"EvaluationExperimentArgs\": ['--experiment-name', experiment_evaluate.experiment_name,\n",
    "                                     '--model-dir', eval_model_dir,\n",
    "                                     '--data-dir', eval_data_dir]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを実行することで、Workflow の進行状況がわかります。実行開始から12分程度で完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ワークフローの出力を確認\n",
    "\n",
    "Amazon S3 から `evaluation.json` を取得して確認します。ここにはモデルの評価レポートが書かれています。なお、以下のセルは Step Functions でワークフローの実行が完了してから（`evaluation.json` が出力されてから）実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_execution_output_json = execution.get_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "import json\n",
    "\n",
    "evaluation_s3_uri = output_model_evaluation_s3_uri + '/evaluation.json'\n",
    "evaluation_output = S3Downloader.read_file(evaluation_s3_uri)\n",
    "evaluation_output_dict = json.loads(evaluation_output)\n",
    "print(json.dumps(evaluation_output_dict, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Experiments の確認\n",
    "\n",
    "Workflow の実行が完了したら、Experiment の中をのぞいてみましょう。まずは Experiment のデータを ExperimentAnalytics を使って読み出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"TrialComponentName\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": evaluation_job_name,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    experiment_name=experiment_evaluate.experiment_name,\n",
    "    sort_by=\"parameters.accuracy\",\n",
    "#     search_expression=search_expression,\n",
    "#     sort_by=\"metrics.acc.max\",\n",
    "#     sort_order=\"Ascending\",# Ascending or Descending\n",
    "#     metric_names=['metric1', 'metric2'],\n",
    "#     parameter_names=['accuracy', 'roc_auc'],\n",
    "    input_artifact_names=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み出したデータを DataFrame 形式に変換して表示します。accuracy や average_loss が記録されていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = trial_component_analytics.dataframe()\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表示が省略されて見にくいので、DataFrame を HTML ファイルに変換して保存します。以下のセルを実行すると、このノートブックと同じパスに experiments.html というファイルが作成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('colheader_justify', 'center') \n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <head><title>Experiments</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "# OUTPUT AN HTML FILE\n",
    "with open('experiments.html', 'w') as f:\n",
    "    f.write(html_string.format(table=df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノートブックインスタンスでこのノートブックを実行している場合、以下のセルを実行して experiments.html の内容を確認できます。SageMaker Studio を使用している場合は、直接 html ファイルをクリックして確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('experiments.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Functions Workflow の実行のみ行う場合\n",
    "\n",
    "ここまでで、Step Functions Data Science SDK を使って Workflow を作成し、実行するところまでご紹介しました。実際に使用する際は、作成済みの Workflow をパラメタを指定して実行する部分を繰り返すことになります。ここでは、作成済みの Workflow を実行する部分のみを抜き出してご紹介します。\n",
    "\n",
    "Workflow の ARN を使って既存の Workflow を読み出します。ARN はわからないけれど Workflow 名はわかる、という場合は以下のセルを実行することで ARN を取得することができます。（`workflow_name` に Workflow 名を入れてください）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_name = project_name + '-' + user_name\n",
    "workflow_list = Workflow.list_workflows()\n",
    "workflow_arn = [d['stateMachineArn'] for d in workflow_list  if d['name']==workflow_name][0]\n",
    "workflow_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow を読み出して `execute` で実行します。実行する際にパラメタを指定することができます。SageMaker ジョブはユニークな名前である必要があるため、このノートブックでは datetime を使ってジョブ名を生成しています。学習に使用するコードを変更する場合は、使用したいスクリプト（source.tar.gz）をアップロードしてある S3 パスを以下のセルの `train_code` に設定し、コメントアウトを解除してから実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "JST = tz.gettz('Asia/Tokyo')\n",
    "region = sagemaker_session.boto_region_name\n",
    "# input_data = \"s3://sagemaker-sample-data-{}/processing/census/census-income.csv\".format(region)\n",
    "# train_code = 's3://xxx/code/source.tar.gz'\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "timestamp = datetime.now(tz=JST).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Generate unique names for Pre-Processing Job, Training Job, and Model Evaluation Job for the Step Functions Workflow\n",
    "training_job_name = f\"{project_name}-training-{user_name}-{timestamp}\"  # Each Training Job requires a unique name\n",
    "preprocessing_job_name = f\"{project_name}-preparation-{user_name}-{timestamp}\"  # Each Preprocessing job requires a unique name,\n",
    "evaluation_job_name = f\"{project_name}-evaluation-{user_name}-{timestamp}\"  # Each Evaluation Job requires a unique name\n",
    "\n",
    "# 前処理ジョブ用\n",
    "s3_bucket_base_uri = \"{}{}\".format(\"s3://\", bucket)\n",
    "output_data = \"{}/{}\".format(s3_bucket_base_uri, preprocessing_job_name)\n",
    "prepared_training_data = 0\n",
    "\n",
    "# モデル評価ジョブ用\n",
    "preprocessed_testing_data = \"{}/{}\".format(output_data, \"test_data\")\n",
    "model_data_s3_uri = \"{}/{}/{}\".format(s3_bucket_base_uri, training_job_name, \"output/model.tar.gz\")\n",
    "output_model_evaluation_s3_uri = \"{}/{}/{}\".format(\n",
    "    s3_bucket_base_uri, training_job_name, \"evaluation\"\n",
    ")\n",
    "\n",
    "existing_workflow = Workflow.attach(workflow_arn)\n",
    "execution = existing_workflow.execute(\n",
    "    inputs={\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,  # Each pre processing job (SageMaker processing job) requires a unique name,\n",
    "#         \"PreprocessingInputData\": input_data,\n",
    "        \"PreprocessingCode\": preparation_input_code,\n",
    "        \"PreprocessingOutputData\": output_data,\n",
    "        \"TrainingJobName\": training_job_name,  # Each Sagemaker Training job requires a unique name,\n",
    "        \"TrainingParameters\": {\n",
    "                                     \"sagemaker_program\": \"train.py\",\n",
    "                                     \"sagemaker_submit_directory\":  train_code,\n",
    "                                      \"epochs\": '1'\n",
    "        },\n",
    "        \"TrainingOutputModel\": model_data_s3_uri,\n",
    "        \"ExperimentName\": experiment_evaluate.experiment_name,\n",
    "        \"EvaluationScriptPath\": input_evaluation_code,\n",
    "        \"EvaluationProcessingJobName\": evaluation_job_name,  # Each SageMaker processing job requires a unique name,\n",
    "        \"EvaluationProcessingOutput\": output_model_evaluation_s3_uri,\n",
    "        \"EvaluationExperimentArgs\": ['--experiment-name', experiment_evaluate.experiment_name,\n",
    "                                     '--model-dir', eval_model_dir,\n",
    "                                     '--data-dir', eval_data_dir]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを実行することで、Workflow の進行状況がわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リソースの削除\n",
    "\n",
    "このノートブックの実行が終わったら、不要なリソースを削除することを忘れないでください。このノートブックを最後まで実行してリソースの削除をしたら、ノートブックインスタンス、各種データを保存した S3 バケットも不要であれば削除してください。\n",
    "\n",
    "### Step Functions Workflow の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_workflow.delete()\n",
    "\n",
    "def detach_role_policies(role_name):\n",
    "    response = iam_client.list_attached_role_policies(\n",
    "        RoleName=role_name,\n",
    "    )\n",
    "    policies = response['AttachedPolicies']\n",
    "\n",
    "    for p in policies:\n",
    "        response = iam_client.detach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=p['PolicyArn']\n",
    "        )\n",
    "    \n",
    "detach_role_policies(step_functions_role_name)\n",
    "iam_client.delete_role(RoleName=step_functions_role_name)\n",
    "iam_client.delete_policy(PolicyArn=step_functions_policy_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.Session().client('sagemaker')\n",
    "def cleanup(experiment):\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(sagemaker_boto_client=sm, trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                sagemaker_boto_client=sm,\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # tc is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "    experiment.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(experiment_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda 関数の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_role, lambda_policy \n",
    "\n",
    "detach_role_policies(lambda_role)\n",
    "iam_client.delete_role(RoleName=lambda_role)\n",
    "iam_client.delete_policy(PolicyArn=lambda_policy)\n",
    "    \n",
    "lambda_client.delete_function(FunctionName=lambda_function_name_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECR リポジトリの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in container_image_list:\n",
    "    ecr_client.delete_repository(\n",
    "        repositoryName=i,\n",
    "        force=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.199px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
