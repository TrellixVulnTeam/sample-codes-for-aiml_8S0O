{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow によるモデルの学習とマルチモデルエンドポイントでのホスティング\n",
    "\n",
    "こちらは、TensorFlow を SageMaker 上で学習し、ひとつの推論エンドポイントに複数のモデルをデプロイする [マルチモデルエンドポイント (MME)](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/multi-model-endpoints.html#multi-model-endpoint-instance) の機能を使ってモデルをホスティングするノートブックです。このノートブックでは、SageMaker の pre-build Tensorflow コンテナを使う方法と、OSS の [Multi-Model Server (MMS)](https://github.com/awslabs/multi-model-server) を使う方法の 2通りをご紹介します。状況に合わせて適切な方法をご選択ください。2021年5月現在、マルチモデルエンドポイントは GPU インスタンスには対応していません。\n",
    "\n",
    "[SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) は、SageMaker学習インスタンスへのスクリプトの転送を処理します。学習インスタンスでは、SageMakerのネイティブTensorFlowサポートが学習関連の環境変数を設定し、学習スクリプトを実行します。このチュートリアルでは、SageMaker Python SDKを使用して学習ジョブを起動し、学習されたモデルを展開します。\n",
    "TensorFlow Training についての詳細についてはこちらの[ドキュメント](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#train-a-model-with-tensorflow)にアクセスしてください\n",
    "\n",
    "このノートブックでは、マルチモデルエンドポイントを実現するための 2通りの方法をご紹介しますが、それぞれの基本的な違いは以下の通りです。\n",
    "\n",
    "- SageMaker の pre-build Tensorflow コンテナを使う方法\n",
    "  - Tensorflow 2.2.2 かそれより新しいバージョンを使用する場合、かつ、推論データの前処理、後処理が不要な場合。<br>基本的にこちらの方法がシンプルでおすすめ。\n",
    "- Multi-Model Server を使う方法\n",
    "  - マルチモデルエンドポイントがサポートされていないバージョンの Tensorflow を使う場合、または、推論データの前処理、後処理が必要な場合。\n",
    "\n",
    "**※本ノートブックは TensorFlow バージョン 2 以上で動作します。**\n",
    "\n",
    "---\n",
    "\n",
    "## コンテンツ\n",
    "モデルを学習する部分は通常の SageMaker 学習ジョブで行います。学習ジョブによって作成された学習済みモデルを使ってマルチモデルエンドポイントを作成します。\n",
    "\n",
    "1. [環境のセットアップ](#1.-環境のセットアップ)\n",
    "1. [学習データの準備](#2.-学習データの準備)\n",
    "1. [分散学習用のスクリプトを作成する](#3.-分散学習用のスクリプトを作成する)\n",
    "1. [TensorFlow Estimator を利用して学習ジョブを作成する](#4.-TensorFlow-Estimatorを利用して学習ジョブを作成する)\n",
    "1. [Pre-build Tensorflow コンテナを使ってマルチモデルエンドポイントを作成する](#5.-Pre-build-Tensorflow-コンテナを使ってマルチモデルエンドポイントを作成する)\n",
    "1. [Multi-Model Server を使ってマルチモデルエンドポイントを作成する](#6.-Multi-Model-Server-を使ってマルチモデルエンドポイントを作成する)\n",
    "1. [エンドポイントを削除する](#7.エンドポイントを削除する)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 環境のセットアップ\n",
    "\n",
    "まずは環境のセットアップを行いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sagemaker, urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "s3_output = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'tensorflow-mme'\n",
    "tag = ':latest'\n",
    "\n",
    "print(f'Current SageMaker Python SDK Version = {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注） このノートブックでは SageMaker SDK が 2.19.0 以上で動作します。上記の出力結果がそれ以前のバージョンになった際は、下記のセルの#を削除（コメントアウトを解除）して実行、Jupyterカーネルを再起動し、再度上記のセルを実行し、バージョンがアップデートされたことを確認してください。カーネルが再起動されない場合は、SageMaker SDK バージョン更新が反映されません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --quiet \"sagemaker>=2.19.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 学習データの準備\n",
    "\n",
    "MNISTデータセットは、パブリックS3バケット ``sagemaker-sample-data-<REGION>`` の下のプレフィックス ``tensorflow/mnist`` の下にロードされています。 このプレフィックスの下には4つの ``.npy`` ファイルがあります：\n",
    "* ``train_data.npy``\n",
    "* ``eval_data.npy``\n",
    "* ``train_labels.npy``\n",
    "* ``eval_labels.npy``\n",
    "\n",
    "学習データが保存されている s3 の URI を変数に格納しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_uri = f's3://sagemaker-sample-data-{region}/tensorflow/mnist/'\n",
    "print(training_data_uri)\n",
    "!aws s3 ls {training_data_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 分散学習用のスクリプトを作成する\n",
    "\n",
    "このチュートリアルの学習スクリプトは、TensorFlowの公式の [CNN MNISTの例](https://www.tensorflow.org/tutorials/images/cnn?hl=ja) をベースに作成されました。 SageMaker から渡された `` model_dir`` パラメーターを処理するように変更しています。 これは、分散学習時のデータ共有、チェックポイント、モデルの永続保存などに使用できるS3パスです。 また、学習関連の変数を扱うために、引数をパースする関数も追加しました。\n",
    "\n",
    "学習ジョブの最後に、学習済みモデルを環境変数 ``SM_MODEL_DIR`` に保存されているパスにエクスポートするステップを追加しました。このパスは常に ``/opt/ml/model`` をポイントします。 SageMaker は、学習の終了時にこのフォルダー内のすべてのモデル成果物をS3にアップロードするため、これは重要です。\n",
    "\n",
    "スクリプト全体は次のとおりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TensorFlow Estimatorを利用して学習ジョブを作成する\n",
    "\n",
    "`sagemaker.tensorflow.TensorFlow`　estimator は、スクリプトモード対応の TensorFlow コンテナの指定、学習・推論スクリプトの S3 へのアップロード、および SageMaker 学習ジョブの作成を行います。ここでいくつかの重要なパラメーターを呼び出しましょう。\n",
    "\n",
    "* `py_version`は` 'py3'`に設定されています。レガシーモードは Python 2 のみをサポートしているため、この学習スクリプトはスクリプトモードを使用していることを示しています。Python2は間もなく廃止されますが、 `py_version` を設定することでPython 2でスクリプトモードを使用できます。`'py2'`と` script_mode`を `True`にします。\n",
    "\n",
    "* `distributions` は、分散学習設定を構成するために使用されます。インスタンスのクラスターまたは複数の GPU をまたいで分散学習を行う場合にのみ必要です。ここでは、分散学習スキーマとしてパラメーターサーバーを使用しています。 SageMaker 学習ジョブは同種のクラスターで実行されます。 SageMaker セットアップでパラメーターサーバーのパフォーマンスを向上させるために、クラスター内のすべてのインスタンスでパラメーターサーバーを実行するため、起動するパラメーターサーバーの数を指定する必要はありません。スクリプトモードは、[Horovod](https://github.com/horovod/horovod) による分散学習もサポートしています。 `distributions` の設定方法に関する詳細なドキュメントは[こちら](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training) をご参照ください。\n",
    "\n",
    "* 実際にモデル開発をする際はコード(ここでは `mnist.py` )にバグが混入していないか確認しながら実行することになりますが、学習インスタンスを利用すると、インスタンスの起動に時間がかかるため、学習開始コマンドを打ち込んでから 10 分後に気づいてやり直し、となってしまうことがあります。そのオーバヘッドを防止するために、ローカルモードでの学習が Sagemaker ではサポートされています。``instance_type=local``を指定するだけで、ノートブックインスタンスで学習（＝インスタンスの立ち上げ時間なしで）を試すことができます。よくやるやり方としてはコードの確認用途のため、 epoch の数やデータを減らして動くかどうかの確認を行うことが多いです。\n",
    "\n",
    "また、Spot Instanceを用いて実行する場合は、下記のコードを `Estimator` の `train_instance_type` の次の行に追加しましょう。\n",
    "\n",
    "```python\n",
    "                             max_run = 5000, # 学習は最大で5000秒までにする設定\n",
    "                             use_spot_instances = 'True',\n",
    "                             max_wait = 7200 # 学習完了を待つ最大時間\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             instance_count=2,\n",
    "                             # instance_type='local',\n",
    "                             instance_type='ml.p3.2xlarge',\n",
    "                             framework_version='2.2.2',\n",
    "                             py_version='py37',\n",
    "                             distribution={'parameter_server': {'enabled': True}},\n",
    "                             hyperparameters={\n",
    "                                 \"epochs\": 4,\n",
    "                                 'batch-size':16\n",
    "                             }\n",
    "#                              max_run = 5000, # 学習は最大で5000秒までにする設定\n",
    "#                              use_spot_instances = 'True',\n",
    "#                              max_wait = 7200 # 学習完了を待つ最大時間\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``fit`` による学習ジョブの実行\n",
    "\n",
    "学習ジョブを開始するには、`estimator.fit（training_data_uri）` を呼び出します。\n",
    "\n",
    "ここでは、S3 ロケーションが入力として使用されます。 `fit` は、`training` という名前のデフォルトチャネルを作成します。これは、このS3ロケーションを指します。学習スクリプトでは、 `SM_CHANNEL_TRAINING` に保存されている場所から学習データにアクセスできます。 `fit`は、他のいくつかのタイプの入力も受け入れます。詳細については、APIドキュメント[こちら](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) を参照してください。\n",
    "\n",
    "学習が開始されると、TensorFlow コンテナは mnist.py を実行し、スクリプトの引数として　estimator から`hyperparameters` と `model_dir` を渡します。この例では、estimator 内で定義していないハイパーパラメーターは渡されず、 `model_dir` のデフォルトは `s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>` であるため、スクリプトの実行は次のようになります。\n",
    "```bash\n",
    "python mnist.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>\n",
    "```\n",
    "学習が完了すると、学習ジョブは保存されたモデルを TensorFlow serving にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルチモデルエンドポイントは、指定された S3 パスにデプロイしたい全てのモデルを保存します。このノートブックでは、先ほど実行した学習ジョブの学習済みモデルが保存されているパスをマルチモデルエンドポイント用のパスとして使用します。\n",
    "\n",
    "以下のセルでは、学習済みモデルが保存されたパスを取得しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dirname = os.path.dirname(mnist_estimator.model_data)\n",
    "dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数のモデルを学習させるのは時間がかかるので、このノートブックでは先ほど学習したモデルを複製してデプロイします。\n",
    "\n",
    "以下のセルでは、S3 に保存されている model.tar.gz を model2.tar.gz から model7.tar.gz までの 6 回複製しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $dirname/model.tar.gz $dirname/model2.tar.gz\n",
    "!aws s3 cp $dirname/model.tar.gz $dirname/model3.tar.gz\n",
    "!aws s3 cp $dirname/model.tar.gz $dirname/model4.tar.gz\n",
    "!aws s3 cp $dirname/model.tar.gz $dirname/model5.tar.gz\n",
    "!aws s3 cp $dirname/model.tar.gz $dirname/model6.tar.gz\n",
    "!aws s3 cp $dirname/model.tar.gz $dirname/model7.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あとで推論を実行する際に使用する入力データを準備しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --region {region} s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist/eval_data.npy eval_data.npy\n",
    "!aws --region {region} s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist/eval_labels.npy eval_labels.npy\n",
    "\n",
    "eval_data = np.load('eval_data.npy').reshape(-1,28,28,1)\n",
    "eval_labels = np.load('eval_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットから 50 枚のみ抜き出して `test_data` とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000 # choose your favorite number from 0 to 9950\n",
    "test_data = eval_data[k:k+50]\n",
    "test_data\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        plt.subplot(5, 10, 10* i + j+1)\n",
    "        plt.imshow(test_data[10 * i + j, :].reshape(28, 28), cmap='gray')\n",
    "        plt.title(10* i + j+1)\n",
    "        plt.tick_params(labelbottom=False, labelleft = False)\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-build Tensorflow コンテナを使ってマルチモデルエンドポイントを作成する\n",
    "\n",
    "まずは、SageMaker が用意している pre-build Tensorflow コンテナを使ってマルチモデルエンドポイントを作成してみましょう。Tensorflow 2.2.2 を含む新しいバージョンがマルチモデルエンドポイントに対応しています。Pre-build コンテナの一覧は [こちら](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) から参照可能です。古いバージョンや、SageMaker が用意していないバージョンの Tensorflow を使いたい場合は、[6. Multi-Model Server を使ってマルチモデルエンドポイントを作成する](#6.-Multi-Model-Server-を使ってマルチモデルエンドポイントを作成する) の方法を使用してください。\n",
    "\n",
    "まず TensorFlowModel を作成し、それを引数として MultiDataModel を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import TensorFlowModel\n",
    "model = TensorFlowModel(role=role,\n",
    "                        image_uri= '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.2.2-cpu-py37-ubuntu18.04',\n",
    "                        model_data=mnist_estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiDataModel の引数 `model_data_prefix` には、デプロイしたいモデルたちが保存されている S3 パスを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_name = 'tensorflow-mnist-mme-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_name = endpoint_name\n",
    "\n",
    "mme = MultiDataModel(name=model_name,\n",
    "                     model_data_prefix=dirname + '/',\n",
    "                     model=model,# passing our model - passes container image needed for the endpoint\n",
    "                     sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiDataModel の deploy() を使って推論エンドポイントを起動します。このエンドポイントがマルチモデルエンドポイントとなります。エンドポイントの起動には 10 分ほどかかります。セルの下にしばらく - が表示されたのち、最後に ! が表示されたらエンドポイントの起動完了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mme.deploy(initial_instance_count=1,\n",
    "                       instance_type='ml.m5.xlarge',\n",
    "                       endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントにデプロイされているモデルの一覧を見てみましょう。先ほど複製した合計 7 つのモデルが表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、起動完了した推論エンドポイントに推論リクエストを投げて推論を実行しましょう。`instances` というラベルで入力データを指定して predict() を実行します。合わせて、`TargetModel` に使用したいモデル名を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {'instances': test_data.reshape(-1,28,28,1)}\n",
    "predictions = predictor.predict(data=payload, initial_args={'TargetModel': 'model.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論結果を確認します。0.96 ほどの精度で MNIST 画像分類ができていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_true = 0\n",
    "for i in range(0, 50):\n",
    "    prediction = np.argmax(predictions['predictions'][i])\n",
    "    label = eval_labels[i+k]\n",
    "    if prediction == label:\n",
    "        count_true += 1\n",
    "    print(' [{}]: prediction is {}, label is {}, matched: {}'.format(i+1, prediction, label, prediction == label))\n",
    "    \n",
    "print('Accuracy: ', (count_true/50.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、`model2.tar.gz` を使って推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = predictor.predict(data=payload, initial_args={'TargetModel': 'model2.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もう一度、`model2.tar.gz` を使って推論を実行します。先ほどの初回実行時と推論時間がどれくらい変わったでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = predictor.predict(data=payload, initial_args={'TargetModel': 'model2.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、`model3.tar.gz` を使って推論を実行します。初回実行時は推論結果が返ってくるまでに 3秒程度かかりますが、2回目以降の呼び出しでは 100 ms ほどになっていたのではないでしょうか。これは、初回は S3 からモデルをダウンロードする必要がありますが、2回目以降はモデルがメモリにキャッシュされるためです。メモリに乗り切らないほど多数、もしくはサイズの大きいのモデルをデプロイした場合、メモリからは追い出されますが推論エンドポイントにアタッチされたストレージにモデルは保存されるため、初回推論時ほどの時間はかかりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = predictor.predict(data=payload, initial_args={'TargetModel': 'model3.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新しいモデルのアップロード\n",
    "新しくモデルを追加してみましょう。モデルの追加のためにエンドポイントの設定などを変更する必要はありません。デプロイ済みのモデルが保存されている S3 パスに新しいモデルをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $dirname/model.tar.gz $dirname/model8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アップロードしたモデルがマルチモデルエンドポイントの参照先に反映されているか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しいモデルを使って推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {'instances': test_data.reshape(-1,28,28,1)}\n",
    "predictions = predictor.predict(data=payload, initial_args={'TargetModel': 'model8.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multi-Model Server を使ってマルチモデルエンドポイントを作成する\n",
    "\n",
    "前処理、後処理を定義したい、MME が対応していないバージョンの Tensorflow を使いたい場合、OSS の [Multi-Model Server](https://github.com/awslabs/multi-model-server) を使って MME を実現することが可能です。\n",
    "\n",
    "まずは、推論で使用するコンテナをビルドする準備をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docker/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/inference/Dockerfile\n",
    "\n",
    "# FROM ubuntu:18.04\n",
    "FROM tensorflow/tensorflow:2.2.2-py3\n",
    "\n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "# Install necessary dependencies for MMS and SageMaker Inference Toolkit\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    openjdk-8-jdk-headless \\\n",
    "    python3-dev \\\n",
    "    curl \\\n",
    "    vim \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && curl -O https://bootstrap.pypa.io/get-pip.py \\\n",
    "    && python3 get-pip.py\n",
    "\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
    "RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
    "\n",
    "# Install MXNet, MMS, and SageMaker Inference Toolkit to set up MMS\n",
    "RUN pip3 --no-cache-dir install \\\n",
    "                                multi-model-server \\\n",
    "                                sagemaker-inference \\\n",
    "                                retrying\n",
    "\n",
    "# Copy entrypoint script to the image\n",
    "COPY dockerd-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
    "RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
    "\n",
    "RUN mkdir -p /home/model-server/\n",
    "\n",
    "# Copy the default custom service file to handle incoming data and inference requests\n",
    "COPY model_handler.py /home/model-server/model_handler.py\n",
    "\n",
    "# Define an entrypoint script for the docker image\n",
    "ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
    "\n",
    "# Define command to be passed to the entrypoint\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/inference/dockerd-entrypoint.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "import os\n",
    "from retrying import retry\n",
    "from subprocess import CalledProcessError\n",
    "from sagemaker_inference import model_server\n",
    "\n",
    "def _retry_if_error(exception):\n",
    "    return isinstance(exception, CalledProcessError or OSError)\n",
    "\n",
    "@retry(stop_max_delay=1000 * 50,\n",
    "       retry_on_exception=_retry_if_error)\n",
    "def _start_mms():\n",
    "    # by default the number of workers per model is 1, but we can configure it through the\n",
    "    # environment variable below if desired.\n",
    "    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n",
    "    model_server.start_model_server(handler_service='/home/model-server/model_handler.py:handle')\n",
    "\n",
    "def main():\n",
    "    if sys.argv[1] == 'serve':\n",
    "        _start_mms()\n",
    "    else:\n",
    "        subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n",
    "\n",
    "    # prevent docker exit\n",
    "    subprocess.call(['tail', '-f', '/dev/null'])\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論リクエストを処理する部分を model_handler.py で定義します。`initialize()` で学習済みモデルをロードし、`preprocess()` でデータの前処理、`inference()` で推論実行、`postprocess()` で推論結果の後処理をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/inference/model_handler.py\n",
    "\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os,json,argparse\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "class ModelHandler(object):\n",
    "    \"\"\"\n",
    "    A sample Model handler implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.model = None\n",
    "        self.shapes = None\n",
    "\n",
    "    def initialize(self, context):\n",
    "        \"\"\"\n",
    "        Initialize model. This will be called during model loading time\n",
    "        :param context: Initial context contains model server system properties.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.initialized = True\n",
    "        properties = context.system_properties\n",
    "        # Contains the url parameter passed to the load request\n",
    "        model_dir = properties.get(\"model_dir\") \n",
    "         \n",
    "        # Load  model\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(os.path.join(model_dir, '000000001'))\n",
    "            \n",
    "        except (RuntimeError) as memerr:\n",
    "            if re.search('Failed to allocate (.*) Memory', str(memerr), re.IGNORECASE):\n",
    "                logging.error(\"Memory allocation exception: {}\".format(memerr))\n",
    "                raise MemoryError\n",
    "            raise           \n",
    "\n",
    "    def preprocess(self, request):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        :param request: list of raw requests\n",
    "        :return: list of preprocessed model input data\n",
    "        \"\"\"\n",
    "        # Take the input data and pre-process it make it inference ready\n",
    "\n",
    "        payload = request[0]['body']\n",
    "        \n",
    "        data =  np.frombuffer(payload, dtype=np.float32).reshape(-1,28,28,1)\n",
    "        return data\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        \"\"\"\n",
    "        Internal inference methods\n",
    "        :param model_input: transformed model input data list\n",
    "        :return: list of inference output in NDArray\n",
    "        \"\"\"\n",
    "        prediction = self.model.predict(model_input)\n",
    "        return prediction\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"\n",
    "        Return predict result in as list.\n",
    "        :param inference_output: list of inference output\n",
    "        :return: list of predict results\n",
    "        \"\"\"\n",
    "        print('======inference=======')\n",
    "        return [str(inference_output.tolist())]\n",
    "        \n",
    "    def handle(self, data, context):\n",
    "        \"\"\"\n",
    "        Call preprocess, inference and post-process functions\n",
    "        :param data: input data\n",
    "        :param context: mms context\n",
    "        \"\"\"\n",
    "        \n",
    "        model_input = self.preprocess(data)\n",
    "        model_out = self.inference(model_input)\n",
    "        return self.postprocess(model_out)\n",
    "\n",
    "_service = ModelHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(context)\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    return _service.handle(data, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したファイルを使って Docker イメージをビルドし、Amazon ECR に push します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr_repository_inference = 'tensorflow-mme'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "inference_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_inference + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_inference docker/inference\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_inference\n",
    "!docker tag {ecr_repository_inference + tag} $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "push したコンテナイメージを使って `create_model` を実行します。コンテナイメージを指定する際に、デプロイしたいモデルたちが保存されている S3 パスも指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'tf-MultiModelModel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "container = {\n",
    "    'Image': inference_repository_uri,\n",
    "    'ModelDataUrl': dirname + '/',\n",
    "    'Mode': 'MultiModel'\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_model` で作成したモデルを使って `create_endpoint_config` を実行してエンドポイント設定を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'tf-MultiModelEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 2,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したエンドポイント設定を使って推論エンドポイントを起動します。このエンドポイントがマルチモデルエンドポイントとなります。エンドポイントの起動には 10 分ほどかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name_mms = 'tensorflow-mnist-mme-mms-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name_mms)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_mms,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_mms)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name_mms))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name_mms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論エンドポイントの起動が完了したら、`invoke_endpoint()` を使って推論を実行します。まずは `model.tar.gz` を使って推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "predictions = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_mms,\n",
    "    ContentType='application/x-npy',\n",
    "    TargetModel='model.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=test_data.reshape(-1,28,28,1).tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論エンドポイントからは str として結果が返ってくるので、それを numpy に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str = predictions['Body'].read().decode('utf-8')\n",
    "pred = np.array(json.loads(pred_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論結果をラベルデータと比較します。問題なく推論できていそうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_true = 0\n",
    "for i in range(0, 50):\n",
    "    prediction = np.argmax(pred[i])\n",
    "    label = eval_labels[i+k]\n",
    "    if prediction == label:\n",
    "        count_true += 1\n",
    "    print(' [{}]: prediction is {}, label is {}, matched: {}'.format(i+1, prediction, label, prediction == label))\n",
    "    \n",
    "print('Accuracy: ', (count_true/50.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もう一度、`model.tar.gz` で推論を実行してみます。推論にかかる時間はどれくらい変わったでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "predictions = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_mms,\n",
    "    ContentType='application/x-npy',\n",
    "    TargetModel='model.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=test_data.reshape(-1,28,28,1).tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度は `model2.tar.gz` を実行してみます。すぐ上のセルに表示された実行時間と比べてどうでしょうか。\n",
    "\n",
    "初回実行時は推論結果が返ってくるまでに 6 秒程度かかりますが、2回目以降の呼び出しでは 100 ms ほどになっていたのではないでしょうか。これは、初回は S3 からモデルをダウンロードする必要がありますが、2回目以降はモデルがメモリにキャッシュされるためです。メモリに乗り切らないほど多数、もしくはサイズの大きいのモデルをデプロイした場合、メモリからは追い出されますが推論エンドポイントにアタッチされたストレージにモデルは保存されるため、初回推論時ほどの時間はかかりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "predictions = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_mms,\n",
    "    ContentType='application/x-npy',\n",
    "    TargetModel='model2.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=test_data.reshape(-1,28,28,1).tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新しいモデルのアップロード\n",
    "新しくモデルを追加してみましょう。モデルの追加のためにエンドポイントの設定などを変更する必要はありません。デプロイ済みのモデルが保存されている S3 パスに新しいモデルをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $dirname/model.tar.gz $dirname/model9.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アップロードしたモデルを使って推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "predictions = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_mms,\n",
    "    ContentType='application/x-npy',\n",
    "    TargetModel='model9.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=test_data.reshape(-1,28,28,1).tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.エンドポイントを削除する\n",
    "\n",
    "推論用エンドポイントは停止されるまで課金が発生します。そのため。不要になったエンドポイントはすぐに削除することをおすすめします。以下のコードでエンドポイントが削除されます。AWS コンソールの左側のメニューから「エンドポイント」をクリックし、停止したいエンドポイントを選択して削除することも可能です。\n",
    "\n",
    "こちらは、pre-build コンテナを使用して作成したエンドポイントを削除するコードです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらは、カスタムコンテナと Multi-Model Server を使って作成したエンドポイントを削除するコードです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.delete_endpoint(\n",
    "    EndpointName=endpoint_name_mms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
